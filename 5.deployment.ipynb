{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5. Amazon SageMaker Deployment \n",
    "---\n",
    "\n",
    "본 모듈에서는 SageMaker에서 호스팅 엔드포인트를 배포하는 법을 알아봅니다. \n",
    "\n",
    "\n",
    "### AWS Managed Inference Container\n",
    "SageMaker 추론은 각 프레임워크별에 적합한 배포 컨테이너들이 사전에 빌드되어 있으며, TensorFlow는 텐서플로 서빙, 파이토치는 torchserve, MXNet은 MMS(Multi Model Server), scikit learn은 Flask가 내장되어 있습니다. PyTorch, 기존에는 MMS가 내장되어 있었지만, 2020년 말부터 Amazon과 facebook이 공동으로 개발한 torchserve를 내장하기 시작했습니다. \n",
    "\n",
    "배포 컨테이너를 구동할 때에는 추론을 위한 http 요청을 받아들일 수 있는 RESTful API를 실행하는 serve 명령어가 자동으로 실행되면서 엔드포인트가 시작됩니다. 엔드포인트를 시작할 때, SageMaker는 도커 컨테이너에서 사용 가능한 외부의 모델 아티팩트, 데이터, 그리고 기타 환경 설정 정보 등을 배포 인스턴스의 /opt/ml 폴더로 로딩합니다. \n",
    "\n",
    "![container](imgs/inference_container.png)\n",
    "\n",
    "도커 파일은 오픈 소스로 공개되어 있으며, AWS에서는 구 버전부터 최신 버전까지 다양한 버전을 제공하고 있습니다.\n",
    "각 프레임워크의 도커 파일은 아래 링크를 참조하십시오.\n",
    "\n",
    "- TensorFlow containes: https://github.com/aws/sagemaker-tensorflow-containers \n",
    "- PyTorch container: https://github.com/aws/sagemaker-pytorch-container   \n",
    "- MXNet containes: https://github.com/aws/sagemaker-mxnet-containers\n",
    "- Chainer container: https://github.com/aws/sagemaker-chainer-container \n",
    "- Scikit-learn container: https://github.com/aws/sagemaker-scikit-learn-container\n",
    "- SparkML serving container: https://github.com/aws/sagemaker-sparkml-serving-container\n",
    "\n",
    "또한, AWS CLI를 사용하여 프레임워크별로 지원되는 버전을 간단하게 확인 가능합니다.\n",
    "\n",
    "```sh\n",
    "$ aws ecr list-images --repository-name tensorflow-inference --registry-id 76310435188\n",
    "$ aws ecr list-images --repository-name pytorch-inference --registry-id 763104351884\n",
    "$ aws ecr list-images --repository-name mxnet-inference --registry-id 763104351884\n",
    "\n",
    "# EIA(Elastic Inference)\n",
    "$ aws ecr list-images --repository-name tensorflow-inference-eia --registry-id 763104351884\n",
    "$ aws ecr list-images --repository-name pytorch-inference-eia --registry-id 763104351884\n",
    "$ aws ecr list-images --repository-name mxnet-inference-eia --registry-id 763104351884\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1. Inference script\n",
    "---\n",
    "\n",
    "아래 코드 셀은 `src` 디렉토리에 SageMaker 추론 스크립트인 `inference.py`를 저장합니다.<br>\n",
    "\n",
    "이 스크립트는 SageMaker 상에서 호스팅 엔드포인트를 쉽게 배포할 수 이는 high-level 툴킷인 SageMaker inference toolkit의 인터페이스를\n",
    "사용하고 있으며, 여러분께서는 인터페이스에 정의된 핸들러(handler) 함수들만 구현하시면 됩니다. 아래 인터페이스는 텐서플로를 제외한 프레임워크들에서 공용으로 사용됩니다. \n",
    "- `model_fn()`: S3나 model zoo에 저장된 모델을 추론 인스턴스의 메모리로 로드 후, 모델을 리턴하는 방법을 정의하는 전처리 함수입니다.\n",
    "- `input_fn()`: 사용자로부터 입력받은 내용을 모델 추론에 적합하게 변환하는 전처리 함수로, content_type 인자값을 통해 입력값 포맷을 확인할 수 있습니다.\n",
    "- `predict_fn()`: model_fn()에서 리턴받은 모델과 input_fn()에서 변환된 데이터로 추론을 수행합니다.\n",
    "- `output_fn()`: 추론 결과를 반환하는 후처리 함수입니다.\n",
    "\n",
    "Tip: `input_fn(), predict_fn(), output_fn()`을 각각 구현하는 대신, 세 함수들을 한꺼번에 묶어서 `transform()` 함수에 구현하는 것도 가능합니다. 아래 Code snippet 예시를 참조하십시오.\n",
    "\n",
    "```python\n",
    "# Option 1\n",
    "def model_fn(model_dir):\n",
    "    model = Your_Model()\n",
    "    return model\n",
    "\n",
    "def input_fn(request_body, content_type):\n",
    "    if content_type == 'text/csv'\n",
    "        ...\n",
    "    else:\n",
    "        pass:\n",
    "        \n",
    "def predict_fn(request_body, content_type):\n",
    "    # Preform prediction\n",
    "    return model(input_data)\n",
    "      \n",
    "def output_fn(prediction, content_type):\n",
    "    # Serialize the prediction result \n",
    "```\n",
    "\n",
    "```python\n",
    "# Option 2\n",
    "def model_fn(model_dir):\n",
    "    model = Your_Model()\n",
    "    return model\n",
    "\n",
    "def transform_fn(model, input_data, content_type, accept):\n",
    "    # All-in-one function, including input_fn, predict_fn(), and output_fn()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker 훈련 컨테이너에서 1.6.0을 사용하였기 때문에, 로컬 추론 테스트 시에도 동일한 버전으로 추론합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/inference.py\n",
    "\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import io\n",
    "import tarfile\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import copy\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch import topk\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "JSON_CONTENT_TYPE = 'application/json'\n",
    "\n",
    "# Loads the model into memory from storage and return the model.\n",
    "def model_fn(model_dir):\n",
    "    logger.info(\"==> model_dir : {}\".format(model_dir))\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    last_hidden_units = model.fc.in_features\n",
    "    model.fc = torch.nn.Linear(last_hidden_units, 186)\n",
    "    model.load_state_dict(torch.load(os.path.join(model_dir, 'model.pth')))\n",
    "    return model\n",
    "\n",
    "# Deserialize the request body\n",
    "def input_fn(request_body, request_content_type='application/x-image'):\n",
    "    print('An input_fn that loads a image tensor')\n",
    "    print(request_content_type)\n",
    "    if request_content_type == 'application/x-image':             \n",
    "        img = np.array(Image.open(io.BytesIO(request_body)))\n",
    "    elif request_content_type == 'application/x-npy':    \n",
    "        img = np.frombuffer(request_body, dtype='uint8').reshape(137, 236)   \n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'Requested unsupported ContentType in content_type : ' + request_content_type)\n",
    "\n",
    "    img = 255 - img\n",
    "    img = img[:,:,np.newaxis]\n",
    "    img = np.repeat(img, 3, axis=2)    \n",
    "\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    img_tensor = test_transforms(img)\n",
    "\n",
    "    return img_tensor         \n",
    "        \n",
    "\n",
    "# Predicts on the deserialized object with the model from model_fn()\n",
    "def predict_fn(input_data, model):\n",
    "    logger.info('Entering the predict_fn function')\n",
    "    start_time = time.time()\n",
    "    input_data = input_data.unsqueeze(0)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    input_data = input_data.to(device)\n",
    "                          \n",
    "    result = {}\n",
    "                                                 \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_data)\n",
    "        pred_probs = F.softmax(logits, dim=1).data.squeeze()   \n",
    "        outputs = topk(pred_probs, 5)                  \n",
    "        result['score'] = outputs[0].detach().cpu().numpy()\n",
    "        result['class'] = outputs[1].detach().cpu().numpy()\n",
    "    \n",
    "    print(\"--- Elapsed time: %s secs ---\" % (time.time() - start_time))    \n",
    "    return result        \n",
    "\n",
    "# Serialize the prediction result into the response content type\n",
    "def output_fn(pred_output, accept=JSON_CONTENT_TYPE):\n",
    "    return json.dumps({'score': pred_output['score'].tolist(), \n",
    "                       'class': pred_output['class'].tolist()}), accept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Local Endpoint Inference\n",
    "---\n",
    "\n",
    "충분한 검증 및 테스트 없이 훈련된 모델을 곧바로 실제 운영 환경에 배포하기에는 많은 위험 요소들이 있습니다. 따라서, 로컬 모드를 사용하여 실제 운영 환경에 배포하기 위한 추론 인스턴스를 시작하기 전에 노트북 인스턴스의 로컬 환경에서 모델을 배포하는 것을 권장합니다. 이를 로컬 모드 엔드포인트(Local Mode Endpoint)라고 합니다.\n",
    "\n",
    "먼저, 로컬 모드 엔드포인트의 컨테이너 배포 이전에 로컬 환경 상에서 직접 추론을 수행하여 결과를 확인하고, 곧바로 로컬 모드 엔드포인트를 배포해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`content_type='application/x-image'` 일 경우 추론을 수행하는 예시입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An input_fn that loads a image tensor\n",
      "application/x-image\n",
      "==> model_dir : ./model\n",
      "Entering the predict_fn function\n",
      "--- Elapsed time: 3.429753065109253 secs ---\n",
      "{'score': array([0.5855128 , 0.3301886 , 0.01439991, 0.01150947, 0.00949198],\n",
      "      dtype=float32), 'class': array([  3,   2,  64, 179, 168])}\n"
     ]
    }
   ],
   "source": [
    "from src.inference import model_fn, input_fn, predict_fn, output_fn\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "file_path = 'test_imgs/test_0.jpg'\n",
    "with open(file_path, mode='rb') as file:\n",
    "    img_byte = bytearray(file.read())\n",
    "data = input_fn(img_byte)\n",
    "model = model_fn('./model')\n",
    "result = predict_fn(data, model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`content_type='application/x-npy'` 일 경우 추론을 수행하는 예시이며, numpy 행렬을 그대로 전송하게 됩니다. 속도는 `content_type='application/x-image'` 보다 더 빠르지만, `tobytes()`로 \n",
    "변환하여 전송할 경우 numpy 행렬의 `dtype`과 행렬 `shape`이 보존되지 않으므로 별도의 처리가 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An input_fn that loads a image tensor\n",
      "application/x-npy\n",
      "==> model_dir : ./model\n",
      "Entering the predict_fn function\n",
      "--- Elapsed time: 0.019454479217529297 secs ---\n",
      "{'score': array([0.5855128 , 0.3301886 , 0.01439991, 0.01150947, 0.00949198],\n",
      "      dtype=float32), 'class': array([  3,   2,  64, 179, 168])}\n"
     ]
    }
   ],
   "source": [
    "img_arr = np.array(Image.open(file_path))\n",
    "data = input_fn(img_arr.tobytes(), request_content_type='application/x-npy')\n",
    "model = model_fn('./model')\n",
    "result = predict_fn(data, model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Mode Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "아래 코드 셀을 실행 후, 로그를 확인해 보세요. MMS에 대한 세팅값들을 확인하실 수 있습니다.\n",
    "\n",
    "```bash\n",
    "Attaching to z1ciqmehg6-algo-1-wida0\n",
    "z1ciqmehg6-algo-1-wida0 | ['torchserve', '--start', '--model-store', '/.sagemaker/ts/models', '--ts-config', '/etc/sagemaker-ts.properties', '--log-config', '/opt/conda/lib/python3.6/site-packages/sagemaker_pytorch_serving_container/etc/log4j.properties', '--models', 'model.mar']\n",
    "z1ciqmehg6-algo-1-wida0 | 2021-04-22 04:09:27,813 [INFO ] main org.pytorch.serve.ModelServer - \n",
    "z1ciqmehg6-algo-1-wida0 | Torchserve version: 0.2.1\n",
    "z1ciqmehg6-algo-1-wida0 | TS Home: /opt/conda/lib/python3.6/site-packages\n",
    "z1ciqmehg6-algo-1-wida0 | Current directory: /\n",
    "z1ciqmehg6-algo-1-wida0 | Temp directory: /home/model-server/tmp\n",
    "z1ciqmehg6-algo-1-wida0 | Number of GPUs: 0\n",
    "z1ciqmehg6-algo-1-wida0 | Number of CPUs: 8\n",
    "z1ciqmehg6-algo-1-wida0 | Max heap size: 15352 M\n",
    "z1ciqmehg6-algo-1-wida0 | Python executable: /opt/conda/bin/python\n",
    "z1ciqmehg6-algo-1-wida0 | Config file: /etc/sagemaker-ts.properties\n",
    "z1ciqmehg6-algo-1-wida0 | Inference address: http://0.0.0.0:8080\n",
    "z1ciqmehg6-algo-1-wida0 | Management address: http://0.0.0.0:8080\n",
    "z1ciqmehg6-algo-1-wida0 | Metrics address: http://127.0.0.1:8082\n",
    "z1ciqmehg6-algo-1-wida0 | Model Store: /.sagemaker/ts/models\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 디버깅 Tip\n",
    "만약 로컬에서 추론이 잘 되는데, 엔드포인트 배포에서 에러가 발생하면 프레임워크 버전이 맞지 않거나 컨테이너 환경 변수 설정이 잘못되었을 가능성이 높습니다.\n",
    "예를 들어, PyTorch 1.6.0에서 훈련한 모델은 PyTorch 1.3.1에서 추론이 되지 않습니다.\n",
    "프레임워크 버전은 가급적 동일한 버전으로 통일하되, 버전 통일이 불가능하면 가장 비슷한 버전을 사용해 보세요. 예를 들어, PyTorch 1.6.0으로 훈련한 모델을 1.5.0 버전 상에서 배포할 수 있습니다.<br>\n",
    "만약 비슷한 버전에서도 추론이 되지 않는다면, BYOC(Bring Your Own Container)로 Amazon ECR에 동일한 버전의 컨테이너를 등록할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to z1ciqmehg6-algo-1-wida0\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m ['torchserve', '--start', '--model-store', '/.sagemaker/ts/models', '--ts-config', '/etc/sagemaker-ts.properties', '--log-config', '/opt/conda/lib/python3.6/site-packages/sagemaker_pytorch_serving_container/etc/log4j.properties', '--models', 'model.mar']\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:27,813 [INFO ] main org.pytorch.serve.ModelServer - \n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m Torchserve version: 0.2.1\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m TS Home: /opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m Current directory: /\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m Temp directory: /home/model-server/tmp\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m Number of GPUs: 0\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m Number of CPUs: 8\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m Max heap size: 15352 M\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m Python executable: /opt/conda/bin/python\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m Config file: /etc/sagemaker-ts.properties\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m Inference address: http://0.0.0.0:8080\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m Management address: http://0.0.0.0:8080\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m Metrics address: http://127.0.0.1:8082\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m Model Store: /.sagemaker/ts/models\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m Initial Models: model.mar\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m Log dir: /logs\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m Metrics dir: /logs\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m Netty threads: 0\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m Netty client threads: 0\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m Default workers per model: 8\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m Blacklist Regex: N/A\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m Maximum Response Size: 6553500\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m Maximum Request Size: 6553500\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m Prefer direct buffer: false\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m Custom python dependency for model allowed: false\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m Metrics report format: prometheus\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m Enable metrics API: true\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:27,853 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:28,720 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag 78d4c7da070c452b88785461ef0555ba\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:28,732 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:28,749 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,015 [INFO ] W-9005-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9005\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,018 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9003\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,016 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9000\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,016 [INFO ] W-9006-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9006\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,015 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9001\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,019 [INFO ] W-9006-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]50\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,019 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9002\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,019 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]51\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,018 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]54\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,020 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,020 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,018 [INFO ] W-9005-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]52\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,018 [INFO ] W-9007-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9007\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,022 [INFO ] W-9005-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,022 [INFO ] W-9005-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,022 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,022 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,020 [INFO ] W-9006-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,020 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]57\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,023 [INFO ] W-9006-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,023 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,023 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,026 [INFO ] W-9007-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]56\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,027 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]53\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,027 [INFO ] W-9007-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,027 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,027 [INFO ] W-9007-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,027 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,035 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,035 [INFO ] W-9004-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9004\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,035 [INFO ] W-9006-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9006\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,036 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,036 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,036 [INFO ] W-9004-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]55\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,036 [INFO ] W-9007-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9007\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,036 [INFO ] W-9004-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,036 [INFO ] W-9004-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9004\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,036 [INFO ] W-9005-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9005\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,037 [INFO ] W-9004-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,036 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,067 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,067 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,070 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,078 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,078 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,078 [INFO ] W-9007-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9007.\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,078 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,078 [INFO ] W-9004-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9004.\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,078 [INFO ] W-9006-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9006.\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,078 [INFO ] W-9005-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9005.\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,080 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m Model server started.\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,349 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:698503716b79,timestamp:1619064569\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,352 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:83.26641464233398|#Level:Host|#hostname:698503716b79,timestamp:1619064569\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,353 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:9.648555755615234|#Level:Host|#hostname:698503716b79,timestamp:1619064569\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,355 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:10.4|#Level:Host|#hostname:698503716b79,timestamp:1619064569\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,357 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:46706.67578125|#Level:Host|#hostname:698503716b79,timestamp:1619064569\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,358 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:14016.5078125|#Level:Host|#hostname:698503716b79,timestamp:1619064569\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:29,371 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:23.9|#Level:Host|#hostname:698503716b79,timestamp:1619064569\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:30,124 [INFO ] W-9005-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - ==> model_dir : /home/model-server/tmp/models/78d4c7da070c452b88785461ef0555ba\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:30,126 [INFO ] W-9005-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - ==> model_dir : /home/model-server/tmp/models/78d4c7da070c452b88785461ef0555ba\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:30,127 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - ==> model_dir : /home/model-server/tmp/models/78d4c7da070c452b88785461ef0555ba\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:30,142 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - ==> model_dir : /home/model-server/tmp/models/78d4c7da070c452b88785461ef0555ba\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:30,159 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - ==> model_dir : /home/model-server/tmp/models/78d4c7da070c452b88785461ef0555ba\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:30,174 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - ==> model_dir : /home/model-server/tmp/models/78d4c7da070c452b88785461ef0555ba\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:30,192 [INFO ] W-9007-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - ==> model_dir : /home/model-server/tmp/models/78d4c7da070c452b88785461ef0555ba\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:30,193 [INFO ] W-9007-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - ==> model_dir : /home/model-server/tmp/models/78d4c7da070c452b88785461ef0555ba\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:30,195 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - ==> model_dir : /home/model-server/tmp/models/78d4c7da070c452b88785461ef0555ba\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:30,196 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - ==> model_dir : /home/model-server/tmp/models/78d4c7da070c452b88785461ef0555ba\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:30,223 [INFO ] W-9006-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - ==> model_dir : /home/model-server/tmp/models/78d4c7da070c452b88785461ef0555ba\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:30,226 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - ==> model_dir : /home/model-server/tmp/models/78d4c7da070c452b88785461ef0555ba\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:30,226 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - ==> model_dir : /home/model-server/tmp/models/78d4c7da070c452b88785461ef0555ba\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:30,258 [INFO ] W-9006-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - ==> model_dir : /home/model-server/tmp/models/78d4c7da070c452b88785461ef0555ba\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:30,366 [INFO ] W-9004-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - ==> model_dir : /home/model-server/tmp/models/78d4c7da070c452b88785461ef0555ba\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:30,366 [INFO ] W-9004-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - ==> model_dir : /home/model-server/tmp/models/78d4c7da070c452b88785461ef0555ba\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,036 [WARN ] W-9007-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,036 [WARN ] W-9007-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,038 [WARN ] W-9006-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,038 [WARN ] W-9006-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,046 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,046 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,048 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,048 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,048 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,049 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,055 [WARN ] W-9005-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,055 [WARN ] W-9005-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,057 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,057 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,138 [WARN ] W-9007-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -   0%|          | 0.00/44.7M [00:00<?, ?B/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,140 [WARN ] W-9006-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -   0%|          | 0.00/44.7M [00:00<?, ?B/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,146 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -   0%|          | 0.00/44.7M [00:00<?, ?B/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,149 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -   0%|          | 0.00/44.7M [00:00<?, ?B/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,150 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -   0%|          | 0.00/44.7M [00:00<?, ?B/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,155 [WARN ] W-9005-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -   0%|          | 0.00/44.7M [00:00<?, ?B/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,157 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -   0%|          | 0.00/44.7M [00:00<?, ?B/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,183 [WARN ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,183 [WARN ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,240 [WARN ] W-9007-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  10%|▉         | 4.31M/44.7M [00:00<00:00, 44.2MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,241 [WARN ] W-9006-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  10%|▉         | 4.38M/44.7M [00:00<00:00, 45.2MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,249 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -   8%|▊         | 3.37M/44.7M [00:00<00:01, 35.3MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,250 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  10%|▉         | 4.42M/44.7M [00:00<00:00, 46.4MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,251 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  11%|█         | 4.81M/44.7M [00:00<00:00, 49.8MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,283 [WARN ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -   0%|          | 0.00/44.7M [00:00<?, ?B/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,285 [WARN ] W-9005-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  12%|█▏        | 5.49M/44.7M [00:00<00:00, 57.5MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,287 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  12%|█▏        | 5.45M/44.7M [00:00<00:00, 57.2MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,350 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  15%|█▌        | 6.73M/44.7M [00:00<00:01, 34.7MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,351 [WARN ] W-9006-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  21%|██        | 9.25M/44.7M [00:00<00:00, 48.4MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,355 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  21%|██▏       | 9.50M/44.7M [00:00<00:00, 50.1MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,356 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  21%|██▏       | 9.57M/44.7M [00:00<00:00, 49.4MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,363 [WARN ] W-9007-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  21%|██        | 9.19M/44.7M [00:00<00:00, 48.0MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,386 [WARN ] W-9005-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  25%|██▍       | 11.0M/44.7M [00:00<00:00, 48.9MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,387 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  24%|██▍       | 10.9M/44.7M [00:00<00:00, 48.6MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,395 [WARN ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  19%|█▉        | 8.44M/44.7M [00:00<00:00, 88.5MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,450 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  31%|███       | 13.9M/44.7M [00:00<00:00, 52.7MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,451 [WARN ] W-9006-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  31%|███       | 13.9M/44.7M [00:00<00:00, 46.3MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,455 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  32%|███▏      | 14.3M/44.7M [00:00<00:00, 48.9MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,456 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  32%|███▏      | 14.3M/44.7M [00:00<00:00, 48.5MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,464 [WARN ] W-9007-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  31%|███       | 13.8M/44.7M [00:00<00:00, 43.3MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,486 [WARN ] W-9005-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  36%|███▋      | 16.3M/44.7M [00:00<00:00, 51.7MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,488 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  36%|███▋      | 16.2M/44.7M [00:00<00:00, 51.6MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,537 [WARN ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  38%|███▊      | 16.9M/44.7M [00:00<00:00, 82.4MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,561 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  45%|████▌     | 20.2M/44.7M [00:00<00:00, 53.9MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,563 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  45%|████▌     | 20.1M/44.7M [00:00<00:00, 57.7MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,566 [WARN ] W-9006-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  46%|████▌     | 20.6M/44.7M [00:00<00:00, 55.5MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,570 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  47%|████▋     | 20.9M/44.7M [00:00<00:00, 56.6MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,580 [WARN ] W-9007-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  46%|████▋     | 20.7M/44.7M [00:00<00:00, 54.2MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,588 [WARN ] W-9005-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  50%|████▉     | 22.2M/44.7M [00:00<00:00, 55.6MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,588 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  48%|████▊     | 21.2M/44.7M [00:00<00:00, 51.9MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,661 [WARN ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  55%|█████▌    | 24.8M/44.7M [00:00<00:00, 69.7MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,663 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  57%|█████▋    | 25.4M/44.7M [00:00<00:00, 53.1MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,664 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  57%|█████▋    | 25.6M/44.7M [00:00<00:00, 55.1MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,666 [WARN ] W-9006-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  58%|█████▊    | 25.9M/44.7M [00:00<00:00, 52.8MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,670 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  59%|█████▉    | 26.3M/44.7M [00:00<00:00, 53.8MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,681 [WARN ] W-9007-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  58%|█████▊    | 26.0M/44.7M [00:00<00:00, 51.7MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,688 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  61%|██████    | 27.4M/44.7M [00:00<00:00, 56.0MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,690 [WARN ] W-9005-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  62%|██████▏   | 27.6M/44.7M [00:00<00:00, 55.4MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,762 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  69%|██████▉   | 30.8M/44.7M [00:00<00:00, 54.1MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,764 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  71%|███████   | 31.5M/44.7M [00:00<00:00, 57.2MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,770 [WARN ] W-9006-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  72%|███████▏  | 31.9M/44.7M [00:00<00:00, 56.2MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,771 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  72%|███████▏  | 32.2M/44.7M [00:00<00:00, 56.5MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,776 [WARN ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  71%|███████   | 31.6M/44.7M [00:00<00:00, 65.1MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,780 [WARN ] W-9007-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  73%|███████▎  | 32.6M/44.7M [00:00<00:00, 57.3MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,789 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  75%|███████▍  | 33.3M/44.7M [00:00<00:00, 58.2MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,796 [WARN ] W-9005-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  75%|███████▍  | 33.4M/44.7M [00:00<00:00, 56.9MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,862 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  83%|████████▎ | 37.2M/44.7M [00:00<00:00, 58.4MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,864 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  84%|████████▍ | 37.7M/44.7M [00:00<00:00, 59.5MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,866 [WARN ] W-9007-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  87%|████████▋ | 38.7M/44.7M [00:00<00:00, 59.3MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,867 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  99%|█████████▉| 44.4M/44.7M [00:00<00:00, 63.5MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,867 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 100%|█████████▉| 44.5M/44.7M [00:00<00:00, 63.2MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,868 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  88%|████████▊ | 39.4M/44.7M [00:00<00:00, 59.6MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,870 [WARN ] W-9006-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  84%|████████▍ | 37.6M/44.7M [00:00<00:00, 56.5MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,871 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  84%|████████▍ | 37.6M/44.7M [00:00<00:00, 56.5MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,875 [WARN ] W-9006-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  99%|█████████▉| 44.1M/44.7M [00:00<00:00, 60.2MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,876 [WARN ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  85%|████████▍ | 37.9M/44.7M [00:00<00:00, 62.6MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,876 [WARN ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 100%|██████████| 44.7M/44.7M [00:00<00:00, 64.8MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,876 [WARN ] W-9005-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  87%|████████▋ | 38.9M/44.7M [00:00<00:00, 55.9MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:31,880 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  97%|█████████▋| 43.4M/44.7M [00:00<00:00, 57.7MB/s]\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:32,034 [INFO ] W-9007-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2861\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:32,035 [INFO ] W-9007-model_1 TS_METRICS - W-9007-model_1.ms:3291|#Level:Host|#hostname:698503716b79,timestamp:1619064572\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:32,394 [INFO ] pool-1-thread-9 ACCESS_LOG - /172.19.0.1:60114 \"GET /ping HTTP/1.1\" 200 11\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:32,395 [INFO ] pool-1-thread-9 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:698503716b79,timestamp:null\n",
      "!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.pytorch.model.PyTorchPredictor at 0x7fb5cc2ba748>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:33,530 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4383\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:33,530 [INFO ] W-9001-model_1 TS_METRICS - W-9001-model_1.ms:4788|#Level:Host|#hostname:698503716b79,timestamp:1619064573\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:33,569 [INFO ] W-9005-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4424\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:33,570 [INFO ] W-9005-model_1 TS_METRICS - W-9005-model_1.ms:4826|#Level:Host|#hostname:698503716b79,timestamp:1619064573\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:33,596 [INFO ] W-9004-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4445\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:33,596 [INFO ] W-9004-model_1 TS_METRICS - W-9004-model_1.ms:4853|#Level:Host|#hostname:698503716b79,timestamp:1619064573\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:33,600 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4435\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:33,600 [INFO ] W-9000-model_1 TS_METRICS - W-9000-model_1.ms:4860|#Level:Host|#hostname:698503716b79,timestamp:1619064573\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:33,622 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4449\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:33,622 [INFO ] W-9002-model_1 TS_METRICS - W-9002-model_1.ms:4880|#Level:Host|#hostname:698503716b79,timestamp:1619064573\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:33,624 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4455\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:33,624 [INFO ] W-9003-model_1 TS_METRICS - W-9003-model_1.ms:4881|#Level:Host|#hostname:698503716b79,timestamp:1619064573\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:33,627 [INFO ] W-9006-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4458\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:33,627 [INFO ] W-9006-model_1 TS_METRICS - W-9006-model_1.ms:4883|#Level:Host|#hostname:698503716b79,timestamp:1619064573\n"
     ]
    }
   ],
   "source": [
    "local_model_path = f'file://{os.getcwd()}/model/model.tar.gz'\n",
    "endpoint_name = \"local-endpoint-bangali-classifier-{}\".format(int(time.time()))\n",
    "\n",
    "local_pytorch_model = PyTorchModel(model_data=local_model_path,\n",
    "                                   role=role,\n",
    "                                   entry_point='./src/inference.py',\n",
    "                                   framework_version='1.6.0',\n",
    "                                   py_version='py3')\n",
    "\n",
    "local_pytorch_model.deploy(instance_type='local', \n",
    "                           initial_instance_count=1, \n",
    "                           endpoint_name=endpoint_name,\n",
    "                           wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로컬에서 컨테이너를 배포했기 때문에 컨테이너가 현재 실행 중임을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE                                                                          COMMAND                  CREATED             STATUS              PORTS                              NAMES\n",
      "698503716b79        763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.6.0-cpu-py3   \"python /usr/local/b…\"   18 seconds ago      Up 16 seconds       0.0.0.0:8080->8080/tcp, 8081/tcp   z1ciqmehg6-algo-1-wida0\n"
     ]
    }
   ],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker SDK `predict()` 메서드로 추론을 수행할 수도 있지만, 이번에는 boto3의 `invoke_endpoint()` 메서드로 추론을 수행해 보겠습니다.<br>\n",
    "Boto3는 서비스 레벨의 low-level SDK로, ML 실험에 초점을 맞춰 일부 기능들이 추상화된 high-level SDK인 SageMaker SDK와 달리\n",
    "SageMaker API를 완벽하게 제어할 수 있습으며, 프로덕션 및 자동화 작업에 적합합니다.\n",
    "\n",
    "참고로 `invoke_endpoint()` 호출을 위한 런타임 클라이언트 인스턴스 생성 시, 로컬 배포 모드에서는 `sagemaker.local.LocalSagemakerRuntimeClient()`를 호출해야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:44,180 [INFO ] W-9007-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - An input_fn that loads a image tensor\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:44,180 [INFO ] W-9007-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - application/x-npy\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:44,180 [INFO ] W-9007-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Entering the predict_fn function\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:44,180 [INFO ] W-9007-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Entering the predict_fn function\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:44,216 [INFO ] W-9007-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - --- Elapsed time: 0.0353550910949707 secs ---\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:44,217 [INFO ] W-9007-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 39\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:44,216 [INFO ] W-9007-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:36.83|#ModelName:model,Level:Model|#hostname:698503716b79,requestID:ee8b7769-d51f-431d-986f-a8a5b6b77f4f,timestamp:1619064584\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:44,218 [INFO ] W-9007-model_1 ACCESS_LOG - /172.19.0.1:60120 \"POST /invocations HTTP/1.1\" 200 48\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:44,218 [INFO ] W-9007-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:698503716b79,timestamp:null\n",
      "{\"score\": [0.5855128169059753, 0.3301886022090912, 0.014399887062609196, 0.011509465985000134, 0.00949197169393301], \"class\": [3, 2, 64, 179, 168]}\n"
     ]
    }
   ],
   "source": [
    "client = sagemaker.local.LocalSagemakerClient()\n",
    "runtime_client = sagemaker.local.LocalSagemakerRuntimeClient()\n",
    "endpoint_name = local_pytorch_model.endpoint_name\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType='application/x-npy',\n",
    "    Accept='application/json',\n",
    "    Body=img_arr.tobytes()\n",
    "    )\n",
    "print(response['Body'].read().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ContentType을 x-image로 추론하는 예시입니다. 동일한 결과가 출력되는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:47,039 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - An input_fn that loads a image tensor\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:47,040 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - application/x-image\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:47,040 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Entering the predict_fn function\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:47,040 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Entering the predict_fn function\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:47,092 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 62\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:47,092 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - --- Elapsed time: 0.05186748504638672 secs ---\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:47,092 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:60.75|#ModelName:model,Level:Model|#hostname:698503716b79,requestID:c1305b6e-9baf-4774-a8c2-3bcc2062438d,timestamp:1619064587\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:47,092 [INFO ] W-9001-model_1 ACCESS_LOG - /172.19.0.1:60120 \"POST /invocations HTTP/1.1\" 200 63\n",
      "\u001b[36mz1ciqmehg6-algo-1-wida0 |\u001b[0m 2021-04-22 04:09:47,093 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:698503716b79,timestamp:null\n",
      "{'score': [0.5855128169059753, 0.3301886022090912, 0.014399887062609196, 0.011509465985000134, 0.00949197169393301], 'class': [3, 2, 64, 179, 168]}\n"
     ]
    }
   ],
   "source": [
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType='application/x-image',\n",
    "    Accept='application/json',\n",
    "    Body=img_byte\n",
    "    )\n",
    "\n",
    "print(json.loads(response['Body'].read().decode()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Mode Endpoint Clean-up\n",
    "\n",
    "엔드포인트를 계속 사용하지 않는다면, 엔드포인트를 삭제해야 합니다. \n",
    "SageMaker SDK에서는 `delete_endpoint()` 메소드로 간단히 삭제할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_endpoint(client, endpoint_name):\n",
    "    response = client.describe_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "    model_name = response['ProductionVariants'][0]['ModelName']\n",
    "\n",
    "    client.delete_model(ModelName=model_name)    \n",
    "    client.delete_endpoint(EndpointName=endpoint_name)\n",
    "    client.delete_endpoint_config(EndpointConfigName=endpoint_name)    \n",
    "    \n",
    "    print(f'--- Deleted model: {model_name}')\n",
    "    print(f'--- Deleted endpoint: {endpoint_name}')\n",
    "    print(f'--- Deleted endpoint_config: {endpoint_name}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gracefully stopping... (press Ctrl+C again to force)\n",
      "--- Deleted model: pytorch-inference-2021-04-22-04-09-21-366\n",
      "--- Deleted endpoint: local-endpoint-bangali-classifier-1619064557\n",
      "--- Deleted endpoint_config: local-endpoint-bangali-classifier-1619064557\n"
     ]
    }
   ],
   "source": [
    "delete_endpoint(client, endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컨테이너가 삭제된 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n"
     ]
    }
   ],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. SageMaker Hosted Endpoint Inference\n",
    "---\n",
    "\n",
    "이제 실제 운영 환경에 엔드포인트 배포를 수행해 보겠습니다. 로컬 모드 엔드포인트와 대부분의 코드가 동일하며, 모델 아티팩트 경로(`model_data`)와 인스턴스 유형(`instance_type`)만 변경해 주시면 됩니다. SageMaker가 관리하는 배포 클러스터를 프로비저닝하는 시간이 소요되기 때문에 추론 서비스를 시작하는 데에는 약 5~10분 정도 소요됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "client = boto3.client('sagemaker')\n",
    "runtime_client = boto3.client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_path(sm_client, max_results=1, name_contains='pytorch-training'):\n",
    "    training_job = sm_client.list_training_jobs(MaxResults=max_results,\n",
    "                                         NameContains=name_contains,\n",
    "                                         SortBy='CreationTime', \n",
    "                                         SortOrder='Descending')\n",
    "    training_job_name = training_job['TrainingJobSummaries'][0]['TrainingJobName']\n",
    "    training_job_description = sm_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "    model_path = training_job_description['ModelArtifacts']['S3ModelArtifacts']  \n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-143656149352/pytorch-training-2021-04-22-03-43-18-411/output/model.tar.gz\n",
      "-------------!CPU times: user 4.31 s, sys: 456 ms, total: 4.76 s\n",
      "Wall time: 6min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_path = get_model_path(client, max_results=3)\n",
    "endpoint_name = \"endpoint-bangali-classifier-{}\".format(int(time.time()))\n",
    "print(model_path)\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data=model_path,\n",
    "                                   role=role,\n",
    "                                   entry_point='./src/inference.py',\n",
    "                                   framework_version='1.6.0',\n",
    "                                   py_version='py3')\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type='ml.m5.xlarge', \n",
    "                                 initial_instance_count=1, \n",
    "                                 endpoint_name=endpoint_name,\n",
    "                                 wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointName': 'endpoint-bangali-classifier-1619064696',\n",
       " 'EndpointArn': 'arn:aws:sagemaker:us-east-1:143656149352:endpoint/endpoint-bangali-classifier-1619064696',\n",
       " 'EndpointConfigName': 'endpoint-bangali-classifier-1619064696',\n",
       " 'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "   'DeployedImages': [{'SpecifiedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.6.0-cpu-py3',\n",
       "     'ResolvedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference@sha256:61106396693b4b14927ec360e1a0bdfd0464363c0b1717e3bdf15438f18a4924',\n",
       "     'ResolutionTime': datetime.datetime(2021, 4, 22, 4, 11, 46, 609000, tzinfo=tzlocal())}],\n",
       "   'CurrentWeight': 1.0,\n",
       "   'DesiredWeight': 1.0,\n",
       "   'CurrentInstanceCount': 1,\n",
       "   'DesiredInstanceCount': 1}],\n",
       " 'EndpointStatus': 'InService',\n",
       " 'CreationTime': datetime.datetime(2021, 4, 22, 4, 11, 42, 547000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2021, 4, 22, 4, 18, 6, 760000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '0644ffa2-e20a-4093-a4f0-e9a41c1969c1',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '0644ffa2-e20a-4093-a4f0-e9a41c1969c1',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '766',\n",
       "   'date': 'Thu, 22 Apr 2021 04:18:13 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name = pytorch_model.endpoint_name\n",
    "client.describe_endpoint(EndpointName = endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추론을 수행합니다. 로컬 모드의 코드와 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': [0.5855132341384888, 0.33018821477890015, 0.01439987774938345, 0.011509452946484089, 0.009491969831287861], 'class': [3, 2, 64, 179, 168]}\n"
     ]
    }
   ],
   "source": [
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType='application/x-image',\n",
    "    Accept='application/json',\n",
    "    Body=img_byte\n",
    "    )\n",
    "\n",
    "print(json.loads(response['Body'].read().decode()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Hosted Endpoint Clean-up\n",
    "\n",
    "엔드포인트를 계속 사용하지 않는다면, 불필요한 과금을 피하기 위해 엔드포인트를 삭제해야 합니다. \n",
    "SageMaker SDK에서는 `delete_endpoint()` 메소드로 간단히 삭제할 수 있으며, UI에서도 쉽게 삭제할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_endpoint(client, endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
