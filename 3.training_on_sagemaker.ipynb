{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3. Training on Amazon SageMaker\n",
    "---\n",
    "\n",
    "본 모듈에서는 Amazon SageMaker API를 호출하여 모델 훈련을 수행합니다. Multi-GPU 분산 훈련에 더 관심이 있거나, SageMaker 기본 용법에 익숙하신 분들은 이 모듈을 건너 뛰고 Module 4로 곧바로 진행하시면 됩니다.\n",
    "\n",
    "앞의 모듈과 달리 SageMaker notebook instance는 저렴한 인스턴스(예: `ml.t3.medium`)를 사용하시면 되고, 훈련 인스턴스 지정 시 GPU 기반 인스턴스(예: `ml.p2.xlarge`, `ml.p3.2xlarge`)를 선택하시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Training script\n",
    "---\n",
    "\n",
    "아래 코드 셀은 `src` 디렉토리에 SageMaker 훈련 스크립트인 `train.py`를 저장합니다.<br>\n",
    "Module 2를 진행하셨다면 아래 스크립트가 Module 2의 코드와 대부분 일치하다는 점을 알 수 있습니다. 다시 말해, SageMaker 훈련 스크립트 파일은 기존 온프레미스에서 사용했던 Python 스크립트 파일과 크게 다르지 않으며, SageMaker 훈련 컨테이너에서 수행하기 위한 추가적인 환경 변수들만 설정하시면 됩니다.\n",
    "\n",
    "환경 변수 설정의 code snippet은 아래과 같습니다.\n",
    "\n",
    "```python\n",
    "# SageMaker Container environment\n",
    "parser.add_argument('--train_dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n",
    "parser.add_argument('--num_gpus', type=int, default=os.environ['SM_NUM_GPUS'])\n",
    "parser.add_argument('--model_dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/train.py\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import sys\n",
    "import time, datetime\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import recall_score\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "HEIGHT = 137\n",
    "WIDTH = 236\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "\n",
    "class BangaliDataset(Dataset):\n",
    "    def __init__(self, imgs, label_df=None, transform=None):\n",
    "        self.imgs = imgs\n",
    "        self.label_df = label_df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_idx = self.label_df.iloc[idx].id\n",
    "        img = (self.imgs[img_idx]).astype(np.uint8)\n",
    "        img = 255 - img\n",
    "    \n",
    "        img = img[:,:,np.newaxis]\n",
    "        img = np.repeat(img, 3, axis=2)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(image=img)['image']        \n",
    "        \n",
    "        if self.label_df is not None:\n",
    "            label_1 = self.label_df.iloc[idx].grapheme_root\n",
    "            label_2 = self.label_df.iloc[idx].vowel_diacritic\n",
    "            label_3 = self.label_df.iloc[idx].consonant_diacritic           \n",
    "            return img, np.array([label_1, label_2, label_3])        \n",
    "        else:\n",
    "            return img\n",
    "        \n",
    "        \n",
    "def _set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    mx.random.seed(seed)\n",
    "\n",
    "def _get_images(train_dir, num_folds=5, vld_fold_idx=4, data_type='train'):\n",
    "\n",
    "    logger.info(\"=== Getting Labels ===\")\n",
    "    logger.info(train_dir)\n",
    "    \n",
    "    label_df = pd.read_csv(os.path.join(train_dir, 'train_folds.csv'))\n",
    "    #label_df = pd.read_csv(f'{train_dir}/train_folds.csv')\n",
    "     \n",
    "    trn_fold = [i for i in range(num_folds) if i not in [vld_fold_idx]]\n",
    "    vld_fold = [vld_fold_idx]\n",
    "\n",
    "    trn_idx = label_df.loc[label_df['fold'].isin(trn_fold)].index\n",
    "    vld_idx = label_df.loc[label_df['fold'].isin(vld_fold)].index\n",
    "\n",
    "    logger.info(\"=== Getting Images ===\")    \n",
    "    files = [f'{train_dir}/{data_type}_image_data_{i}.feather' for i in range(4)]\n",
    "    logger.info(files)\n",
    "    \n",
    "    image_df_list = [pd.read_feather(f) for f in files]\n",
    "    imgs = [df.iloc[:, 1:].values.reshape(-1, HEIGHT, WIDTH) for df in image_df_list]\n",
    "    del image_df_list\n",
    "    gc.collect()\n",
    "    imgs = np.concatenate(imgs, axis=0)\n",
    "    \n",
    "    trn_df = label_df.loc[trn_idx]\n",
    "    vld_df = label_df.loc[vld_idx]\n",
    "    \n",
    "    return imgs, trn_df, vld_df       \n",
    "                           \n",
    "                           \n",
    "def _get_data_loader(imgs, trn_df, vld_df):\n",
    "\n",
    "    import albumentations as A\n",
    "    from albumentations import (\n",
    "        Rotate,HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "        Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "        IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, RandomBrightnessContrast, IAAPiecewiseAffine,\n",
    "        IAASharpen, IAAEmboss, Flip, OneOf, Compose\n",
    "    )\n",
    "    from albumentations.pytorch import ToTensor, ToTensorV2\n",
    "\n",
    "    train_transforms = A.Compose([\n",
    "        Rotate(20),\n",
    "            OneOf([\n",
    "                IAAAdditiveGaussianNoise(),\n",
    "                GaussNoise(),\n",
    "            ], p=0.2),\n",
    "            OneOf([\n",
    "                MotionBlur(p=.2),\n",
    "                MedianBlur(blur_limit=3, p=0.1),\n",
    "                Blur(blur_limit=3, p=0.1),\n",
    "            ], p=0.2),\n",
    "            ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n",
    "            OneOf([\n",
    "                OpticalDistortion(p=0.3),\n",
    "                GridDistortion(p=.1),\n",
    "                IAAPiecewiseAffine(p=0.3),\n",
    "            ], p=0.2),\n",
    "            OneOf([\n",
    "                CLAHE(clip_limit=2),\n",
    "                IAASharpen(),\n",
    "                IAAEmboss(),\n",
    "                RandomBrightnessContrast(),            \n",
    "            ], p=0.3),\n",
    "            HueSaturationValue(p=0.3),\n",
    "        ToTensor()\n",
    "        ], p=1.0)\n",
    "\n",
    "\n",
    "    valid_transforms = A.Compose([\n",
    "        ToTensor()\n",
    "    ])\n",
    "\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    trn_dataset = BangaliDataset(imgs=imgs, label_df=trn_df, transform=train_transforms)\n",
    "    vld_dataset = BangaliDataset(imgs=imgs, label_df=vld_df, transform=valid_transforms)\n",
    "\n",
    "    trn_loader = DataLoader(trn_dataset, shuffle=True, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE)\n",
    "    vld_loader = DataLoader(vld_dataset, shuffle=False, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    return trn_loader, vld_loader\n",
    "\n",
    "\n",
    "def _rand_bbox(size, lam):\n",
    "    '''\n",
    "    CutMix Helper function.\n",
    "    Retrieved from https://github.com/clovaai/CutMix-PyTorch/blob/master/train.py\n",
    "    '''\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    # 폭과 높이는 주어진 이미지의 폭과 높이의 beta distribution에서 뽑은 lambda로 얻는다\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    \n",
    "    # patch size 의 w, h 는 original image 의 w,h 에 np.sqrt(1-lambda) 를 곱해준 값입니다.\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # patch의 중심점은 uniform하게 뽑힘\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "\n",
    "def _format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "\n",
    "def train_model(args):\n",
    "    #num_epochs, num_folds, vld_fold_idx, batch_size, lr, log_interval, train_dir, model_dir):\n",
    "    from torchvision import datasets, models\n",
    "    from tqdm import tqdm\n",
    "   \n",
    "    imgs, trn_df, vld_df = _get_images(args.train_dir, args.num_folds, args.vld_fold_idx, data_type='train')\n",
    "    trn_loader, vld_loader = _get_data_loader(imgs, trn_df, vld_df)\n",
    "\n",
    "    logger.info(\"=== Getting Pre-trained model ===\")    \n",
    "    model = models.resnet18(pretrained=True)\n",
    "    last_hidden_units = model.fc.in_features\n",
    "    model.fc = torch.nn.Linear(last_hidden_units, 186)\n",
    "    model = model.cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max',\n",
    "                                                          verbose=True, patience=5, \n",
    "                                                          factor=0.5)\n",
    "\n",
    "    best_score = -1\n",
    "    training_stats = []\n",
    "    logger.info(\"=== Start Training ===\")    \n",
    "\n",
    "    for epoch_id in range(args.num_epochs):\n",
    "\n",
    "        ################################################################################\n",
    "        # ==> Training phase\n",
    "        ################################################################################    \n",
    "        trn_loss = []\n",
    "        model.train()\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for batch_id, (inputs, targets) in enumerate((trn_loader)):\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            targets_gra = targets[:, 0]\n",
    "            targets_vow = targets[:, 1]\n",
    "            targets_con = targets[:, 2]\n",
    "\n",
    "            # 50%의 확률로 원본 데이터 그대로 사용    \n",
    "            if np.random.rand() < 0.5:\n",
    "                logits = model(inputs)\n",
    "                grapheme = logits[:, :168]\n",
    "                vowel = logits[:, 168:179]\n",
    "                cons = logits[:, 179:]\n",
    "\n",
    "                loss1 = loss_fn(grapheme, targets_gra)\n",
    "                loss2 = loss_fn(vowel, targets_vow)\n",
    "                loss3 = loss_fn(cons, targets_con) \n",
    "\n",
    "            else:\n",
    "\n",
    "                lam = np.random.beta(1.0, 1.0) \n",
    "                rand_index = torch.randperm(inputs.size()[0])\n",
    "                shuffled_targets_gra = targets_gra[rand_index]\n",
    "                shuffled_targets_vow = targets_vow[rand_index]\n",
    "                shuffled_targets_con = targets_con[rand_index]\n",
    "\n",
    "                bbx1, bby1, bbx2, bby2 = _rand_bbox(inputs.size(), lam)\n",
    "                inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "                # 픽셀 비율과 정확히 일치하도록 lambda 파라메터 조정  \n",
    "                lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))\n",
    "\n",
    "                logits = model(inputs)\n",
    "                grapheme = logits[:,:168]\n",
    "                vowel = logits[:, 168:179]\n",
    "                cons = logits[:, 179:]\n",
    "\n",
    "                loss1 = loss_fn(grapheme, targets_gra) * lam + loss_fn(grapheme, shuffled_targets_gra) * (1. - lam)\n",
    "                loss2 = loss_fn(vowel, targets_vow) * lam + loss_fn(vowel, shuffled_targets_vow) * (1. - lam)\n",
    "                loss3 = loss_fn(cons, targets_con) * lam + loss_fn(cons, shuffled_targets_con) * (1. - lam)\n",
    "\n",
    "            loss = 0.5 * loss1 + 0.25 * loss2 + 0.25 * loss3    \n",
    "            trn_loss.append(loss.item())\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Printing vital information\n",
    "            if (batch_id + 1) % (args.log_interval) == 0:\n",
    "                s = f'[Epoch {epoch_id} Batch {batch_id+1}/{len(trn_loader)}] ' \\\n",
    "                f'loss: {running_loss / args.log_interval:.4f}'\n",
    "                print(s)\n",
    "                running_loss = 0\n",
    "\n",
    "        # Measure how long this epoch took.\n",
    "        trn_time = _format_time(time.time() - t0)        \n",
    "\n",
    "        ################################################################################\n",
    "        # ==> Validation phase\n",
    "        ################################################################################\n",
    "        val_loss = []\n",
    "        val_true = []\n",
    "        val_pred = []\n",
    "        model.eval()\n",
    "        \n",
    "        # === Validation phase ===\n",
    "        logger.info('=== Start Validation ===')        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in vld_loader:\n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "                logits = model(inputs)\n",
    "                grapheme = logits[:,:168]\n",
    "                vowel = logits[:, 168:179]\n",
    "                cons = logits[:, 179:]\n",
    "\n",
    "                loss= 0.5* loss_fn(grapheme, targets[:,0]) + 0.25*loss_fn(vowel, targets[:,1]) + \\\n",
    "                0.25*loss_fn(vowel, targets[:,2])\n",
    "                val_loss.append(loss.item())\n",
    "\n",
    "                grapheme = grapheme.cpu().argmax(dim=1).data.numpy()\n",
    "                vowel = vowel.cpu().argmax(dim=1).data.numpy()\n",
    "                cons = cons.cpu().argmax(dim=1).data.numpy()\n",
    "\n",
    "                val_true.append(targets.cpu().numpy())\n",
    "                val_pred.append(np.stack([grapheme, vowel, cons], axis=1))                \n",
    "\n",
    "        val_true = np.concatenate(val_true)\n",
    "        val_pred = np.concatenate(val_pred)\n",
    "        val_loss = np.mean(val_loss)\n",
    "        trn_loss = np.mean(trn_loss)\n",
    "\n",
    "        score_g = recall_score(val_true[:,0], val_pred[:,0], average='macro')\n",
    "        score_v = recall_score(val_true[:,1], val_pred[:,1], average='macro')\n",
    "        score_c = recall_score(val_true[:,2], val_pred[:,2], average='macro')\n",
    "        final_score = np.average([score_g, score_v, score_c], weights=[2,1,1])\n",
    "\n",
    "        # Printing vital information\n",
    "        s = f'[Epoch {epoch_id}] ' \\\n",
    "        f'trn_loss: {trn_loss:.4f}, vld_loss: {val_loss:.4f}, score: {final_score:.4f}, ' \\\n",
    "        f'score_each: [{score_g:.4f}, {score_v:.4f}, {score_c:.4f}]'          \n",
    "        print(s)\n",
    "\n",
    "        ################################################################################\n",
    "        # ==> Save checkpoint and training stats\n",
    "        ################################################################################        \n",
    "        if final_score > best_score:\n",
    "            best_score = final_score\n",
    "            state_dict = model.cpu().state_dict()\n",
    "            model = model.cuda()\n",
    "            torch.save(state_dict, os.path.join(args.model_dir, 'model.pth'))\n",
    "\n",
    "        # Record all statistics from this epoch\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_id + 1,\n",
    "                'trn_loss': trn_loss,\n",
    "                'trn_time': trn_time,            \n",
    "                'val_loss': val_loss,\n",
    "                'score': final_score,\n",
    "                'score_g': score_g,\n",
    "                'score_v': score_v,\n",
    "                'score_c': score_c            \n",
    "            }\n",
    "        )      \n",
    "        \n",
    "        # === Save Model Parameters ===\n",
    "        logger.info(\"Model successfully saved at: {}\".format(args.model_dir))            \n",
    "\n",
    "        \n",
    "def parser_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument('--num_epochs', type=int, default=1)\n",
    "    parser.add_argument('--num_folds', type=int, default=5)\n",
    "    parser.add_argument('--vld_fold_idx', type=int, default=4)\n",
    "    parser.add_argument('--batch_size', type=int, default=256)\n",
    "    parser.add_argument('--lr', type=float, default=0.001)\n",
    "    parser.add_argument('--log_interval', type=int, default=10) \n",
    "\n",
    "    # SageMaker Container environment\n",
    "    parser.add_argument('--train_dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n",
    "    parser.add_argument('--num_gpus', type=int, default=os.environ['SM_NUM_GPUS'])\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    #parser.add_argument('--model_output_dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "    args = parser.parse_args() \n",
    "    return args\n",
    "        \n",
    "    \n",
    "if __name__ =='__main__':\n",
    "\n",
    "    #parse arguments\n",
    "    args = parser_args() \n",
    "    args.use_cuda = args.num_gpus > 0\n",
    "    print(\"args.use_cuda : {} , args.num_gpus : {}\".format(\n",
    "        args.use_cuda, args.num_gpus))\n",
    "    args.device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")\n",
    "    train_model(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Training on SageMaker\n",
    "---\n",
    "\n",
    "훈련 스크립트가 준비되었다면 SageMaker 훈련을 수행하는 법은 매우 간단합니다. SageMaker Python SDK 활용 시, Estimator 인스턴스를 생성하고 해당 인스턴스의 `fit()` 메서드를 호출하는 것이 전부입니다.\n",
    "\n",
    "#### 1) `Estimator` 인스턴스 생성 \n",
    "훈련 컨테이너에 필요한 설정들을 지정합니다. 본 핸즈온에서는 훈련 스크립트 파일이 포함된 경로인 소스 경로와(`source_dir`)와 훈련 스크립트 Python 파일만 엔트리포인트(`entry_point`)로 지정해 주면 됩니다.\n",
    "\n",
    "#### 2) `fit()` 메서드 호출\n",
    "`estimator.fit(YOUR_TRAINING_DATA_URI)` 메서드를 호출하면, 훈련에 필요한 인스턴스를 시작하고 컨테이너 환경을 시작합니다. \n",
    "필수 인자값은 훈련 데이터가 존해자는 S3 경로(`s3://`)이며, 로컬 모드로 훈련 시에는 로컬 경로(`file://`)를 지정하시면 됩니다. \n",
    "\n",
    "인자값 중 wait은 디폴트 값으로 `wait=True`이며, 모든 훈련 작업이 완료될 때까지 코드 셀이 freezing됩니다. 만약 다른 코드 셀을 실행하거나, 다른 훈련 job을 시작하고 싶다면 `wait=False`로 설정하여 Asynchronous 모드로 변경하면 됩니다.\n",
    "\n",
    "SageMaker 훈련이 끝나면 컨테이너 환경과 훈련 인스턴스는 자동으로 삭제됩니다. 이 때, SageMaker는 자동으로 `SM_MODEL_DIR` 경로에 저장된 최종 모델 아티팩트를 `model.tar.gz`로 압축하여 훈련 컨테이너 환경에서 S3 bucket으로 저장합니다. 당연히, S3 bucket에 저장된 모델 아티팩트를 다운로드받아 로컬 상에서 곧바로 테스트할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = 'bangali/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(entry_point='train.py',\n",
    "                    source_dir='src',\n",
    "                    role=role,\n",
    "                    instance_type='ml.p3.2xlarge',\n",
    "                    instance_count=1,\n",
    "                    framework_version='1.6.0',\n",
    "                    py_version='py3',\n",
    "                    hyperparameters = {'num_epochs': 1, \n",
    "                                       'num_folds': 5,\n",
    "                                       'vld_fold_idx': 4,\n",
    "                                       'batch_size': 256,\n",
    "                                       'lr': 0.001,\n",
    "                                       'log_interval': 10,\n",
    "                                      }                       \n",
    "                   )\n",
    "s3_input_train = sagemaker.TrainingInput(s3_data='s3://{}/{}'.format(bucket, prefix), content_type='csv')    \n",
    "#s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}'.format(bucket, prefix), content_type='csv') # SDK v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-22 03:43:18 Starting - Starting the training job...\n",
      "2021-04-22 03:43:42 Starting - Launching requested ML instancesProfilerReport-1619062998: InProgress\n",
      "......\n",
      "2021-04-22 03:44:42 Starting - Preparing the instances for training............\n",
      "2021-04-22 03:46:43 Downloading - Downloading input data...\n",
      "2021-04-22 03:47:17 Training - Downloading the training image......\n",
      "2021-04-22 03:48:11 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-04-22 03:48:12,114 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-04-22 03:48:12,140 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-04-22 03:48:12,759 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-04-22 03:48:26,813 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting albumentations\n",
      "  Downloading albumentations-0.5.2-py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow\n",
      "  Downloading pyarrow-3.0.0-cp36-cp36m-manylinux2014_x86_64.whl (20.7 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML in /opt/conda/lib/python3.6/site-packages (from albumentations->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from albumentations->-r requirements.txt (line 1)) (1.5.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.11.1 in /opt/conda/lib/python3.6/site-packages (from albumentations->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mCollecting opencv-python-headless>=4.1.1\n",
      "  Downloading opencv_python_headless-4.5.1.48-cp36-cp36m-manylinux2014_x86_64.whl (37.6 MB)\u001b[0m\n",
      "\u001b[34mCollecting scikit-image>=0.16.1\n",
      "  Downloading scikit_image-0.17.2-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\u001b[0m\n",
      "\u001b[34mCollecting imgaug>=0.4.0\n",
      "  Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\u001b[0m\n",
      "\u001b[34mCollecting imageio\n",
      "  Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mCollecting opencv-python\n",
      "  Downloading opencv_python-4.5.1.48-cp36-cp36m-manylinux2014_x86_64.whl (50.4 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (3.3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /opt/conda/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (8.2.0)\u001b[0m\n",
      "\u001b[34mCollecting Shapely\n",
      "  Downloading Shapely-1.7.1-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34mCollecting tifffile>=2019.7.26\n",
      "  Downloading tifffile-2020.9.3-py3-none-any.whl (148 kB)\u001b[0m\n",
      "\u001b[34mCollecting PyWavelets>=1.1.1\n",
      "  Downloading PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 1)) (2.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: decorator<5,>=4.3 in /opt/conda/lib/python3.6/site-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations->-r requirements.txt (line 1)) (4.4.2)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tifffile, PyWavelets, imageio, Shapely, scikit-image, opencv-python, opencv-python-headless, imgaug, pyarrow, albumentations\u001b[0m\n",
      "\u001b[34mSuccessfully installed PyWavelets-1.1.1 Shapely-1.7.1 albumentations-0.5.2 imageio-2.9.0 imgaug-0.4.0 opencv-python-4.5.1.48 opencv-python-headless-4.5.1.48 pyarrow-3.0.0 scikit-image-0.17.2 tifffile-2020.9.3\u001b[0m\n",
      "\u001b[34m2021-04-22 03:48:38,118 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 256,\n",
      "        \"lr\": 0.001,\n",
      "        \"vld_fold_idx\": 4,\n",
      "        \"log_interval\": 10,\n",
      "        \"num_epochs\": 1,\n",
      "        \"num_folds\": 5\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"ContentType\": \"csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2021-04-22-03-43-18-411\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-143656149352/pytorch-training-2021-04-22-03-43-18-411/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":256,\"log_interval\":10,\"lr\":0.001,\"num_epochs\":1,\"num_folds\":5,\"vld_fold_idx\":4}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-143656149352/pytorch-training-2021-04-22-03-43-18-411/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":256,\"log_interval\":10,\"lr\":0.001,\"num_epochs\":1,\"num_folds\":5,\"vld_fold_idx\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2021-04-22-03-43-18-411\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-143656149352/pytorch-training-2021-04-22-03-43-18-411/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"256\",\"--log_interval\",\"10\",\"--lr\",\"0.001\",\"--num_epochs\",\"1\",\"--num_folds\",\"5\",\"--vld_fold_idx\",\"4\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=256\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.001\u001b[0m\n",
      "\u001b[34mSM_HP_VLD_FOLD_IDX=4\u001b[0m\n",
      "\u001b[34mSM_HP_LOG_INTERVAL=10\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_FOLDS=5\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train.py --batch_size 256 --log_interval 10 --lr 0.001 --num_epochs 1 --num_folds 5 --vld_fold_idx 4\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34margs.use_cuda : True , args.num_gpus : 1\u001b[0m\n",
      "\u001b[34m=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m=== Getting Images ===\u001b[0m\n",
      "\u001b[34m['/opt/ml/input/data/training/train_image_data_0.feather', '/opt/ml/input/data/training/train_image_data_1.feather', '/opt/ml/input/data/training/train_image_data_2.feather', '/opt/ml/input/data/training/train_image_data_3.feather']\u001b[0m\n",
      "\u001b[34m=== Getting Pre-trained model ===\u001b[0m\n",
      "\u001b[34m=== Start Training ===\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.126 algo-1:31 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.608 algo-1:31 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.609 algo-1:31 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.610 algo-1:31 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.613 algo-1:31 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.613 algo-1:31 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.616 algo-1:31 INFO hook.py:584] name:conv1.weight count_params:9408\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.616 algo-1:31 INFO hook.py:584] name:bn1.weight count_params:64\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.616 algo-1:31 INFO hook.py:584] name:bn1.bias count_params:64\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.616 algo-1:31 INFO hook.py:584] name:layer1.0.conv1.weight count_params:36864\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.616 algo-1:31 INFO hook.py:584] name:layer1.0.bn1.weight count_params:64\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.616 algo-1:31 INFO hook.py:584] name:layer1.0.bn1.bias count_params:64\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.616 algo-1:31 INFO hook.py:584] name:layer1.0.conv2.weight count_params:36864\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.616 algo-1:31 INFO hook.py:584] name:layer1.0.bn2.weight count_params:64\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.617 algo-1:31 INFO hook.py:584] name:layer1.0.bn2.bias count_params:64\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.617 algo-1:31 INFO hook.py:584] name:layer1.1.conv1.weight count_params:36864\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.617 algo-1:31 INFO hook.py:584] name:layer1.1.bn1.weight count_params:64\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.617 algo-1:31 INFO hook.py:584] name:layer1.1.bn1.bias count_params:64\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.617 algo-1:31 INFO hook.py:584] name:layer1.1.conv2.weight count_params:36864\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.617 algo-1:31 INFO hook.py:584] name:layer1.1.bn2.weight count_params:64\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.617 algo-1:31 INFO hook.py:584] name:layer1.1.bn2.bias count_params:64\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.617 algo-1:31 INFO hook.py:584] name:layer2.0.conv1.weight count_params:73728\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.617 algo-1:31 INFO hook.py:584] name:layer2.0.bn1.weight count_params:128\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.617 algo-1:31 INFO hook.py:584] name:layer2.0.bn1.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.617 algo-1:31 INFO hook.py:584] name:layer2.0.conv2.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.618 algo-1:31 INFO hook.py:584] name:layer2.0.bn2.weight count_params:128\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.618 algo-1:31 INFO hook.py:584] name:layer2.0.bn2.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.618 algo-1:31 INFO hook.py:584] name:layer2.0.downsample.0.weight count_params:8192\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.618 algo-1:31 INFO hook.py:584] name:layer2.0.downsample.1.weight count_params:128\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.618 algo-1:31 INFO hook.py:584] name:layer2.0.downsample.1.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.622 algo-1:31 INFO hook.py:584] name:layer2.1.conv1.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.622 algo-1:31 INFO hook.py:584] name:layer2.1.bn1.weight count_params:128\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.622 algo-1:31 INFO hook.py:584] name:layer2.1.bn1.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.623 algo-1:31 INFO hook.py:584] name:layer2.1.conv2.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.623 algo-1:31 INFO hook.py:584] name:layer2.1.bn2.weight count_params:128\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.623 algo-1:31 INFO hook.py:584] name:layer2.1.bn2.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.623 algo-1:31 INFO hook.py:584] name:layer3.0.conv1.weight count_params:294912\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.623 algo-1:31 INFO hook.py:584] name:layer3.0.bn1.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.623 algo-1:31 INFO hook.py:584] name:layer3.0.bn1.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.624 algo-1:31 INFO hook.py:584] name:layer3.0.conv2.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.624 algo-1:31 INFO hook.py:584] name:layer3.0.bn2.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.624 algo-1:31 INFO hook.py:584] name:layer3.0.bn2.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.624 algo-1:31 INFO hook.py:584] name:layer3.0.downsample.0.weight count_params:32768\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.624 algo-1:31 INFO hook.py:584] name:layer3.0.downsample.1.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.625 algo-1:31 INFO hook.py:584] name:layer3.0.downsample.1.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.625 algo-1:31 INFO hook.py:584] name:layer3.1.conv1.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.625 algo-1:31 INFO hook.py:584] name:layer3.1.bn1.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.625 algo-1:31 INFO hook.py:584] name:layer3.1.bn1.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.625 algo-1:31 INFO hook.py:584] name:layer3.1.conv2.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.625 algo-1:31 INFO hook.py:584] name:layer3.1.bn2.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.626 algo-1:31 INFO hook.py:584] name:layer3.1.bn2.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.626 algo-1:31 INFO hook.py:584] name:layer4.0.conv1.weight count_params:1179648\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.626 algo-1:31 INFO hook.py:584] name:layer4.0.bn1.weight count_params:512\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.626 algo-1:31 INFO hook.py:584] name:layer4.0.bn1.bias count_params:512\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.626 algo-1:31 INFO hook.py:584] name:layer4.0.conv2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.626 algo-1:31 INFO hook.py:584] name:layer4.0.bn2.weight count_params:512\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.627 algo-1:31 INFO hook.py:584] name:layer4.0.bn2.bias count_params:512\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.627 algo-1:31 INFO hook.py:584] name:layer4.0.downsample.0.weight count_params:131072\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.627 algo-1:31 INFO hook.py:584] name:layer4.0.downsample.1.weight count_params:512\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.627 algo-1:31 INFO hook.py:584] name:layer4.0.downsample.1.bias count_params:512\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.627 algo-1:31 INFO hook.py:584] name:layer4.1.conv1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.627 algo-1:31 INFO hook.py:584] name:layer4.1.bn1.weight count_params:512\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.628 algo-1:31 INFO hook.py:584] name:layer4.1.bn1.bias count_params:512\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.628 algo-1:31 INFO hook.py:584] name:layer4.1.conv2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.628 algo-1:31 INFO hook.py:584] name:layer4.1.bn2.weight count_params:512\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.628 algo-1:31 INFO hook.py:584] name:layer4.1.bn2.bias count_params:512\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.628 algo-1:31 INFO hook.py:584] name:fc.weight count_params:95232\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.628 algo-1:31 INFO hook.py:584] name:fc.bias count_params:186\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.629 algo-1:31 INFO hook.py:586] Total Trainable Params: 11271930\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.629 algo-1:31 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-04-22 03:49:38.641 algo-1:31 INFO hook.py:476] Hook is writing from the hook with pid: 31\n",
      "\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 10/628] loss: 3.0354\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 20/628] loss: 2.6982\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 30/628] loss: 2.4928\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 40/628] loss: 2.2552\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 50/628] loss: 2.1836\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 60/628] loss: 1.9069\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 70/628] loss: 2.0936\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 80/628] loss: 2.0675\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 90/628] loss: 1.7919\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 100/628] loss: 1.9742\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 110/628] loss: 1.5190\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 120/628] loss: 1.8064\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 130/628] loss: 1.6629\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 140/628] loss: 1.5169\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 150/628] loss: 1.6914\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 160/628] loss: 1.3886\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 170/628] loss: 1.5181\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 180/628] loss: 1.6883\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 190/628] loss: 1.8913\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 200/628] loss: 1.6006\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 210/628] loss: 1.8376\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 220/628] loss: 1.5788\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 230/628] loss: 1.7885\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 240/628] loss: 1.4884\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 250/628] loss: 0.9764\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 260/628] loss: 1.6473\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 270/628] loss: 1.5820\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 280/628] loss: 1.4164\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 290/628] loss: 1.4192\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 300/628] loss: 1.0427\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 310/628] loss: 1.5430\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 320/628] loss: 1.2114\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 330/628] loss: 1.0795\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 340/628] loss: 1.1161\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 350/628] loss: 1.5142\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 360/628] loss: 1.0963\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 370/628] loss: 1.1676\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 380/628] loss: 1.7588\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 390/628] loss: 1.2678\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 400/628] loss: 1.1509\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 410/628] loss: 1.3240\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 420/628] loss: 1.8263\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 430/628] loss: 1.2878\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 440/628] loss: 0.8898\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 450/628] loss: 1.6500\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 460/628] loss: 1.2304\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 470/628] loss: 1.3126\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 480/628] loss: 1.0860\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 490/628] loss: 1.3310\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 500/628] loss: 1.8071\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 510/628] loss: 1.3056\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 520/628] loss: 1.1783\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 530/628] loss: 1.4627\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 540/628] loss: 1.4185\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 550/628] loss: 1.1659\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 560/628] loss: 0.9253\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 570/628] loss: 0.6738\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 580/628] loss: 1.2531\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 590/628] loss: 0.4337\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 600/628] loss: 1.4695\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 610/628] loss: 1.2436\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 620/628] loss: 1.1336\u001b[0m\n",
      "\n",
      "2021-04-22 04:01:54 Uploading - Uploading generated training model\n",
      "2021-04-22 04:01:54 Completed - Training job completed\n",
      "\u001b[34m=== Start Validation ===\u001b[0m\n",
      "Training seconds: 927\n",
      "Billable seconds: 927\n",
      "CPU times: user 3.74 s, sys: 272 ms, total: 4.02 s\n",
      "Wall time: 18min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator.fit(s3_input_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 아티팩트 경로를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-143656149352/pytorch-training-2021-04-22-03-43-18-411/output/\n",
      "2021-04-22 04:01:49   41942385 model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_model_dir = estimator.model_data.replace('model.tar.gz', '')\n",
    "print(s3_model_dir)\n",
    "!aws s3 ls {s3_model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Getting Model artifacts\n",
    "---\n",
    "\n",
    "훈련이 완료된 모델 아티팩트를 로컬(jupyter notebook 인스턴스 or 온프레미스)로 복사합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_dir = './model'\n",
    "!rm -rf $local_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-143656149352/pytorch-training-2021-04-22-03-43-18-411/output/model.tar.gz to model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "\n",
    "if not os.path.exists(local_model_dir):\n",
    "    os.makedirs(local_model_dir)\n",
    "\n",
    "!aws s3 cp {s3_model_dir}model.tar.gz {local_model_dir}/model.tar.gz\n",
    "!tar -xzf {local_model_dir}/model.tar.gz -C {local_model_dir}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
