{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3. Training on Amazon SageMaker using SageMaker Data Parallelism Library\n",
    "---\n",
    "\n",
    "***[주의] 본 세션은 최소 1개 이상의 `ml.p3.16xlarge` 훈련 인스턴스가 필요합니다. 이벤트 엔진으로 핸즈온 진행 시는 반드시 훈련 인스턴스의 리소스 가용 여부를 확인해 주세요.***\n",
    "\n",
    "본 모듈에서는 re:Invent 2020에서 새로 추가된 SageMaker Data Parallelism Library를 사용하여 분산 훈련을 수행합니다.\n",
    "분산 훈련 및 디버깅에 대한 세션을 아래 동영상에서 보실 수 있습니다.\n",
    "\n",
    "- [Amazon SageMaker를 통한 딥러닝 분산 학습 및 디버거 프로파일링 활용하기](https://www.youtube.com/watch?v=lsTtoACAPj4)\n",
    "\n",
    "## Amazon SageMaker Distributed Training\n",
    "Amazon SageMaker에서는 2가지 분산 학습 알고리즘을 AWS 분산 학습 클러스터에 특화하여 제공하고 있습니다. \n",
    "SageMaker Data Parallelism library는 GPU 간 또는 여러 인스턴스 간의 네트워크를 통해 주고 받는 모델 파라메터의 병목을 개선합니다. 이를 위해, backward pass에서의 최적화된 AllReduce 오퍼레이터 활용 방법 등의 다양한 기법을 적용하였고, 특히 Balanced Fusion Buffer로 Elastic Fabric Adapter의 성능을 최대한 발휘할 수 있도록 하였습니다. \n",
    "SageMaker Model Parallelism library는 모델 구조를 분석하여 방향성 비사이클 그래프로 구성한 다음, subgraph형태로 분리하여 각 GPU 노드에서 학습을 진행하는 방식입니다. 따라서, 단일 GPU 노드에서는 일부 모델 파라미터에 대해서만 학습이 일어나므로 GPU 메모리의 부담이 줄어듭니다.\n",
    "\n",
    "### Traditional Data Parallelism\n",
    "![smdataparallel1](imgs/smdataparallel1.jpg)\n",
    "**[그림 1]** Parameter Server and Ring All-reduce\n",
    "\n",
    "딥러닝 훈련은 매 스텝(step)마다 gradient를 계산해서 모델의 파라메터를 반복적으로 업데이트하는 과정입니다. Data parallel 환경에서는 여러 GPU에서 동일한 모델 파라미터와 다른 입력값으로 각 GPU마다 다른 gradient를 계산하고, 이를 다 합산한 값을 가지고 각 GPU의 모델 파라메터를 업데이트합니다.\n",
    "대표적인 방식으로, 파라메터 서버(Parameter Server)와 Horovod에서 쓰이는 Ring All-reduce 방식이 있습니다.\n",
    "\n",
    "파라메터 서버는 각 GPU에서 계산된 gradient를 파라메터 서버에서 취합 후 모델 파라메터를 관리하는 방식입니다. 이 방식의 문제는 특정 파라메터가 몰리는 GPU 쪽 트래픽에서 병목이 발생합니다. \n",
    "따라서, 모든 GPU가 고르게 데이터를 주고 받기 위해 별도의 파라메터 서버를 사용하는 대신 Ring All-reduce 방식도 많이 사용합니다. 특정 GPU에 트래픽이 몰리는 현상을 개선할 수 있지만, 이 또한 communication cost가 많이 소요됩니다. **[그림 1]** 의 오른쪽과 같이 현 GPU의 gradient를 다음 GPU로 패싱하면서 한 바퀴를 돌고 gradient들을 누적 합산하여 평균 gradient를 계산해야 합니다. 그런 다음, 모델 파라메터 업데이트를 위해 다시 한 바퀴를 돌아야 하기 때문에 두 바퀴를 돌게 됩니다.\n",
    "\n",
    "### Balanced Fusion Buffers (BFB)\n",
    "\n",
    "SageMaker Data Parallelism Library은 파라메터 서버를 그대로 이용하면서도 모든 GPU를 고르게 활용하는데, 여기에서 핵심이 되는 컨셉이 바로 Balanced Fusion Buffer입니다. \n",
    "\n",
    "쉬운 예시로, 4장의 GPU가 있고 2개의 파라메터가 있다고 가정하겠습니다.파라메터 Wh1 용량이 500메가이고 Wh2 용량이 100메가라면 기존 파라메터 서버에서는 2장의 GPU만 활용하고 나머지 2장의 GPU는 놀고 있습니다. 게다가 첫번째 GPU에서 500메가 파라메터를 계산하느라 추가적인 병목이 발생합니다.\n",
    "\n",
    "Balanced Fusion Buffer는 각 GPU마다 동일한 버퍼 사이즈를 할당합니다. 각 GPU에서 동일하게 150메가 파라메터를 처리하므로 모든 GPU 리소스를 사용할 수 있습니다. 구체적으로는 Wh1을 4개로 분할하여 각 GPU에 할당하고, 마지막 GPU에서는 150메가 버퍼 중에서 50메가는 Wh1 파라메터를 처리하고, 100메가는 Wh2 파라메터를 처리합니다. \n",
    "\n",
    "\n",
    "또한, 기존 방법은 어떤 gradient가 준비되었는지 확인하기 위한 별도의 협상 과정인 negotiation이 필요한데, Balanced Fusion Buffer에서는 순서를 기억하고 있다가 곧바로 매핑해 주기 때문에 negotiation에 들어가는 추가 지연이 없습니다.\n",
    "\n",
    "예를 들어, 버퍼 사이즈가 100MB이고 5 3 4 2 1 순서대로 들어오는 gradient 용량은 각각 50MB, 50MB, 250MB, 75MB, 100MB 라고 가정하겠습니다. \n",
    "0번 GPU는 5와 3이 0번 버퍼로 들어갑니다. 그리고 4는 용량이 크기 때문에 3개의 버퍼에 분산되어 들어갑니다. 2는 3번 버퍼에서 남는 공간과 그 다음 버퍼의 앞에 들어가고요 (클릭) 1도 마찬가지로 4번 버퍼의 남는 공간과 마지막 버퍼의 앞에 들어갑니다.\n",
    "\n",
    "그런데, 1번 GPU에서 순서가 꼬여서 5 4 3 2 1 이 들어오면 어떻게 될까요? 3이 4 뒤에 있다 하더라도 Balanced Fusion Buffer에 매핑되는 순서는 변함이 없기 때문에, 버퍼 내 순서가 그대로 보장됩니다. **[그림 2]** 에서 4보다 3이 먼저 들어가는 것을 확인할 수 있습니다.\n",
    "\n",
    "![smdataparallel2](imgs/smdataparallel2.jpg)\n",
    "**[그림 2]** Balanced Fusion Buffer without negotiation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Training script\n",
    "---\n",
    "\n",
    "아래 코드 셀은 `src` 디렉토리에 SageMaker 훈련 스크립트인 `train_smdataparallel.py`를 저장합니다.<br>\n",
    "훈련에 필요한 코드는 몇 줄의 코드만 수정하시면 되며, 자세한 내용을 개발자 문서에서 확인할 수 있습니다.\n",
    "\n",
    "#### Code Snippet: PyTorch Training Script \n",
    "```python\n",
    "import smdistributed.dataparallel.torch.distributed as dist\n",
    "from smdistributed.dataparallel.torch.parallel.distributed import DistributedDataParallel as DDP\n",
    "\n",
    "# Scale Parameters\n",
    "dist.init_process_group()\n",
    "batch_size //= dist.get_world_size() // 8\n",
    "batch_size = max(batch_size, 1)\n",
    "\n",
    "# Set rank in DistributedSampler\n",
    "train_sampler = DistributedSampler(train_dataset, num_replicas=dist.get_world_size(), rank=dist.get_rank())\n",
    "train_loader = torch.utils.data.DataLoader(..)\n",
    "\n",
    "# Pin each GPU\n",
    "model = DDP(model.to(device))\n",
    "torch.cuda.set_device(dist.get_local_rank())\n",
    "model.cuda(dist.get_local_rank())\n",
    "\n",
    "# Checkpoint on master node\n",
    "if dist.get_rank() == 0:\n",
    "    torch.save(checkpoint_dir)\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/train_smdataparallel.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/train_smdataparallel.py\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import sys\n",
    "import time, datetime\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import recall_score\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Import SMDataParallel PyTorch Modules\n",
    "from smdistributed.dataparallel.torch.parallel.distributed import DistributedDataParallel as DDP\n",
    "import smdistributed.dataparallel.torch.distributed as dist\n",
    "dist.init_process_group()\n",
    "\n",
    "\n",
    "HEIGHT = 137\n",
    "WIDTH = 236\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "\n",
    "class BangaliDataset(Dataset):\n",
    "    def __init__(self, imgs, label_df=None, transform=None):\n",
    "        self.imgs = imgs\n",
    "        self.label_df = label_df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_idx = self.label_df.iloc[idx].id\n",
    "        img = (self.imgs[img_idx]).astype(np.uint8)\n",
    "        img = 255 - img\n",
    "    \n",
    "        img = img[:,:,np.newaxis]\n",
    "        img = np.repeat(img, 3, axis=2)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(image=img)['image']        \n",
    "        \n",
    "        if self.label_df is not None:\n",
    "            label_1 = self.label_df.iloc[idx].grapheme_root\n",
    "            label_2 = self.label_df.iloc[idx].vowel_diacritic\n",
    "            label_3 = self.label_df.iloc[idx].consonant_diacritic           \n",
    "            return img, np.array([label_1, label_2, label_3])        \n",
    "        else:\n",
    "            return img\n",
    "        \n",
    "        \n",
    "def _set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    mx.random.seed(seed)\n",
    "\n",
    "def _get_images(train_dir, num_folds=5, vld_fold_idx=4, data_type='train'):\n",
    "\n",
    "    logger.info(\"=== Getting Labels ===\")\n",
    "    logger.info(train_dir)\n",
    "    \n",
    "    label_df = pd.read_csv(os.path.join(train_dir, 'train_folds.csv'))\n",
    "    #label_df = pd.read_csv(f'{train_dir}/train_folds.csv')\n",
    "     \n",
    "    trn_fold = [i for i in range(num_folds) if i not in [vld_fold_idx]]\n",
    "    vld_fold = [vld_fold_idx]\n",
    "\n",
    "    trn_idx = label_df.loc[label_df['fold'].isin(trn_fold)].index\n",
    "    vld_idx = label_df.loc[label_df['fold'].isin(vld_fold)].index\n",
    "\n",
    "    logger.info(\"=== Getting Images ===\")    \n",
    "    files = [f'{train_dir}/{data_type}_image_data_{i}.feather' for i in range(4)]\n",
    "    logger.info(files)\n",
    "    \n",
    "    image_df_list = [pd.read_feather(f) for f in files]\n",
    "    imgs = [df.iloc[:, 1:].values.reshape(-1, HEIGHT, WIDTH) for df in image_df_list]\n",
    "    del image_df_list\n",
    "    gc.collect()\n",
    "    imgs = np.concatenate(imgs, axis=0)\n",
    "    \n",
    "    trn_df = label_df.loc[trn_idx]\n",
    "    vld_df = label_df.loc[vld_idx]\n",
    "    \n",
    "    return imgs, trn_df, vld_df       \n",
    "                           \n",
    "                           \n",
    "def _get_data_loader(imgs, trn_df, vld_df):\n",
    "\n",
    "    import albumentations as A\n",
    "    from albumentations import (\n",
    "        Rotate,HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "        Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "        IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, RandomBrightnessContrast, IAAPiecewiseAffine,\n",
    "        IAASharpen, IAAEmboss, Flip, OneOf, Compose\n",
    "    )\n",
    "    from albumentations.pytorch import ToTensor, ToTensorV2\n",
    "\n",
    "    train_transforms = A.Compose([\n",
    "        Rotate(20),\n",
    "            OneOf([\n",
    "                IAAAdditiveGaussianNoise(),\n",
    "                GaussNoise(),\n",
    "            ], p=0.2),\n",
    "            OneOf([\n",
    "                MotionBlur(p=.2),\n",
    "                MedianBlur(blur_limit=3, p=0.1),\n",
    "                Blur(blur_limit=3, p=0.1),\n",
    "            ], p=0.2),\n",
    "            ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n",
    "            OneOf([\n",
    "                OpticalDistortion(p=0.3),\n",
    "                GridDistortion(p=.1),\n",
    "                IAAPiecewiseAffine(p=0.3),\n",
    "            ], p=0.2),\n",
    "            OneOf([\n",
    "                CLAHE(clip_limit=2),\n",
    "                IAASharpen(),\n",
    "                IAAEmboss(),\n",
    "                RandomBrightnessContrast(),            \n",
    "            ], p=0.3),\n",
    "            HueSaturationValue(p=0.3),\n",
    "        ToTensor()\n",
    "        ], p=1.0)\n",
    "\n",
    "\n",
    "    valid_transforms = A.Compose([\n",
    "        ToTensor()\n",
    "    ])\n",
    "\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    trn_dataset = BangaliDataset(imgs=imgs, label_df=trn_df, transform=train_transforms)\n",
    "    vld_dataset = BangaliDataset(imgs=imgs, label_df=vld_df, transform=valid_transforms)\n",
    "    \n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    \n",
    "    trn_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        trn_dataset, \n",
    "        num_replicas=world_size, # worldsize만큼 분할\n",
    "        rank=rank)\n",
    "    \n",
    "    trn_loader = DataLoader(trn_dataset, \n",
    "                            shuffle=False, \n",
    "                            num_workers=8,\n",
    "                            pin_memory=True,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                           sampler=trn_sampler)  \n",
    "    \n",
    "    vld_loader = DataLoader(vld_dataset, shuffle=False, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE)  \n",
    "    return trn_loader, vld_loader\n",
    "\n",
    "\n",
    "def _rand_bbox(size, lam):\n",
    "    '''\n",
    "    CutMix Helper function.\n",
    "    Retrieved from https://github.com/clovaai/CutMix-PyTorch/blob/master/train.py\n",
    "    '''\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    # 폭과 높이는 주어진 이미지의 폭과 높이의 beta distribution에서 뽑은 lambda로 얻는다\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    \n",
    "    # patch size 의 w, h 는 original image 의 w,h 에 np.sqrt(1-lambda) 를 곱해준 값입니다.\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # patch의 중심점은 uniform하게 뽑힘\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "\n",
    "def _format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "            \n",
    "def train_model(args):\n",
    "    from torchvision import datasets, models\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    imgs, trn_df, vld_df = _get_images(args.train_dir, args.num_folds, args.vld_fold_idx, data_type='train')\n",
    "    trn_loader, vld_loader = _get_data_loader(imgs, trn_df, vld_df)\n",
    "\n",
    "    \n",
    "    logger.info(\"=== Getting Pre-trained model ===\")    \n",
    "    model = models.resnet18(pretrained=True)\n",
    "    last_hidden_units = model.fc.in_features\n",
    "    model.fc = torch.nn.Linear(last_hidden_units, 186)\n",
    "#     len_buffer =  len(list(module.buffers()))\n",
    "\n",
    "#     logger.info(\"=== Buffer ===\")    \n",
    "#     print(f\"len_buffer={len_buffer}\")\n",
    "#     print(list(model.buffers()))\n",
    "    \n",
    "    # SDP: Pin each GPU to a single process\n",
    "    # Use SMDataParallel PyTorch DDP for efficient distributed training\n",
    "    model = DDP(model.to(args.device), broadcast_buffers=False)\n",
    "\n",
    "    # SDP: Pin each GPU to a single SDP process.\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    model.cuda(args.local_rank)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max',\n",
    "                                                          verbose=True, patience=5, \n",
    "                                                          factor=0.5)\n",
    "\n",
    "    best_score = -1\n",
    "    training_stats = []\n",
    "    logger.info(\"=== Start Training ===\")    \n",
    "\n",
    "    for epoch_id in range(args.num_epochs):\n",
    "\n",
    "        ################################################################################\n",
    "        # ==> Training phase\n",
    "        ################################################################################    \n",
    "        trn_loss = []\n",
    "        model.train()\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for batch_id, (inputs, targets) in enumerate((trn_loader)):\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            targets_gra = targets[:, 0]\n",
    "            targets_vow = targets[:, 1]\n",
    "            targets_con = targets[:, 2]\n",
    "\n",
    "            # 50%의 확률로 원본 데이터 그대로 사용    \n",
    "            if np.random.rand() < 0.5:\n",
    "                logits = model(inputs)\n",
    "                grapheme = logits[:, :168]\n",
    "                vowel = logits[:, 168:179]\n",
    "                cons = logits[:, 179:]\n",
    "\n",
    "                loss1 = loss_fn(grapheme, targets_gra)\n",
    "                loss2 = loss_fn(vowel, targets_vow)\n",
    "                loss3 = loss_fn(cons, targets_con) \n",
    "\n",
    "            else:\n",
    "\n",
    "                lam = np.random.beta(1.0, 1.0) \n",
    "                rand_index = torch.randperm(inputs.size()[0])\n",
    "                shuffled_targets_gra = targets_gra[rand_index]\n",
    "                shuffled_targets_vow = targets_vow[rand_index]\n",
    "                shuffled_targets_con = targets_con[rand_index]\n",
    "\n",
    "                bbx1, bby1, bbx2, bby2 = _rand_bbox(inputs.size(), lam)\n",
    "                inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "                # 픽셀 비율과 정확히 일치하도록 lambda 파라메터 조정  \n",
    "                lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))\n",
    "\n",
    "                logits = model(inputs)\n",
    "                grapheme = logits[:,:168]\n",
    "                vowel = logits[:, 168:179]\n",
    "                cons = logits[:, 179:]\n",
    "\n",
    "                loss1 = loss_fn(grapheme, targets_gra) * lam + loss_fn(grapheme, shuffled_targets_gra) * (1. - lam)\n",
    "                loss2 = loss_fn(vowel, targets_vow) * lam + loss_fn(vowel, shuffled_targets_vow) * (1. - lam)\n",
    "                loss3 = loss_fn(cons, targets_con) * lam + loss_fn(cons, shuffled_targets_con) * (1. - lam)\n",
    "\n",
    "            loss = 0.5 * loss1 + 0.25 * loss2 + 0.25 * loss3    \n",
    "            trn_loss.append(loss.item())\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Printing vital information\n",
    "            if (batch_id + 1) % (args.log_interval) == 0:\n",
    "                s = f'[Epoch {epoch_id} Batch {batch_id+1}/{len(trn_loader)}] ' \\\n",
    "                f'loss: {running_loss / args.log_interval:.4f}'\n",
    "                print(s)\n",
    "                running_loss = 0\n",
    "\n",
    "        # Measure how long this epoch took.\n",
    "        trn_time = _format_time(time.time() - t0)        \n",
    "\n",
    "\n",
    "        if args.rank == 0:    \n",
    "            ################################################################################\n",
    "            # ==> Validation phase\n",
    "            ################################################################################\n",
    "            val_loss = []\n",
    "            val_true = []\n",
    "            val_pred = []\n",
    "            model.eval()\n",
    "\n",
    "            # === Validation phase ===\n",
    "            logger.info('=== Start Validation ===')        \n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in vld_loader:\n",
    "                    inputs = inputs.cuda()\n",
    "                    targets = targets.cuda()\n",
    "                    logits = model(inputs)\n",
    "                    grapheme = logits[:,:168]\n",
    "                    vowel = logits[:, 168:179]\n",
    "                    cons = logits[:, 179:]\n",
    "\n",
    "                    loss= 0.5* loss_fn(grapheme, targets[:,0]) + 0.25*loss_fn(vowel, targets[:,1]) + \\\n",
    "                    0.25*loss_fn(vowel, targets[:,2])\n",
    "                    val_loss.append(loss.item())\n",
    "\n",
    "                    grapheme = grapheme.cpu().argmax(dim=1).data.numpy()\n",
    "                    vowel = vowel.cpu().argmax(dim=1).data.numpy()\n",
    "                    cons = cons.cpu().argmax(dim=1).data.numpy()\n",
    "\n",
    "                    val_true.append(targets.cpu().numpy())\n",
    "                    val_pred.append(np.stack([grapheme, vowel, cons], axis=1))                \n",
    "\n",
    "            val_true = np.concatenate(val_true)\n",
    "            val_pred = np.concatenate(val_pred)\n",
    "            val_loss = np.mean(val_loss)\n",
    "            trn_loss = np.mean(trn_loss)\n",
    "\n",
    "            score_g = recall_score(val_true[:,0], val_pred[:,0], average='macro')\n",
    "            score_v = recall_score(val_true[:,1], val_pred[:,1], average='macro')\n",
    "            score_c = recall_score(val_true[:,2], val_pred[:,2], average='macro')\n",
    "            final_score = np.average([score_g, score_v, score_c], weights=[2,1,1])\n",
    "\n",
    "            # Printing vital information\n",
    "            s = f'[Epoch {epoch_id}] ' \\\n",
    "            f'trn_loss: {trn_loss:.4f}, vld_loss: {val_loss:.4f}, score: {final_score:.4f}, ' \\\n",
    "            f'score_each: [{score_g:.4f}, {score_v:.4f}, {score_c:.4f}]'          \n",
    "            print(s)\n",
    "\n",
    "            ################################################################################\n",
    "            # ==> Save checkpoint and training stats\n",
    "            ################################################################################        \n",
    "            if final_score > best_score:\n",
    "                best_score = final_score\n",
    "                state_dict = model.cpu().state_dict()\n",
    "                model = model.cuda()\n",
    "                torch.save(state_dict, os.path.join(args.model_dir, 'model.pt'))\n",
    "\n",
    "            # Record all statistics from this epoch\n",
    "            training_stats.append(\n",
    "                {\n",
    "                    'epoch': epoch_id + 1,\n",
    "                    'trn_loss': trn_loss,\n",
    "                    'trn_time': trn_time,            \n",
    "                    'val_loss': val_loss,\n",
    "                    'score': final_score,\n",
    "                    'score_g': score_g,\n",
    "                    'score_v': score_v,\n",
    "                    'score_c': score_c            \n",
    "                }\n",
    "            )      \n",
    "\n",
    "            # === Save Model Parameters ===\n",
    "            logger.info(\"Model successfully saved at: {}\".format(args.model_dir))            \n",
    "\n",
    "        \n",
    "def parser_args():\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument('--num_epochs', type=int, default=1)\n",
    "    parser.add_argument('--num_folds', type=int, default=5)\n",
    "    parser.add_argument('--vld_fold_idx', type=int, default=4)\n",
    "    parser.add_argument('--batch_size', type=int, default=256)\n",
    "    parser.add_argument('--lr', type=float, default=0.001)\n",
    "    parser.add_argument('--seed', type=int, default=1)\n",
    "    parser.add_argument('--log_interval', type=int, default=10) \n",
    "\n",
    "    # SageMaker Container environment    \n",
    "    parser.add_argument('--train_dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ['SM_MODEL_DIR'])    \n",
    "    parser.add_argument('--num_gpus', type=int, default=os.environ['SM_NUM_GPUS'])\n",
    " \n",
    "    args = parser.parse_args() \n",
    "    return args\n",
    "        \n",
    "    \n",
    "if __name__ =='__main__':\n",
    "\n",
    "    #parse arguments\n",
    "    args = parser_args() \n",
    "    \n",
    "    args.world_size = dist.get_world_size()\n",
    "    args.rank = dist.get_rank()\n",
    "    args.local_rank = dist.get_local_rank()\n",
    "    #print(f\"rank={args.rank}, local_rank={args.local_rank}\")\n",
    "    args.batch_size //= args.world_size // 8\n",
    "    args.batch_size = max(args.batch_size, 1)\n",
    "    \n",
    "    args.use_cuda = args.num_gpus > 0\n",
    "    print(\"args.use_cuda : {} , args.num_gpus : {}\".format(\n",
    "        args.use_cuda, args.num_gpus))\n",
    "    args.device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")\n",
    "\n",
    "    train_model(args)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Training on SageMaker\n",
    "---\n",
    "\n",
    "훈련 스크립트가 준비되었다면 SageMaker 훈련을 수행하는 법은 매우 간단합니다. SageMaker Python SDK 활용 시, Estimator 인스턴스를 생성하고 해당 인스턴스의 `fit()` 메서드를 호출하는 것이 전부입니다.\n",
    "\n",
    "#### 1) `Estimator` 인스턴스 생성 \n",
    "훈련 컨테이너에 필요한 설정들을 지정합니다. 본 핸즈온에서는 훈련 스크립트 파일이 포함된 경로인 소스 경로와(`source_dir`)와 훈련 스크립트 Python 파일만 엔트리포인트(`entry_point`)로 지정해 주면 됩니다.\n",
    "\n",
    "#### 2) `fit()` 메서드 호출\n",
    "`estimator.fit(YOUR_TRAINING_DATA_URI)` 메서드를 호출하면, 훈련에 필요한 인스턴스를 시작하고 컨테이너 환경을 시작합니다. \n",
    "필수 인자값은 훈련 데이터가 존해자는 S3 경로(`s3://`)이며, 로컬 모드로 훈련 시에는 로컬 경로(`file://`)를 지정하시면 됩니다. \n",
    "\n",
    "인자값 중 wait은 디폴트 값으로 `wait=True`이며, 모든 훈련 작업이 완료될 때까지 코드 셀이 freezing됩니다. 만약 다른 코드 셀을 실행하거나, 다른 훈련 job을 시작하고 싶다면 `wait=False`로 설정하여 Asynchronous 모드로 변경하면 됩니다.\n",
    "\n",
    "SageMaker 훈련이 끝나면 컨테이너 환경과 훈련 인스턴스는 자동으로 삭제됩니다. 이 때, SageMaker는 자동으로 `SM_MODEL_DIR` 경로에 저장된 최종 모델 아티팩트를 `model.tar.gz`로 압축하여 훈련 컨테이너 환경에서 S3 bucket으로 저장합니다. 당연히, S3 bucket에 저장된 모델 아티팩트를 다운로드받아 로컬 상에서 곧바로 테스트할 수 있습니다.\n",
    "\n",
    "\n",
    "#### Code Snippet: SageMaker Estimator 호출\n",
    "```python\n",
    "# 분산 훈련 수행 선언\n",
    "distribution = {\"smdistributed\": {\"dataparallel\": {\"enabled\": True}}}\n",
    "estimator = PyTorch(\n",
    "    instance_type='ml.p4d.24xlarge', # ml.p3.16xlarge, ml.p3dn.24xlarge 지원\n",
    "    instance_count=2,\n",
    "    framework_version='1.6.0',\n",
    "    py_version='py36’,\n",
    "    ...\n",
    "    distribution=distribution,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = 'bangali/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(base_job_name='pytorch-smdataparallel-bangali-handwritten',\n",
    "                    entry_point='train_smdataparallel.py',\n",
    "                    source_dir='src',\n",
    "                    role=role,\n",
    "                    instance_type='ml.p3.16xlarge',\n",
    "                    instance_count=2,\n",
    "                    framework_version='1.6.0',\n",
    "                    py_version='py36',\n",
    "                    hyperparameters = {'num_epochs': 10, \n",
    "                                       'num_folds': 5,\n",
    "                                       'vld_fold_idx': 4,\n",
    "                                       'batch_size': 256,\n",
    "                                       'lr': 0.001,\n",
    "                                       'log_interval': 10,\n",
    "                                      },\n",
    "                    distribution={'smdistributed':{\n",
    "                                        'dataparallel':{\n",
    "                                                'enabled': True\n",
    "                                             }\n",
    "                                      }\n",
    "                                  },\n",
    "                    debugger_hook_config=False)\n",
    "s3_input_train = sagemaker.TrainingInput(s3_data='s3://{}/{}'.format(bucket, prefix), content_type='csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-02 06:04:17 Starting - Starting the training job...\n",
      "2021-03-02 06:04:41 Starting - Launching requested ML instancesProfilerReport-1614665057: InProgress\n",
      "............\n",
      "2021-03-02 06:06:42 Starting - Preparing the instances for training.........\n",
      "2021-03-02 06:08:14 Downloading - Downloading input data......\n",
      "2021-03-02 06:09:18 Training - Downloading the training image.........\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-03-02 06:10:45,894 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-03-02 06:10:45,972 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2021-03-02 06:10:52,233 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[35m2021-03-02 06:10:52,233 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2021-03-02 06:10:52,715 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mCollecting albumentations\n",
      "  Downloading albumentations-0.5.2-py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (3.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.11.1 in /opt/conda/lib/python3.6/site-packages (from albumentations->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from albumentations->-r requirements.txt (line 1)) (1.5.4)\u001b[0m\n",
      "\u001b[35mCollecting scikit-image>=0.16.1\n",
      "  Downloading scikit_image-0.17.2-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\u001b[0m\n",
      "\u001b[35mCollecting opencv-python-headless>=4.1.1\n",
      "  Downloading opencv_python_headless-4.5.1.48-cp36-cp36m-manylinux2014_x86_64.whl (37.6 MB)\u001b[0m\n",
      "\u001b[35mCollecting imgaug>=0.4.0\n",
      "  Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: PyYAML in /opt/conda/lib/python3.6/site-packages (from albumentations->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[35mCollecting imageio\n",
      "  Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (1.15.0)\u001b[0m\n",
      "\u001b[35mCollecting Shapely\n",
      "  Downloading Shapely-1.7.1-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: Pillow in /opt/conda/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (8.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (3.3.4)\u001b[0m\n",
      "\u001b[35mCollecting opencv-python\n",
      "  Downloading opencv_python-4.5.1.48-cp36-cp36m-manylinux2014_x86_64.whl (50.4 MB)\u001b[0m\n",
      "\u001b[35mCollecting PyWavelets>=1.1.1\n",
      "  Downloading PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4 MB)\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-03-02 06:10:53,511 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-03-02 06:10:53,591 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-03-02 06:10:53,603 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[34m2021-03-02 06:10:53,603 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-03-02 06:10:54,079 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting albumentations\n",
      "  Downloading albumentations-0.5.2-py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from albumentations->-r requirements.txt (line 1)) (1.5.4)\u001b[0m\n",
      "\u001b[34mCollecting opencv-python-headless>=4.1.1\n",
      "  Downloading opencv_python_headless-4.5.1.48-cp36-cp36m-manylinux2014_x86_64.whl (37.6 MB)\u001b[0m\n",
      "\u001b[34mCollecting imgaug>=0.4.0\n",
      "  Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML in /opt/conda/lib/python3.6/site-packages (from albumentations->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting scikit-image>=0.16.1\n",
      "  Downloading scikit_image-0.17.2-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.11.1 in /opt/conda/lib/python3.6/site-packages (from albumentations->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (3.3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /opt/conda/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (8.1.0)\u001b[0m\n",
      "\u001b[34mCollecting imageio\n",
      "  Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mCollecting Shapely\n",
      "  Downloading Shapely-1.7.1-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34mCollecting opencv-python\n",
      "  Downloading opencv_python-4.5.1.48-cp36-cp36m-manylinux2014_x86_64.whl (50.4 MB)\u001b[0m\n",
      "\u001b[35mCollecting networkx>=2.0\n",
      "  Downloading networkx-2.5-py3-none-any.whl (1.6 MB)\u001b[0m\n",
      "\u001b[35mCollecting tifffile>=2019.7.26\n",
      "  Downloading tifffile-2020.9.3-py3-none-any.whl (148 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (1.3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (0.10.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations->-r requirements.txt (line 1)) (4.4.2)\u001b[0m\n",
      "\u001b[34mCollecting PyWavelets>=1.1.1\n",
      "  Downloading PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4 MB)\u001b[0m\n",
      "\u001b[35mInstalling collected packages: tifffile, PyWavelets, networkx, imageio, Shapely, scikit-image, opencv-python, opencv-python-headless, imgaug, albumentations\u001b[0m\n",
      "\u001b[34mCollecting tifffile>=2019.7.26\n",
      "  Downloading tifffile-2020.9.3-py3-none-any.whl (148 kB)\u001b[0m\n",
      "\u001b[34mCollecting networkx>=2.0\n",
      "  Downloading networkx-2.5-py3-none-any.whl (1.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations->-r requirements.txt (line 1)) (4.4.2)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tifffile, PyWavelets, networkx, imageio, Shapely, scikit-image, opencv-python, opencv-python-headless, imgaug, albumentations\u001b[0m\n",
      "\u001b[35mSuccessfully installed PyWavelets-1.1.1 Shapely-1.7.1 albumentations-0.5.2 imageio-2.9.0 imgaug-0.4.0 networkx-2.5 opencv-python-4.5.1.48 opencv-python-headless-4.5.1.48 scikit-image-0.17.2 tifffile-2020.9.3\n",
      "\u001b[0m\n",
      "\u001b[35m2021-03-02 06:11:02,915 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2021-03-02 06:11:02,915 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2021-03-02 06:11:02,917 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-03-02 06:11:02,917 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.221.128\u001b[0m\n",
      "\u001b[35m2021-03-02 06:11:03,919 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-03-02 06:11:03,919 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.221.128\u001b[0m\n",
      "\u001b[34mSuccessfully installed PyWavelets-1.1.1 Shapely-1.7.1 albumentations-0.5.2 imageio-2.9.0 imgaug-0.4.0 networkx-2.5 opencv-python-4.5.1.48 opencv-python-headless-4.5.1.48 scikit-image-0.17.2 tifffile-2020.9.3\n",
      "\u001b[0m\n",
      "\u001b[34m2021-03-02 06:11:04,740 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2021-03-02 06:11:04,740 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2021-03-02 06:11:04,743 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2021-03-02 06:11:04,745 sagemaker-training-toolkit INFO     Cannot connect to host algo-2 at port 22\u001b[0m\n",
      "\u001b[34m2021-03-02 06:11:04,745 sagemaker-training-toolkit ERROR    Connection failed\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/sagemaker_training/smdataparallel.py\", line 306, in _can_connect\n",
      "    client.connect(host, port=port)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/paramiko/client.py\", line 368, in connect\n",
      "    raise NoValidConnectionsError(errors)\u001b[0m\n",
      "\u001b[34mparamiko.ssh_exception.NoValidConnectionsError: [Errno None] Unable to connect to port 22 on 10.0.207.154\u001b[0m\n",
      "\u001b[34m2021-03-02 06:11:04,749 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[35m2021-03-02 06:11:04,929 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[35m2021-03-02 06:11:05,011 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2021-03-02 06:11:05,011 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-03-02 06:11:05,011 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2021-03-02 06:11:05,011 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[34m2021-03-02 06:11:05,758 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[35m2021-03-02 06:11:05,016 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2021-03-02 06:11:05,834 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2021-03-02 06:11:05,835 sagemaker-training-toolkit INFO     Can connect to host algo-2 at port 22\u001b[0m\n",
      "\u001b[34m2021-03-02 06:11:05,835 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[34m2021-03-02 06:11:05,835 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2021-03-02 06:11:05,835 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2021-03-02 06:11:05,835 sagemaker-training-toolkit INFO     Host: ['algo-1', 'algo-2']\u001b[0m\n",
      "\u001b[34m2021-03-02 06:11:05,835 sagemaker-training-toolkit INFO     instance type: ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34m2021-03-02 06:11:05,913 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": true,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 256,\n",
      "        \"lr\": 0.001,\n",
      "        \"vld_fold_idx\": 4,\n",
      "        \"log_interval\": 10,\n",
      "        \"num_epochs\": 10,\n",
      "        \"num_folds\": 5\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"ContentType\": \"csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-smdataparallel-bangali-handwrit-2021-03-02-06-04-16-979\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-143656149352/pytorch-smdataparallel-bangali-handwrit-2021-03-02-06-04-16-979/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_smdataparallel\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_smdataparallel.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":256,\"log_interval\":10,\"lr\":0.001,\"num_epochs\":10,\"num_folds\":5,\"vld_fold_idx\":4}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_smdataparallel.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_smdataparallel\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-143656149352/pytorch-smdataparallel-bangali-handwrit-2021-03-02-06-04-16-979/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"batch_size\":256,\"log_interval\":10,\"lr\":0.001,\"num_epochs\":10,\"num_folds\":5,\"vld_fold_idx\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-smdataparallel-bangali-handwrit-2021-03-02-06-04-16-979\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-143656149352/pytorch-smdataparallel-bangali-handwrit-2021-03-02-06-04-16-979/source/sourcedir.tar.gz\",\"module_name\":\"train_smdataparallel\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_smdataparallel.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"256\",\"--log_interval\",\"10\",\"--lr\",\"0.001\",\"--num_epochs\",\"10\",\"--num_folds\",\"5\",\"--vld_fold_idx\",\"4\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=256\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.001\u001b[0m\n",
      "\u001b[34mSM_HP_VLD_FOLD_IDX=4\u001b[0m\n",
      "\u001b[34mSM_HP_LOG_INTERVAL=10\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_FOLDS=5\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8,algo-2:8 -np 16 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 2 -x NCCL_SOCKET_IFNAME=eth0 -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_HOMOGENEOUS=1 -x FI_PROVIDER=sockets -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -x SMDATAPARALLEL_SERVER_ADDR=algo-1 -x SMDATAPARALLEL_SERVER_PORT=7592 -x SAGEMAKER_INSTANCE_TYPE=ml.p3.16xlarge smddprun /opt/conda/bin/python3.6 -m mpi4py train_smdataparallel.py --batch_size 256 --log_interval 10 --lr 0.001 --num_epochs 10 --num_folds 5 --vld_fold_idx 4\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-03-02 06:11:06 Training - Training image download completed. Training in progress.\u001b[35m2021-03-02 06:11:08,025 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=55, name='orted', status='sleeping', started='06:11:06')]\u001b[0m\n",
      "\u001b[35m2021-03-02 06:11:08,025 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=55, name='orted', status='sleeping', started='06:11:06')]\u001b[0m\n",
      "\u001b[35m2021-03-02 06:11:08,025 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=55, name='orted', status='sleeping', started='06:11:06')]\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:args.use_cuda : True , args.num_gpus : 8\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:args.use_cuda : True , args.num_gpus : 8\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:args.use_cuda : True , args.num_gpus : 8\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:args.use_cuda : True , args.num_gpus : 8\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:args.use_cuda : True , args.num_gpus : 8\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:args.use_cuda : True , args.num_gpus : 8\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:args.use_cuda : True , args.num_gpus : 8\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:args.use_cuda : True , args.num_gpus : 8\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:args.use_cuda : True , args.num_gpus : 8\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:args.use_cuda : True , args.num_gpus : 8\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:args.use_cuda : True , args.num_gpus : 8\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:args.use_cuda : True , args.num_gpus : 8\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:args.use_cuda : True , args.num_gpus : 8\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:args.use_cuda : True , args.num_gpus : 8\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:args.use_cuda : True , args.num_gpus : 8\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Running smdistributed.dataparallel v1.0.0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:args.use_cuda : True , args.num_gpus : 8\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:=== Getting Images ===\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:['/opt/ml/input/data/training/train_image_data_0.feather', '/opt/ml/input/data/training/train_image_data_1.feather', '/opt/ml/input/data/training/train_image_data_2.feather', '/opt/ml/input/data/training/train_image_data_3.feather']\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:=== Getting Images ===\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:['/opt/ml/input/data/training/train_image_data_0.feather', '/opt/ml/input/data/training/train_image_data_1.feather', '/opt/ml/input/data/training/train_image_data_2.feather', '/opt/ml/input/data/training/train_image_data_3.feather']\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:=== Getting Images ===\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:=== Getting Images ===\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:=== Getting Images ===\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:['/opt/ml/input/data/training/train_image_data_0.feather', '/opt/ml/input/data/training/train_image_data_1.feather', '/opt/ml/input/data/training/train_image_data_2.feather', '/opt/ml/input/data/training/train_image_data_3.feather']\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:['/opt/ml/input/data/training/train_image_data_0.feather', '/opt/ml/input/data/training/train_image_data_1.feather', '/opt/ml/input/data/training/train_image_data_2.feather', '/opt/ml/input/data/training/train_image_data_3.feather']\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:=== Getting Images ===\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:['/opt/ml/input/data/training/train_image_data_0.feather', '/opt/ml/input/data/training/train_image_data_1.feather', '/opt/ml/input/data/training/train_image_data_2.feather', '/opt/ml/input/data/training/train_image_data_3.feather']\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:['/opt/ml/input/data/training/train_image_data_0.feather', '/opt/ml/input/data/training/train_image_data_1.feather', '/opt/ml/input/data/training/train_image_data_2.feather', '/opt/ml/input/data/training/train_image_data_3.feather']\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:=== Getting Images ===\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:['/opt/ml/input/data/training/train_image_data_0.feather', '/opt/ml/input/data/training/train_image_data_1.feather', '/opt/ml/input/data/training/train_image_data_2.feather', '/opt/ml/input/data/training/train_image_data_3.feather']\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:=== Getting Images ===\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:['/opt/ml/input/data/training/train_image_data_0.feather', '/opt/ml/input/data/training/train_image_data_1.feather', '/opt/ml/input/data/training/train_image_data_2.feather', '/opt/ml/input/data/training/train_image_data_3.feather']\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:=== Getting Images ===\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:['/opt/ml/input/data/training/train_image_data_0.feather', '/opt/ml/input/data/training/train_image_data_1.feather', '/opt/ml/input/data/training/train_image_data_2.feather', '/opt/ml/input/data/training/train_image_data_3.feather']\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:=== Getting Images ===\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:=== Getting Images ===\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:['/opt/ml/input/data/training/train_image_data_0.feather', '/opt/ml/input/data/training/train_image_data_1.feather', '/opt/ml/input/data/training/train_image_data_2.feather', '/opt/ml/input/data/training/train_image_data_3.feather']\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:['/opt/ml/input/data/training/train_image_data_0.feather', '/opt/ml/input/data/training/train_image_data_1.feather', '/opt/ml/input/data/training/train_image_data_2.feather', '/opt/ml/input/data/training/train_image_data_3.feather']\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:=== Getting Images ===\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:=== Getting Images ===\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:['/opt/ml/input/data/training/train_image_data_0.feather', '/opt/ml/input/data/training/train_image_data_1.feather', '/opt/ml/input/data/training/train_image_data_2.feather', '/opt/ml/input/data/training/train_image_data_3.feather']\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:['/opt/ml/input/data/training/train_image_data_0.feather', '/opt/ml/input/data/training/train_image_data_1.feather', '/opt/ml/input/data/training/train_image_data_2.feather', '/opt/ml/input/data/training/train_image_data_3.feather']\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:=== Getting Images ===\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:=== Getting Images ===\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:['/opt/ml/input/data/training/train_image_data_0.feather', '/opt/ml/input/data/training/train_image_data_1.feather', '/opt/ml/input/data/training/train_image_data_2.feather', '/opt/ml/input/data/training/train_image_data_3.feather']\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:['/opt/ml/input/data/training/train_image_data_0.feather', '/opt/ml/input/data/training/train_image_data_1.feather', '/opt/ml/input/data/training/train_image_data_2.feather', '/opt/ml/input/data/training/train_image_data_3.feather']\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:=== Getting Images ===\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:['/opt/ml/input/data/training/train_image_data_0.feather', '/opt/ml/input/data/training/train_image_data_1.feather', '/opt/ml/input/data/training/train_image_data_2.feather', '/opt/ml/input/data/training/train_image_data_3.feather']\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[1,5]<stdout>:=== Getting Pre-trained model ===\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:=== Getting Pre-trained model ===\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:=== Getting Pre-trained model ===\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:=== Getting Pre-trained model ===\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:=== Getting Pre-trained model ===\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:=== Getting Pre-trained model ===\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:=== Getting Pre-trained model ===\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:=== Getting Pre-trained model ===\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:=== Getting Pre-trained model ===\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:=== Getting Pre-trained model ===\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:=== Getting Pre-trained model ===\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:=== Getting Pre-trained model ===\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:=== Getting Pre-trained model ===\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:=== Getting Pre-trained model ===\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:=== Getting Pre-trained model ===\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:=== Getting Pre-trained model ===\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:=== Start Training ===\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:=== Start Training ===\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:=== Start Training ===\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:=== Start Training ===\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:=== Start Training ===\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:=== Start Training ===\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:=== Start Training ===\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:=== Start Training ===\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:=== Start Training ===\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:=== Start Training ===\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:=== Start Training ===\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:=== Start Training ===\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:=== Start Training ===\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:=== Start Training ===\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:=== Start Training ===\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:=== Start Training ===\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-03-02 06:12:14.155 algo-1:65 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-03-02 06:12:14.166 algo-1:55 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-03-02 06:12:14.166 algo-1:58 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-03-02 06:12:14.168 algo-1:60 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-03-02 06:12:14.169 algo-1:61 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-03-02 06:12:14.170 algo-1:67 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-03-02 06:12:14.170 algo-1:63 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-03-02 06:12:14.174 algo-1:66 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-03-02 06:12:14.185 algo-2:73 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-03-02 06:12:14.187 algo-2:70 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-03-02 06:12:14.188 algo-2:66 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-03-02 06:12:14.188 algo-2:62 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-03-02 06:12:14.191 algo-2:64 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-03-02 06:12:14.192 algo-2:72 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-03-02 06:12:14.194 algo-2:68 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-03-02 06:12:14.222 algo-2:74 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-03-02 06:12:14.362 algo-1:67 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-03-02 06:12:14.362 algo-1:60 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-03-02 06:12:14.362 algo-1:58 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-03-02 06:12:14.362 algo-1:61 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-03-02 06:12:14.362 algo-1:65 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-03-02 06:12:14.362 algo-1:55 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-03-02 06:12:14.362 algo-1:63 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-03-02 06:12:14.362 algo-1:66 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-03-02 06:12:14.380 algo-2:68 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-03-02 06:12:14.380 algo-2:70 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-03-02 06:12:14.380 algo-2:66 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-03-02 06:12:14.380 algo-2:73 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-03-02 06:12:14.380 algo-2:72 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-03-02 06:12:14.380 algo-2:62 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-03-02 06:12:14.380 algo-2:64 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-03-02 06:12:14.380 algo-2:74 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 0 Batch 10/40] loss: 2.8402\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 0 Batch 10/40] loss: 2.9133\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 0 Batch 10/40] loss: 3.0502\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 0 Batch 10/40] loss: 3.0625\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 0 Batch 10/40] loss: 2.9498\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 0 Batch 10/40] loss: 2.8376\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 0 Batch 10/40] loss: 2.9739\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 0 Batch 10/40] loss: 2.8676\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 0 Batch 10/40] loss: 2.9593\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 0 Batch 10/40] loss: 2.9360\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 0 Batch 10/40] loss: 2.9229\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 0 Batch 10/40] loss: 3.0292\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 0 Batch 10/40] loss: 2.9223\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 0 Batch 10/40] loss: 2.8621\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 0 Batch 10/40] loss: 2.8978\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 0 Batch 10/40] loss: 3.0071\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 0 Batch 20/40] loss: 2.2289\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 0 Batch 20/40] loss: 2.3371\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 0 Batch 20/40] loss: 2.5041\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 0 Batch 20/40] loss: 2.1094\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 0 Batch 20/40] loss: 2.5060\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 0 Batch 20/40] loss: 2.1628\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 0 Batch 20/40] loss: 2.1389\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 0 Batch 20/40] loss: 2.1084\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 0 Batch 20/40] loss: 2.2664\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 0 Batch 20/40] loss: 2.2270\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 0 Batch 20/40] loss: 2.5519\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 0 Batch 20/40] loss: 2.2868\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 0 Batch 20/40] loss: 2.2799\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 0 Batch 20/40] loss: 2.1549\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 0 Batch 20/40] loss: 2.3412\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 0 Batch 20/40] loss: 2.2924\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 0 Batch 30/40] loss: 1.5730\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 0 Batch 30/40] loss: 2.0300\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 0 Batch 30/40] loss: 1.9191\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 0 Batch 30/40] loss: 1.9125\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 0 Batch 30/40] loss: 2.0857\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 0 Batch 30/40] loss: 1.5566\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 0 Batch 30/40] loss: 1.8483\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 0 Batch 30/40] loss: 1.8555\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 0 Batch 30/40] loss: 1.8569\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 0 Batch 30/40] loss: 1.6538\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 0 Batch 30/40] loss: 2.0431\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 0 Batch 30/40] loss: 1.9065\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 0 Batch 30/40] loss: 1.8813\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 0 Batch 30/40] loss: 2.1897\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 0 Batch 30/40] loss: 1.8257\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 0 Batch 30/40] loss: 1.9258\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 0 Batch 40/40] loss: 2.1556\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 0 Batch 40/40] loss: 1.7370\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 0 Batch 40/40] loss: 1.4551\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 0 Batch 40/40] loss: 1.7491\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 0 Batch 40/40] loss: 1.7784\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 0 Batch 40/40] loss: 1.5080\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 0 Batch 40/40] loss: 1.5836\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 0 Batch 40/40] loss: 1.8882\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 0 Batch 40/40] loss: 1.9352\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 0 Batch 40/40] loss: 1.2175\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 0 Batch 40/40] loss: 1.4516\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 0 Batch 40/40] loss: 1.7716\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 0 Batch 40/40] loss: 1.2129\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 0 Batch 40/40] loss: 1.6556\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 0 Batch 40/40] loss: 1.8345\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 0 Batch 40/40] loss: 2.0404\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:=== Start Validation ===\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[1,0]<stdout>:[Epoch 0] trn_loss: 2.3238, vld_loss: 1.5416, score: 0.5718, score_each: [0.4769, 0.6939, 0.6395]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Model successfully saved at: /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 1 Batch 10/40] loss: 1.4637\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 1 Batch 10/40] loss: 1.5463\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 1 Batch 10/40] loss: 1.6857\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 1 Batch 10/40] loss: 1.4459\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 1 Batch 10/40] loss: 1.4196\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 1 Batch 10/40] loss: 1.6493\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 1 Batch 10/40] loss: 1.7038\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 1 Batch 10/40] loss: 1.4178\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 1 Batch 10/40] loss: 1.4853\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 1 Batch 10/40] loss: 1.5725\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 1 Batch 10/40] loss: 1.5323\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 1 Batch 10/40] loss: 1.8691\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 1 Batch 10/40] loss: 1.7042\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 1 Batch 10/40] loss: 1.5550\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 1 Batch 10/40] loss: 1.3859\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 1 Batch 10/40] loss: 1.6832\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 1 Batch 20/40] loss: 1.3653\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 1 Batch 20/40] loss: 1.5182\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 1 Batch 20/40] loss: 0.9883\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 1 Batch 20/40] loss: 1.3808\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 1 Batch 20/40] loss: 1.7684\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 1 Batch 20/40] loss: 1.1043\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 1 Batch 20/40] loss: 1.1526\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 1 Batch 20/40] loss: 1.4661\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 1 Batch 20/40] loss: 1.2659\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 1 Batch 20/40] loss: 1.6082\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 1 Batch 20/40] loss: 1.2356\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 1 Batch 20/40] loss: 1.5507\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 1 Batch 20/40] loss: 1.3264\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 1 Batch 20/40] loss: 1.2930\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 1 Batch 20/40] loss: 1.5822\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 1 Batch 20/40] loss: 1.8390\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 1 Batch 30/40] loss: 1.5326\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 1 Batch 30/40] loss: 1.2025\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 1 Batch 30/40] loss: 1.8277\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 1 Batch 30/40] loss: 1.9352\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 1 Batch 30/40] loss: 1.7805\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 1 Batch 30/40] loss: 1.4632\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 1 Batch 30/40] loss: 1.4549\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 1 Batch 30/40] loss: 1.5532\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 1 Batch 30/40] loss: 1.5809\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 1 Batch 30/40] loss: 1.8143\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 1 Batch 30/40] loss: 1.1334\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 1 Batch 30/40] loss: 0.9812\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 1 Batch 30/40] loss: 1.4630\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 1 Batch 30/40] loss: 1.2651\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 1 Batch 30/40] loss: 1.1009\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 1 Batch 30/40] loss: 1.5068\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 1 Batch 40/40] loss: 1.6553\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 1 Batch 40/40] loss: 1.1382\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 1 Batch 40/40] loss: 1.1018\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 1 Batch 40/40] loss: 0.9564\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 1 Batch 40/40] loss: 1.6006\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 1 Batch 40/40] loss: 1.4194\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 1 Batch 40/40] loss: 1.2014\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 1 Batch 40/40] loss: 0.6192\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 1 Batch 40/40] loss: 1.1332\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 1 Batch 40/40] loss: 1.5770\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 1 Batch 40/40] loss: 1.3312\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 1 Batch 40/40] loss: 1.2757\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 1 Batch 40/40] loss: 1.5959\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 1 Batch 40/40] loss: 1.9789\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 1 Batch 40/40] loss: 1.5398\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 1 Batch 40/40] loss: 1.6011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:=== Start Validation ===\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 1] trn_loss: 1.4299, vld_loss: 1.1913, score: 0.7955, score_each: [0.8142, 0.8871, 0.6665]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Model successfully saved at: /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 2 Batch 10/40] loss: 1.0356\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 2 Batch 10/40] loss: 1.1076\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 2 Batch 10/40] loss: 1.4556\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 2 Batch 10/40] loss: 0.8911\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 2 Batch 10/40] loss: 1.5216\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 2 Batch 10/40] loss: 1.7462\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 2 Batch 10/40] loss: 1.7591\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 2 Batch 10/40] loss: 1.6505\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 2 Batch 10/40] loss: 1.0209\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 2 Batch 10/40] loss: 1.2461\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 2 Batch 10/40] loss: 1.2003\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 2 Batch 10/40] loss: 1.4583\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 2 Batch 10/40] loss: 1.1235\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 2 Batch 10/40] loss: 1.3155\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 2 Batch 10/40] loss: 1.4219\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 2 Batch 10/40] loss: 1.0263\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 2 Batch 20/40] loss: 0.9273\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 2 Batch 20/40] loss: 0.8521\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 2 Batch 20/40] loss: 1.2836\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 2 Batch 20/40] loss: 1.3379\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 2 Batch 20/40] loss: 0.7097\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 2 Batch 20/40] loss: 1.4639\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 2 Batch 20/40] loss: 0.7608\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 2 Batch 20/40] loss: 1.1803\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 2 Batch 20/40] loss: 1.4699\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 2 Batch 20/40] loss: 1.4575\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 2 Batch 20/40] loss: 1.2923\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 2 Batch 20/40] loss: 1.3407\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 2 Batch 20/40] loss: 0.4568\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 2 Batch 20/40] loss: 1.1609\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 2 Batch 20/40] loss: 0.9990\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 2 Batch 20/40] loss: 1.1260\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 2 Batch 30/40] loss: 1.2770\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 2 Batch 30/40] loss: 1.2873\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 2 Batch 30/40] loss: 0.6504\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 2 Batch 30/40] loss: 1.1498\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 2 Batch 30/40] loss: 0.9319\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 2 Batch 30/40] loss: 1.3319\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 2 Batch 30/40] loss: 1.5479\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 2 Batch 30/40] loss: 1.5610\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 2 Batch 30/40] loss: 0.9863\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 2 Batch 30/40] loss: 1.4901\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 2 Batch 30/40] loss: 1.4832\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 2 Batch 30/40] loss: 1.2743\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 2 Batch 30/40] loss: 1.4146\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 2 Batch 30/40] loss: 1.1591\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 2 Batch 30/40] loss: 1.5424\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 2 Batch 30/40] loss: 1.2320\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 2 Batch 40/40] loss: 1.0184\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 2 Batch 40/40] loss: 0.8187\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 2 Batch 40/40] loss: 0.9063\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 2 Batch 40/40] loss: 1.4974\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 2 Batch 40/40] loss: 1.0384\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 2 Batch 40/40] loss: 1.0272\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 2 Batch 40/40] loss: 1.3558\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 2 Batch 40/40] loss: 1.2885\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 2 Batch 40/40] loss: 1.5697\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 2 Batch 40/40] loss: 1.6499\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 2 Batch 40/40] loss: 1.5153\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 2 Batch 40/40] loss: 1.0478\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 2 Batch 40/40] loss: 1.0430\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 2 Batch 40/40] loss: 1.0234\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 2 Batch 40/40] loss: 1.2293\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 2 Batch 40/40] loss: 1.3750\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:=== Start Validation ===\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 2] trn_loss: 1.3556, vld_loss: 1.2457, score: 0.8435, score_each: [0.7882, 0.9184, 0.8792]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Model successfully saved at: /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 3 Batch 10/40] loss: 1.3140\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 3 Batch 10/40] loss: 1.2895\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 3 Batch 10/40] loss: 1.6820\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 3 Batch 10/40] loss: 2.0077\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 3 Batch 10/40] loss: 1.4150\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 3 Batch 10/40] loss: 1.5096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 3 Batch 10/40] loss: 0.9898\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 3 Batch 10/40] loss: 1.3231\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 3 Batch 10/40] loss: 1.2329\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 3 Batch 10/40] loss: 1.0941\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 3 Batch 10/40] loss: 0.8673\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 3 Batch 10/40] loss: 1.7545\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 3 Batch 10/40] loss: 1.3281\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 3 Batch 10/40] loss: 0.7611\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 3 Batch 10/40] loss: 1.6397\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 3 Batch 10/40] loss: 1.1532\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[1,0]<stdout>:[Epoch 3 Batch 20/40] loss: 1.0503\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 3 Batch 20/40] loss: 1.0428\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 3 Batch 20/40] loss: 1.0796\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 3 Batch 20/40] loss: 0.9183\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 3 Batch 20/40] loss: 1.3849\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 3 Batch 20/40] loss: 1.4606\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 3 Batch 20/40] loss: 1.5220\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 3 Batch 20/40] loss: 1.3371\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 3 Batch 20/40] loss: 1.4372\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 3 Batch 20/40] loss: 1.0746\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 3 Batch 20/40] loss: 1.1110\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 3 Batch 20/40] loss: 0.6362\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 3 Batch 20/40] loss: 1.1992\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 3 Batch 20/40] loss: 1.2190\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 3 Batch 20/40] loss: 1.2849\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 3 Batch 20/40] loss: 1.4154\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 3 Batch 30/40] loss: 1.1737\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 3 Batch 30/40] loss: 1.6040\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 3 Batch 30/40] loss: 1.1438\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 3 Batch 30/40] loss: 0.9860\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 3 Batch 30/40] loss: 1.1073\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 3 Batch 30/40] loss: 1.2212\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 3 Batch 30/40] loss: 1.1273\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 3 Batch 30/40] loss: 1.0639\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 3 Batch 30/40] loss: 1.5061\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 3 Batch 30/40] loss: 0.9825\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 3 Batch 30/40] loss: 1.5034\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 3 Batch 30/40] loss: 0.8692\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 3 Batch 30/40] loss: 1.6147\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 3 Batch 30/40] loss: 1.0629\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 3 Batch 30/40] loss: 0.7937\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 3 Batch 30/40] loss: 1.3746\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 3 Batch 40/40] loss: 1.4966\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 3 Batch 40/40] loss: 1.0854\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 3 Batch 40/40] loss: 0.8623\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 3 Batch 40/40] loss: 1.0575\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 3 Batch 40/40] loss: 0.8559\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 3 Batch 40/40] loss: 1.4368\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 3 Batch 40/40] loss: 1.0583\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 3 Batch 40/40] loss: 1.2832\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 3 Batch 40/40] loss: 1.2670\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 3 Batch 40/40] loss: 1.1161\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 3 Batch 40/40] loss: 1.2273\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 3 Batch 40/40] loss: 1.6395\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 3 Batch 40/40] loss: 1.3689\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 3 Batch 40/40] loss: 1.1256\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 3 Batch 40/40] loss: 0.7129\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 3 Batch 40/40] loss: 0.6042\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:=== Start Validation ===\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 3] trn_loss: 1.5754, vld_loss: 1.1615, score: 0.8952, score_each: [0.8635, 0.9540, 0.8998]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Model successfully saved at: /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 4 Batch 10/40] loss: 1.2487\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 4 Batch 10/40] loss: 1.0988\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 4 Batch 10/40] loss: 1.0164\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 4 Batch 10/40] loss: 1.4007\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 4 Batch 10/40] loss: 1.2686\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 4 Batch 10/40] loss: 1.1593\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 4 Batch 10/40] loss: 0.8710\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 4 Batch 10/40] loss: 1.6548\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 4 Batch 10/40] loss: 0.8473\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 4 Batch 10/40] loss: 1.2214\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 4 Batch 10/40] loss: 0.7586\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 4 Batch 10/40] loss: 1.0117\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 4 Batch 10/40] loss: 0.5934\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 4 Batch 10/40] loss: 0.9643\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 4 Batch 10/40] loss: 1.0291\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 4 Batch 10/40] loss: 1.1217\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 4 Batch 20/40] loss: 0.7348\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 4 Batch 20/40] loss: 1.1449\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 4 Batch 20/40] loss: 0.6439\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 4 Batch 20/40] loss: 1.0044\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 4 Batch 20/40] loss: 0.7666\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 4 Batch 20/40] loss: 1.4987\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 4 Batch 20/40] loss: 1.3623\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 4 Batch 20/40] loss: 2.0480\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 4 Batch 20/40] loss: 1.3172\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 4 Batch 20/40] loss: 1.3186\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 4 Batch 20/40] loss: 1.6242\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 4 Batch 20/40] loss: 0.9685\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 4 Batch 20/40] loss: 1.2223\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 4 Batch 20/40] loss: 0.9921\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 4 Batch 20/40] loss: 0.6174\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 4 Batch 20/40] loss: 1.1162\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 4 Batch 30/40] loss: 1.2260\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 4 Batch 30/40] loss: 0.7563\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 4 Batch 30/40] loss: 1.1298\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 4 Batch 30/40] loss: 1.4265\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 4 Batch 30/40] loss: 0.9831\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 4 Batch 30/40] loss: 0.9546\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 4 Batch 30/40] loss: 1.0764\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 4 Batch 30/40] loss: 0.4928\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 4 Batch 30/40] loss: 0.9183\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 4 Batch 30/40] loss: 1.0310\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 4 Batch 30/40] loss: 1.1823\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 4 Batch 30/40] loss: 1.0657\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 4 Batch 30/40] loss: 0.6402\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 4 Batch 30/40] loss: 1.2137\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 4 Batch 30/40] loss: 0.6927\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 4 Batch 30/40] loss: 1.1741\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 4 Batch 40/40] loss: 0.3635\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 4 Batch 40/40] loss: 1.1285\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 4 Batch 40/40] loss: 1.2968\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 4 Batch 40/40] loss: 1.3935\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 4 Batch 40/40] loss: 0.7365\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 4 Batch 40/40] loss: 1.0120\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 4 Batch 40/40] loss: 0.8853\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 4 Batch 40/40] loss: 1.1459\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 4 Batch 40/40] loss: 0.9163\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 4 Batch 40/40] loss: 1.4154\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 4 Batch 40/40] loss: 1.5980\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 4 Batch 40/40] loss: 1.1046\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 4 Batch 40/40] loss: 0.6854\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 4 Batch 40/40] loss: 1.0249\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 4 Batch 40/40] loss: 0.7814\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 4 Batch 40/40] loss: 0.7660\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:=== Start Validation ===\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 4] trn_loss: 1.2464, vld_loss: 1.2133, score: 0.9108, score_each: [0.8975, 0.9533, 0.8948]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Model successfully saved at: /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 5 Batch 10/40] loss: 1.1436\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 5 Batch 10/40] loss: 1.2228\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 5 Batch 10/40] loss: 0.5438\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 5 Batch 10/40] loss: 1.1376\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 5 Batch 10/40] loss: 1.1321\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 5 Batch 10/40] loss: 0.9371\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 5 Batch 10/40] loss: 1.2907\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 5 Batch 10/40] loss: 0.8617\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 5 Batch 10/40] loss: 0.9002\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 5 Batch 10/40] loss: 0.8077\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 5 Batch 10/40] loss: 0.4214\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 5 Batch 10/40] loss: 0.9700\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 5 Batch 10/40] loss: 1.3743\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 5 Batch 10/40] loss: 1.7813\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 5 Batch 10/40] loss: 1.4469\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 5 Batch 10/40] loss: 1.0378\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 5 Batch 20/40] loss: 1.3560\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 5 Batch 20/40] loss: 0.7106\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 5 Batch 20/40] loss: 1.0377\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 5 Batch 20/40] loss: 0.8886\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 5 Batch 20/40] loss: 0.8613\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 5 Batch 20/40] loss: 1.0735\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 5 Batch 20/40] loss: 1.5083\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 5 Batch 20/40] loss: 0.9369\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 5 Batch 20/40] loss: 0.4410\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 5 Batch 20/40] loss: 0.7614\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 5 Batch 20/40] loss: 0.5205\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 5 Batch 20/40] loss: 1.7142\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 5 Batch 20/40] loss: 0.7098\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 5 Batch 20/40] loss: 1.1874\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 5 Batch 20/40] loss: 0.7960\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 5 Batch 20/40] loss: 1.5268\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[1,8]<stdout>:[Epoch 5 Batch 30/40] loss: 0.7209\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 5 Batch 30/40] loss: 1.0202\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 5 Batch 30/40] loss: 0.9606\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 5 Batch 30/40] loss: 1.0598\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 5 Batch 30/40] loss: 0.7510\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 5 Batch 30/40] loss: 1.0282\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 5 Batch 30/40] loss: 0.8249\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 5 Batch 30/40] loss: 1.3329\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 5 Batch 30/40] loss: 1.2701\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 5 Batch 30/40] loss: 1.1590\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 5 Batch 30/40] loss: 0.7948\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 5 Batch 30/40] loss: 1.3051\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 5 Batch 30/40] loss: 1.0578\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 5 Batch 30/40] loss: 1.1002\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 5 Batch 30/40] loss: 1.0048\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 5 Batch 30/40] loss: 0.5489\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 5 Batch 40/40] loss: 1.1066\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 5 Batch 40/40] loss: 1.5024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 5 Batch 40/40] loss: 1.2283\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 5 Batch 40/40] loss: 1.2149\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 5 Batch 40/40] loss: 0.9940\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 5 Batch 40/40] loss: 1.2118\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 5 Batch 40/40] loss: 1.1858\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 5 Batch 40/40] loss: 1.6756\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 5 Batch 40/40] loss: 0.9141\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 5 Batch 40/40] loss: 1.3067\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 5 Batch 40/40] loss: 0.8966\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 5 Batch 40/40] loss: 1.2703\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 5 Batch 40/40] loss: 1.0388\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 5 Batch 40/40] loss: 1.0372\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 5 Batch 40/40] loss: 1.0149\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 5 Batch 40/40] loss: 1.0594\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:=== Start Validation ===\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 5] trn_loss: 1.1074, vld_loss: 1.1234, score: 0.9240, score_each: [0.9177, 0.9558, 0.9047]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Model successfully saved at: /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 6 Batch 10/40] loss: 0.8916\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 6 Batch 10/40] loss: 0.8031\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 6 Batch 10/40] loss: 0.9907\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 6 Batch 10/40] loss: 1.2465\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 6 Batch 10/40] loss: 0.9265\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 6 Batch 10/40] loss: 1.0287\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 6 Batch 10/40] loss: 0.8602\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 6 Batch 10/40] loss: 0.9418\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 6 Batch 10/40] loss: 1.1597\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 6 Batch 10/40] loss: 1.2657\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 6 Batch 10/40] loss: 0.9993\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 6 Batch 10/40] loss: 0.5284\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 6 Batch 10/40] loss: 1.7602\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 6 Batch 10/40] loss: 0.7649\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 6 Batch 10/40] loss: 0.9069\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 6 Batch 10/40] loss: 0.8887\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 6 Batch 20/40] loss: 0.9799\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 6 Batch 20/40] loss: 0.7965\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 6 Batch 20/40] loss: 0.9236\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 6 Batch 20/40] loss: 1.1100\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 6 Batch 20/40] loss: 1.2094\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 6 Batch 20/40] loss: 0.9742\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 6 Batch 20/40] loss: 1.0990\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 6 Batch 20/40] loss: 0.9150\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 6 Batch 20/40] loss: 1.1108\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 6 Batch 20/40] loss: 0.8376\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 6 Batch 20/40] loss: 0.9059\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 6 Batch 20/40] loss: 1.0370\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 6 Batch 20/40] loss: 1.0416\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 6 Batch 20/40] loss: 0.8712\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 6 Batch 20/40] loss: 0.8292\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 6 Batch 20/40] loss: 1.0214\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 6 Batch 30/40] loss: 1.0380\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 6 Batch 30/40] loss: 0.9553\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 6 Batch 30/40] loss: 0.9314\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 6 Batch 30/40] loss: 0.6501\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 6 Batch 30/40] loss: 1.0437\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 6 Batch 30/40] loss: 1.0671\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 6 Batch 30/40] loss: 1.0998\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 6 Batch 30/40] loss: 1.3998\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 6 Batch 30/40] loss: 1.5864\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 6 Batch 30/40] loss: 0.6078\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 6 Batch 30/40] loss: 0.4576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 6 Batch 30/40] loss: 0.9496\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 6 Batch 30/40] loss: 0.8460\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 6 Batch 30/40] loss: 1.1418\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 6 Batch 30/40] loss: 0.9356\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 6 Batch 30/40] loss: 0.8410\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 6 Batch 40/40] loss: 1.2205\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 6 Batch 40/40] loss: 0.8208\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 6 Batch 40/40] loss: 1.2650\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 6 Batch 40/40] loss: 0.8799\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 6 Batch 40/40] loss: 1.2611\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 6 Batch 40/40] loss: 1.0494\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 6 Batch 40/40] loss: 0.6774\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 6 Batch 40/40] loss: 0.8779\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 6 Batch 40/40] loss: 0.9014\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 6 Batch 40/40] loss: 0.6783\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 6 Batch 40/40] loss: 1.0747\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 6 Batch 40/40] loss: 0.8193\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 6 Batch 40/40] loss: 0.8696\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 6 Batch 40/40] loss: 0.7424\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 6 Batch 40/40] loss: 0.8043\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 6 Batch 40/40] loss: 1.1644\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:=== Start Validation ===\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 6] trn_loss: 1.1928, vld_loss: 1.0828, score: 0.9207, score_each: [0.9061, 0.9523, 0.9184]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Model successfully saved at: /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 7 Batch 10/40] loss: 1.4967\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 7 Batch 10/40] loss: 1.0323\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 7 Batch 10/40] loss: 0.8450\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 7 Batch 10/40] loss: 1.2123\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 7 Batch 10/40] loss: 0.9500\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 7 Batch 10/40] loss: 0.7485\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 7 Batch 10/40] loss: 0.9177\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 7 Batch 10/40] loss: 1.2330\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 7 Batch 10/40] loss: 1.3065\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 7 Batch 10/40] loss: 0.9885\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 7 Batch 10/40] loss: 1.0279\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 7 Batch 10/40] loss: 0.6338\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 7 Batch 10/40] loss: 1.1181\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 7 Batch 10/40] loss: 1.3183\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 7 Batch 10/40] loss: 1.3304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 7 Batch 10/40] loss: 0.7835\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 7 Batch 20/40] loss: 1.1330\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 7 Batch 20/40] loss: 0.6516\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 7 Batch 20/40] loss: 1.2935\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 7 Batch 20/40] loss: 1.5678\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 7 Batch 20/40] loss: 1.1716\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 7 Batch 20/40] loss: 1.0978\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 7 Batch 20/40] loss: 1.4657\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 7 Batch 20/40] loss: 0.8197\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 7 Batch 20/40] loss: 0.3921\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 7 Batch 20/40] loss: 0.4155\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 7 Batch 20/40] loss: 0.9931\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 7 Batch 20/40] loss: 1.0740\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 7 Batch 20/40] loss: 1.0725\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 7 Batch 20/40] loss: 1.3468\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 7 Batch 20/40] loss: 0.7612\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 7 Batch 20/40] loss: 1.0616\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 7 Batch 30/40] loss: 1.4294\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 7 Batch 30/40] loss: 0.9596\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 7 Batch 30/40] loss: 1.0274\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 7 Batch 30/40] loss: 0.9727\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 7 Batch 30/40] loss: 1.2637\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 7 Batch 30/40] loss: 0.9770\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 7 Batch 30/40] loss: 0.7437\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 7 Batch 30/40] loss: 0.8182\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 7 Batch 30/40] loss: 1.6133\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 7 Batch 30/40] loss: 1.2071\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 7 Batch 30/40] loss: 1.0068\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 7 Batch 30/40] loss: 0.6952\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 7 Batch 30/40] loss: 1.0874\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 7 Batch 30/40] loss: 1.0983\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 7 Batch 30/40] loss: 1.2044\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 7 Batch 30/40] loss: 0.5904\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 7 Batch 40/40] loss: 1.1614\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 7 Batch 40/40] loss: 0.4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 7 Batch 40/40] loss: 0.7859\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 7 Batch 40/40] loss: 0.6482\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 7 Batch 40/40] loss: 1.5115\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 7 Batch 40/40] loss: 1.6839\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 7 Batch 40/40] loss: 1.0934\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 7 Batch 40/40] loss: 0.6690\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 7 Batch 40/40] loss: 0.7224\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 7 Batch 40/40] loss: 0.9491\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 7 Batch 40/40] loss: 1.0543\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 7 Batch 40/40] loss: 0.8431\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 7 Batch 40/40] loss: 0.7024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 7 Batch 40/40] loss: 0.5212\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 7 Batch 40/40] loss: 0.8037\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 7 Batch 40/40] loss: 0.7460\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:=== Start Validation ===\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[1,0]<stdout>:[Epoch 7] trn_loss: 1.2584, vld_loss: 1.1216, score: 0.9253, score_each: [0.9136, 0.9595, 0.9145]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Model successfully saved at: /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 8 Batch 10/40] loss: 0.9226\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 8 Batch 10/40] loss: 0.9002\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 8 Batch 10/40] loss: 0.8841\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 8 Batch 10/40] loss: 0.9265\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 8 Batch 10/40] loss: 1.2898\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 8 Batch 10/40] loss: 1.0144\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 8 Batch 10/40] loss: 1.2343\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 8 Batch 10/40] loss: 1.2575\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 8 Batch 10/40] loss: 0.6322\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 8 Batch 10/40] loss: 0.8864\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 8 Batch 10/40] loss: 0.9004\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 8 Batch 10/40] loss: 1.0486\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 8 Batch 10/40] loss: 1.0554\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 8 Batch 10/40] loss: 1.0144\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 8 Batch 10/40] loss: 1.0987\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 8 Batch 10/40] loss: 1.0407\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 8 Batch 20/40] loss: 0.7691\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 8 Batch 20/40] loss: 0.7934\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 8 Batch 20/40] loss: 1.0394\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 8 Batch 20/40] loss: 0.7897\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 8 Batch 20/40] loss: 0.8522\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 8 Batch 20/40] loss: 0.6423\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 8 Batch 20/40] loss: 0.9294\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 8 Batch 20/40] loss: 1.3083\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 8 Batch 20/40] loss: 0.8267\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 8 Batch 20/40] loss: 0.9721\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 8 Batch 20/40] loss: 1.2091\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 8 Batch 20/40] loss: 0.8546\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 8 Batch 20/40] loss: 0.5971\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 8 Batch 20/40] loss: 1.0989\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 8 Batch 20/40] loss: 1.4706\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 8 Batch 20/40] loss: 0.8454\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 8 Batch 30/40] loss: 0.7862\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 8 Batch 30/40] loss: 0.8787\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 8 Batch 30/40] loss: 1.0809\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 8 Batch 30/40] loss: 1.5085\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 8 Batch 30/40] loss: 0.9977\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 8 Batch 30/40] loss: 1.4437\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 8 Batch 30/40] loss: 1.0565\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 8 Batch 30/40] loss: 0.3311\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 8 Batch 30/40] loss: 1.1599\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 8 Batch 30/40] loss: 0.8680\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 8 Batch 30/40] loss: 0.8080\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 8 Batch 30/40] loss: 0.9975\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 8 Batch 30/40] loss: 1.2222\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 8 Batch 30/40] loss: 0.8945\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 8 Batch 30/40] loss: 0.9495\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 8 Batch 30/40] loss: 1.0712\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 8 Batch 40/40] loss: 0.6968\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 8 Batch 40/40] loss: 1.1235\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 8 Batch 40/40] loss: 0.8339\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 8 Batch 40/40] loss: 0.9480\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 8 Batch 40/40] loss: 1.0637\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 8 Batch 40/40] loss: 1.0540\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 8 Batch 40/40] loss: 0.9852\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 8 Batch 40/40] loss: 1.2378\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 8 Batch 40/40] loss: 1.0926\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 8 Batch 40/40] loss: 0.5867\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 8 Batch 40/40] loss: 0.7484\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 8 Batch 40/40] loss: 1.9614\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 8 Batch 40/40] loss: 0.8521\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 8 Batch 40/40] loss: 1.1059\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 8 Batch 40/40] loss: 1.0667\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 8 Batch 40/40] loss: 0.8841\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:=== Start Validation ===\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 8] trn_loss: 0.9702, vld_loss: 1.1927, score: 0.9354, score_each: [0.9265, 0.9617, 0.9267]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Model successfully saved at: /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 9 Batch 10/40] loss: 0.6373\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 9 Batch 10/40] loss: 0.6021\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 9 Batch 10/40] loss: 0.8334\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 9 Batch 10/40] loss: 1.0476\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 9 Batch 10/40] loss: 0.5974\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 9 Batch 10/40] loss: 1.0450\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 9 Batch 10/40] loss: 1.3795\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 9 Batch 10/40] loss: 0.8973\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 9 Batch 10/40] loss: 0.8636\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 9 Batch 10/40] loss: 0.5641\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 9 Batch 10/40] loss: 0.7623\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 9 Batch 10/40] loss: 1.2202\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 9 Batch 10/40] loss: 0.9580\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 9 Batch 10/40] loss: 1.4253\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 9 Batch 10/40] loss: 1.0540\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 9 Batch 10/40] loss: 0.6148\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 9 Batch 20/40] loss: 0.9204\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 9 Batch 20/40] loss: 0.8075\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 9 Batch 20/40] loss: 0.9051\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 9 Batch 20/40] loss: 1.1028\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 9 Batch 20/40] loss: 0.7876\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 9 Batch 20/40] loss: 0.3706\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 9 Batch 20/40] loss: 0.7611\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 9 Batch 20/40] loss: 0.9598\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 9 Batch 20/40] loss: 0.9135\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 9 Batch 20/40] loss: 1.3879\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 9 Batch 20/40] loss: 0.7553\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 9 Batch 20/40] loss: 0.9297\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 9 Batch 20/40] loss: 1.2341\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 9 Batch 20/40] loss: 0.6693\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 9 Batch 20/40] loss: 1.0496\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 9 Batch 20/40] loss: 0.5893\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 9 Batch 30/40] loss: 0.7870\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 9 Batch 30/40] loss: 0.7954\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 9 Batch 30/40] loss: 1.0767\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 9 Batch 30/40] loss: 0.8945\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 9 Batch 30/40] loss: 1.8013\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 9 Batch 30/40] loss: 0.9794\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 9 Batch 30/40] loss: 0.4093\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 9 Batch 30/40] loss: 1.1162\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 9 Batch 30/40] loss: 1.1069\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 9 Batch 30/40] loss: 0.9803\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 9 Batch 30/40] loss: 0.7006\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 9 Batch 30/40] loss: 0.9371\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 9 Batch 30/40] loss: 1.0277\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 9 Batch 30/40] loss: 0.7679\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 9 Batch 30/40] loss: 1.2132\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 9 Batch 30/40] loss: 0.4899\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[Epoch 9 Batch 40/40] loss: 1.0851\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[Epoch 9 Batch 40/40] loss: 0.6466\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[Epoch 9 Batch 40/40] loss: 1.3776\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[Epoch 9 Batch 40/40] loss: 1.3032\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[Epoch 9 Batch 40/40] loss: 0.8661\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[Epoch 9 Batch 40/40] loss: 1.1682\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[Epoch 9 Batch 40/40] loss: 1.6885\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[Epoch 9 Batch 40/40] loss: 1.0797\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[Epoch 9 Batch 40/40] loss: 0.5402\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[Epoch 9 Batch 40/40] loss: 1.0758\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[Epoch 9 Batch 40/40] loss: 1.3986\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[Epoch 9 Batch 40/40] loss: 0.6498\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[Epoch 9 Batch 40/40] loss: 0.2931\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 9 Batch 40/40] loss: 1.1259\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[Epoch 9 Batch 40/40] loss: 1.1773\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[Epoch 9 Batch 40/40] loss: 0.9199\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:=== Start Validation ===\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[Epoch 9] trn_loss: 0.7736, vld_loss: 1.1889, score: 0.9403, score_each: [0.9217, 0.9611, 0.9567]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Model successfully saved at: /opt/ml/model\u001b[0m\n",
      "\u001b[35m2021-03-02 06:30:41,622 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,10.0.207.154' (ECDSA) to the list of known hosts.#015\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:#015  0%|          | 0.00/44.7M [00:00<?, ?B/s][1,1]<stderr>:#015  0%|          | 0.00/44.7M [00:00<?, ?B/s][1,7]<stderr>:#015  0%|          | 0.00/44.7M [00:00<?, ?B/s][1,4]<stderr>:#015  0%|          | 0.00/44.7M [00:00<?, ?B/s][1,6]<stderr>:#015  0%|          | 0.00/44.7M [00:00<?, ?B/s][1,5]<stderr>:#015 49%|████▊     | 21.8M/44.7M [00:00<00:00, 228MB/s][1,1]<stderr>:#015 12%|█▏        | 5.46M/44.7M [00:00<00:00, 57.2MB/s][1,2]<stderr>:Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015 51%|█████     | 22.6M/44.7M [00:00<00:00, 237MB/s][1,4]<stderr>:#015 49%|████▉     | 22.0M/44.7M [00:00<00:00, 231MB/s][1,6]<stderr>:#015 35%|███▍      | 15.4M/44.7M [00:00<00:00, 162MB/s][1,2]<stderr>:#015  0%|          | 0.00/44.7M [00:00<?, ?B/s][1,3]<stderr>:#015  0%|          | 0.00/44.7M [00:00<?, ?B/s][1,5]<stderr>:#015 99%|█████████▉| 44.1M/44.7M [00:00<00:00, 230MB/s][1,1]<stderr>:#015 24%|██▍       | 10.7M/44.7M [00:00<00:00, 56.4MB/s][1,5]<stderr>:#015100%|██████████| 44.7M/44.7M [00:00<00:00, 230MB/s]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015 87%|████████▋ | 38.7M/44.7M [00:00<00:00, 211MB/s][1,4]<stderr>:#015100%|█████████▉| 44.6M/44.7M [00:00<00:00, 233MB/s][1,6]<stderr>:#015 71%|███████▏  | 31.9M/44.7M [00:00<00:00, 165MB/s][1,4]<stderr>:#015100%|██████████| 44.7M/44.7M [00:00<00:00, 232MB/s]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:#015  9%|▉         | 4.02M/44.7M [00:00<00:01, 42.0MB/s][1,3]<stderr>:#015 32%|███▏      | 14.5M/44.7M [00:00<00:00, 152MB/s][1,7]<stderr>:#015100%|██████████| 44.7M/44.7M [00:00<00:00, 200MB/s]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:#015100%|██████████| 44.7M/44.7M [00:00<00:00, 170MB/s]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:#015 52%|█████▏    | 23.4M/44.7M [00:00<00:00, 68.2MB/s][1,2]<stderr>:#015 18%|█▊        | 7.97M/44.7M [00:00<00:00, 41.8MB/s][1,3]<stderr>:#015 69%|██████▉   | 30.9M/44.7M [00:00<00:00, 157MB/s][1,1]<stderr>:#015 85%|████████▍ | 37.8M/44.7M [00:00<00:00, 81.6MB/s][1,3]<stderr>:#015100%|██████████| 44.7M/44.7M [00:00<00:00, 166MB/s]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:#015 45%|████▍     | 19.9M/44.7M [00:00<00:00, 52.3MB/s][1,1]<stderr>:#015100%|██████████| 44.7M/44.7M [00:00<00:00, 105MB/s] \u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:#015 78%|███████▊  | 34.7M/44.7M [00:00<00:00, 65.2MB/s][1,2]<stderr>:#015100%|██████████| 44.7M/44.7M [00:00<00:00, 102MB/s] \u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:#015  0%|          | 0.00/44.7M [00:00<?, ?B/s][1,14]<stderr>:Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:#015  0%|          | 0.00/44.7M [00:00<?, ?B/s][1,12]<stderr>:#015 37%|███▋      | 16.4M/44.7M [00:00<00:00, 172MB/s][1,8]<stderr>:Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  0%|          | 0.00/44.7M [00:00<?, ?B/s][1,14]<stderr>:#015 24%|██▍       | 10.8M/44.7M [00:00<00:00, 114MB/s][1,12]<stderr>:#015 74%|███████▍  | 33.2M/44.7M [00:00<00:00, 173MB/s][1,8]<stderr>:#015 32%|███▏      | 14.4M/44.7M [00:00<00:00, 150MB/s][1,12]<stderr>:#015100%|██████████| 44.7M/44.7M [00:00<00:00, 173MB/s]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:#015 76%|███████▋  | 34.1M/44.7M [00:00<00:00, 135MB/s][1,8]<stderr>:#015 65%|██████▌   | 29.1M/44.7M [00:00<00:00, 152MB/s][1,14]<stderr>:#015100%|██████████| 44.7M/44.7M [00:00<00:00, 192MB/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015100%|██████████| 44.7M/44.7M [00:00<00:00, 158MB/s]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:__main__:=== Start Validation ===\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:__main__:Model successfully saved at: /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:__main__:=== Start Validation ===\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:__main__:Model successfully saved at: /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:__main__:=== Start Validation ===\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:__main__:Model successfully saved at: /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:__main__:=== Start Validation ===\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:__main__:Model successfully saved at: /opt/ml/model\u001b[0m\n",
      "\u001b[34m2021-03-02 06:30:41,609 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:__main__:=== Start Validation ===\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:__main__:Model successfully saved at: /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:__main__:=== Start Validation ===\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:__main__:Model successfully saved at: /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:__main__:=== Start Validation ===\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:__main__:Model successfully saved at: /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:__main__:=== Start Validation ===\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:__main__:Model successfully saved at: /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:__main__:=== Start Validation ===\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:__main__:Model successfully saved at: /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:__main__:=== Start Validation ===\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:__main__:Model successfully saved at: /opt/ml/model\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-03-02 06:31:20 Uploading - Uploading generated training model\u001b[35m2021-03-02 06:31:11,652 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[35m2021-03-02 06:31:11,653 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-03-02 06:31:40 Completed - Training job completed\n",
      "ProfilerReport-1614665057: IssuesFound\n",
      "Training seconds: 2786\n",
      "Billable seconds: 2786\n",
      "CPU times: user 4.51 s, sys: 272 ms, total: 4.78 s\n",
      "Wall time: 27min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator.fit(s3_input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-143656149352/pytorch-smdataparallel-bangali-handwrit-2021-03-02-06-04-16-979/output/\n",
      "2021-03-02 06:31:22   41940706 model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_model_dir = estimator.model_data.replace('model.tar.gz', '')\n",
    "print(s3_model_dir)\n",
    "!aws s3 ls {s3_model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Getting Model artifacts\n",
    "---\n",
    "\n",
    "훈련이 완료된 모델 아티팩트를 로컬(jupyter notebook 인스턴스 or 온프레미스)로 복사합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_dir = './model_ddp'\n",
    "!rm -rf $local_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-143656149352/pytorch-smdataparallel-bangali-handwrit-2021-03-02-06-04-16-979/output/model.tar.gz to model_ddp/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "\n",
    "if not os.path.exists(local_model_dir):\n",
    "    os.makedirs(local_model_dir)\n",
    "\n",
    "!aws s3 cp {s3_model_dir}model.tar.gz {local_model_dir}/model.tar.gz\n",
    "!tar -xzf {local_model_dir}/model.tar.gz -C {local_model_dir}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
