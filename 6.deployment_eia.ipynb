{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 6. Amazon SageMaker Deployment for EIA(Elastic Inference Accelerator)\n",
    "\n",
    "---\n",
    "\n",
    "***[주의] 본 모듈은 PyTorch EIA 1.3.1 버전에서 훈련을 수행한 모델만 배포가 가능합니다. 코드가 정상적으로 수행되지 않는다면, 프레임워크 버전을 동일 버전으로 맞춰 주시기 바랍니다.***\n",
    "\n",
    "본 모듈에서는 Elastic Inference Accelerator(EIA)를 사용하여 모델을 배포해 보겠습니다.\n",
    "\n",
    "### Elastic Inference Accelerator\n",
    "훈련 인스턴스와 달리 실시간 추론 인스턴스는 계속 상시로 띄우는 경우가 많기에, 딥러닝 어플리케이션에서 low latency를 위해 GPU 인스턴스를 사용하면 많은 비용이 발생합니다.\n",
    "\n",
    "Amazon Elastic Inference는 저렴하고 메모리가 작은 GPU 기반 가속기를 Amazon EC2, Amazon ECS, Amazon SageMaker에 연결할 수 있는 서비스로, Accelerator가 CPU 인스턴스에 프로비저닝되고 연결됩니다. EIA를 사용하면 GPU 인스턴스에 근접한 퍼포먼스를 보이면서 인스턴스 실행 비용을 최대 75%까지 절감할 수 있습니다. \n",
    "\n",
    "모든 Amazon SageMaker 인스턴스 유형, EC2 인스턴스 유형 또는 Amazon ECS 작업을 지원하며, 대부분의 딥러닝 프레임워크를 지원하고 있습니다. 지원되는 프레임워크 버전은 AWS CLI로 확인할 수 있습니다.\n",
    "\n",
    "```bash\n",
    "$ aws ecr list-images --repository-name tensorflow-inference-eia --registry-id 763104351884\n",
    "$ aws ecr list-images --repository-name pytorch-inference-eia --registry-id 763104351884\n",
    "$ aws ecr list-images --repository-name mxnet-inference-eia --registry-id 763104351884\n",
    "```\n",
    "\n",
    "참조: https://aws.amazon.com/ko/blogs/korea/amazon-elastic-inference-gpu-powered-deep-learning-inference-acceleration/\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1. Inference script\n",
    "---\n",
    "\n",
    "아래 코드 셀은 `src` 디렉토리에 SageMaker 추론 스크립트인 `inference_eia.py`를 저장합니다.<br>\n",
    "Module 5의 코드와 대부분 동일하지만, `model_fn()` 메서드의 구현이 다른 점을 유의해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/inference_eia.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/inference_eia.py\n",
    "\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import io\n",
    "import tarfile\n",
    "\n",
    "import boto3\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import copy\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch import topk\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "JSON_CONTENT_TYPE = 'application/json'\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    logger.info(\"==> model_dir : {}\".format(model_dir))\n",
    "    traced_model = torch.jit.load(os.path.join(model_dir, 'model_eia.pth'))\n",
    "    return traced_model\n",
    "\n",
    "\n",
    "# Deserialize the request body\n",
    "def input_fn(request_body, request_content_type='application/x-image'):\n",
    "    print('An input_fn that loads a image tensor')\n",
    "    print(request_content_type)\n",
    "    if request_content_type == 'application/x-image':             \n",
    "        img = np.array(Image.open(io.BytesIO(request_body)))\n",
    "    elif request_content_type == 'application/x-npy':    \n",
    "        img = np.frombuffer(request_body, dtype='uint8').reshape(137, 236)   \n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'Requested unsupported ContentType in content_type : ' + request_content_type)\n",
    "\n",
    "    img = 255 - img\n",
    "    img = img[:,:,np.newaxis]\n",
    "    img = np.repeat(img, 3, axis=2)    \n",
    "\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    img_tensor = test_transforms(img)\n",
    "\n",
    "    return img_tensor         \n",
    "        \n",
    "\n",
    "# Predicts on the deserialized object with the model from model_fn()\n",
    "def predict_fn(input_data, model):\n",
    "    logger.info('Entering the predict_fn function')\n",
    "    start_time = time.time()\n",
    "    input_data = input_data.unsqueeze(0)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    input_data = input_data.to(device)\n",
    "                          \n",
    "    result = {}\n",
    "                                                 \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_data)\n",
    "        pred_probs = F.softmax(logits, dim=1).data.squeeze()   \n",
    "        outputs = topk(pred_probs, 5)                  \n",
    "        result['score'] = outputs[0].detach().cpu().numpy()\n",
    "        result['class'] = outputs[1].detach().cpu().numpy()\n",
    "    \n",
    "    print(\"--- Elapsed time: %s secs ---\" % (time.time() - start_time))    \n",
    "    return result        \n",
    "\n",
    "\n",
    "# Serialize the prediction result into the response content type\n",
    "def output_fn(pred_output, accept=JSON_CONTENT_TYPE):\n",
    "    return json.dumps({'score': pred_output['score'].tolist(), \n",
    "                       'class': pred_output['class'].tolist()}), accept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. TorchScript Compile (Tracing)\n",
    "---\n",
    "\n",
    "PyTorch 프레임워크에서 EI를 사용하기 위해서는 [TorchScript](https://pytorch.org/docs/1.3.1/jit.html)로 모델을 컴파일해야 하며, 2020년 8월 시점에서는 PyTorch 1.3.1을 지원하고 있습니다. TorchScript는 PyTorch 코드에서 직렬화 및 최적화 가능한 모델로 컴파일하며 Python 인터프리터의 글로벌 인터프리터 잠금 (GIL)과 무관하기 때문에 Python 외의 언어에서 로드 가능하고  최적화가 용이합니다.\n",
    "\n",
    "TorchScript로 변환하는 방법은 **tracing** 방식과 **scripting** 방식이 있으며, 본 핸즈온에서는 tracing 방식을 사용하겠습니다. <br>\n",
    "참고로 tracing 방식은 샘플 입력 데이터를 모델에 입력 후 그 입력의 흐름(feedforward)을 기록하여 포착하는 메커니즘이며, scripting 방식은 모델 코드를 직접 분석해서 컴파일하는 방식입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip --trusted-host pypi.org --trusted-host files.pythonhosted.org\n",
    "!{sys.executable} -m pip install https://download.pytorch.org/whl/cpu/torchvision-0.4.2%2Bcpu-cp36-cp36m-linux_x86_64.whl\n",
    "!{sys.executable} -m pip install https://s3.amazonaws.com/amazonei-pytorch/torch_eia-1.3.1-cp36-cp36m-manylinux1_x86_64.whl\n",
    "!{sys.executable} -m pip install graphviz==0.13.2   \n",
    "!{sys.executable} -m pip install mxnet-model-server==1.0.8\n",
    "!{sys.executable} -m pip install pillow==7.1.0\n",
    "!{sys.executable} -m pip install sagemaker_containers\n",
    "!{sys.executable} -m pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile\n",
    "\n",
    "Tracing 방식은 특정 input을 모델에 적용했을 때 수행되면서 operation이 저장하기 때문에, 이미지 사이즈와 동일한 크기의 랜덤 입력 데이터를 모델을 적용해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> model_dir : ./model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, os\n",
    "from torchvision import models\n",
    "model_dir = './model'\n",
    "print(\"==> model_dir : {}\".format(model_dir))\n",
    "model = models.resnet18(pretrained=True)\n",
    "last_hidden_units = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(last_hidden_units, 186)\n",
    "model.load_state_dict(torch.load(os.path.join(model_dir, 'model.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data = torch.rand(1,3,137,236)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "input_data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/p4clients/pkgbuild-kCleo/workspace/build/PyTorchECL/PyTorchECL-1.x.548.0/AL2012/DEV.STD.PTHREAD/build/private/src/torch/csrc/jit/eia/eia_interface.h:52: UserWarning: Notice - No last error found\n",
      "/local/p4clients/pkgbuild-kCleo/workspace/build/PyTorchECL/PyTorchECL-1.x.548.0/AL2012/DEV.STD.PTHREAD/build/private/src/torch/csrc/jit/eia/eia_interface.h:52: UserWarning: Notice - No last error found\n"
     ]
    }
   ],
   "source": [
    "with torch.jit.optimized_execution(True, {'target_device': 'eia:0'}): \n",
    "    traced_model = torch.jit.trace(model, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컴파일한 모델로 로컬 환경에서 추론을 수행해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An input_fn that loads a image tensor\n",
      "application/x-image\n",
      "Entering the predict_fn function\n",
      "--- Elapsed time: 0.023025035858154297 secs ---\n",
      "{'score': array([0.62198836, 0.2314413 , 0.04159953, 0.02067479, 0.01897352],\n",
      "      dtype=float32), 'class': array([  3,   2, 169, 168,  70])}\n"
     ]
    }
   ],
   "source": [
    "from src.inference_eia import model_fn, input_fn, predict_fn, output_fn\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "file_path = 'test_imgs/test_0.jpg'\n",
    "with open(file_path, mode='rb') as file:\n",
    "    img_byte = bytearray(file.read())\n",
    "data = input_fn(img_byte)\n",
    "result = predict_fn(data, traced_model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TorchScript 모델을 파일로 직렬화하여 저장합니다. 그런 다음, `tar.gz`로 압축하고 이 파일을 S3로 복사합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(traced_model, './model/model_eia.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_eia.pt\n"
     ]
    }
   ],
   "source": [
    "tar_filename = 'model_eia.tar.gz'\n",
    "!cd model/ && tar -czvf $tar_filename model_eia.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: model/model_eia.tar.gz to s3://sagemaker-us-east-1-143656149352/pytorch-training-2020-08-16-04-47-36-618/output/model_eia.tar.gz\n"
     ]
    }
   ],
   "source": [
    "artifacts_dir = 's3://sagemaker-us-east-1-143656149352/pytorch-training-2020-08-16-04-47-36-618/output/'\n",
    "!aws s3 cp model/$tar_filename $artifacts_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. SageMaker Hosted Endpoint Inference\n",
    "---\n",
    "\n",
    "SageMaker가 관리하는 배포 클러스터를 프로비저닝하는 시간이 소요되기 때문에 추론 서비스를 시작하는 데에는 약 5~10분 정도 소요됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "client = boto3.client('sagemaker')\n",
    "runtime_client = boto3.client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_path(sm_client, max_results=1, name_contains='pytorch'):\n",
    "    training_job = sm_client.list_training_jobs(MaxResults=max_results,\n",
    "                                         NameContains=name_contains,\n",
    "                                         SortBy='CreationTime', \n",
    "                                         SortOrder='Descending')\n",
    "    training_job_name = training_job['TrainingJobSummaries'][0]['TrainingJobName']\n",
    "    training_job_description = sm_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "    model_path = training_job_description['ModelArtifacts']['S3ModelArtifacts']  \n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-143656149352/pytorch-training-2020-08-16-04-47-36-618/output/model_eia.tar.gz\n"
     ]
    }
   ],
   "source": [
    "#model_path = get_model_path(client, max_results=3)\n",
    "model_path = os.path.join(artifacts_dir, tar_filename)\n",
    "print(model_path)\n",
    "endpoint_name = \"endpoint-bangali-classifier-eia-{}\".format(int(time.time()))\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data=model_path,\n",
    "                                   role=role,\n",
    "                                   entry_point='./src/inference_eia.py',\n",
    "                                   framework_version='1.3.1',\n",
    "                                   py_version='py3')\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type='ml.c5.large', \n",
    "                                 initial_instance_count=1, \n",
    "                                 accelerator_type='ml.eia2.large', \n",
    "                                 endpoint_name=endpoint_name,\n",
    "                                 wait=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = boto3.client('sagemaker')\n",
    "# waiter = client.get_waiter('endpoint_in_service')\n",
    "# waiter.wait(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointName': 'endpoint-bangali-classifier-eia-1597846677',\n",
       " 'EndpointArn': 'arn:aws:sagemaker:us-east-1:143656149352:endpoint/endpoint-bangali-classifier-eia-1597846677',\n",
       " 'EndpointConfigName': 'endpoint-bangali-classifier-eia-1597846677',\n",
       " 'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "   'DeployedImages': [{'SpecifiedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference-eia:1.3.1-cpu-py3',\n",
       "     'ResolvedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference-eia@sha256:fa623bbda1358a0b50f89820f2ea3d9f1331ab5336158f510ea267088ef670be',\n",
       "     'ResolutionTime': datetime.datetime(2020, 8, 19, 14, 18, 3, 917000, tzinfo=tzlocal())}],\n",
       "   'CurrentWeight': 1.0,\n",
       "   'DesiredWeight': 1.0,\n",
       "   'CurrentInstanceCount': 1,\n",
       "   'DesiredInstanceCount': 1}],\n",
       " 'EndpointStatus': 'InService',\n",
       " 'CreationTime': datetime.datetime(2020, 8, 19, 14, 18, 2, 136000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2020, 8, 19, 14, 25, 20, 172000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': 'daa07aba-e0f2-4e02-800f-aefd1c91d371',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'daa07aba-e0f2-4e02-800f-aefd1c91d371',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '787',\n",
       "   'date': 'Wed, 19 Aug 2020 14:25:35 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "client = boto3.client('sagemaker')\n",
    "runtime_client = boto3.client('sagemaker-runtime')\n",
    "endpoint_name = pytorch_model.endpoint_name\n",
    "client.describe_endpoint(EndpointName = endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추론을 수행합니다. (`ContentType='application/x-image'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"score\": [0.6219883561134338, 0.23144130408763885, 0.04159948602318764, 0.02067478932440281, 0.018973516300320625], \"class\": [3, 2, 169, 168, 70]}\n"
     ]
    }
   ],
   "source": [
    "with open(file_path, mode='rb') as file:\n",
    "    img_byte = bytearray(file.read())\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType='application/x-image',\n",
    "    Accept='application/json',\n",
    "    Body=img_byte\n",
    "    )\n",
    "print(response['Body'].read().decode())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.1 ms ± 6.12 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit runtime_client.invoke_endpoint(EndpointName=endpoint_name, ContentType='application/x-image', Accept='application/json', Body=img_byte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Hosted Endpoint Clean-up\n",
    "\n",
    "엔드포인트를 계속 사용하지 않는다면, 불필요한 과금을 피하기 위해 엔드포인트를 삭제해야 합니다. \n",
    "SageMaker SDK에서는 `delete_endpoint()` 메소드로 간단히 삭제할 수 있으며, UI에서도 쉽게 삭제할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_endpoint(client, endpoint_name):\n",
    "    response = client.describe_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "    model_name = response['ProductionVariants'][0]['ModelName']\n",
    "\n",
    "    client.delete_model(ModelName=model_name)    \n",
    "    client.delete_endpoint(EndpointName=endpoint_name)\n",
    "    client.delete_endpoint_config(EndpointConfigName=endpoint_name)    \n",
    "    \n",
    "    print(f'--- Deleted model: {model_name}')\n",
    "    print(f'--- Deleted endpoint: {endpoint_name}')\n",
    "    print(f'--- Deleted endpoint_config: {endpoint_name}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_endpoint(client, endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p36",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
