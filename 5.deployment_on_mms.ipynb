{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5. Amazon SageMaker Deployment \n",
    "---\n",
    "\n",
    "본 모듈에서는 SageMaker에서 호스팅 엔드포인트를 배포하는 법을 알아봅니다. \n",
    "\n",
    "\n",
    "### AWS Managed Inference Container\n",
    "SageMaker 추론은 각 프레임워크별에 적합한 배포 컨테이너들이 사전에 빌드되어 있으며, TensorFlow는 텐서플로 서빙, 파이토치는 torchserve, MXNet은 MMS(Multi Model Server), scikit learn은 Flask가 내장되어 있습니다. PyTorch, 기존에는 MMS가 내장되어 있었지만, 2020년 말부터 Amazon과 facebook이 공동으로 개발한 torchserve를 내장하기 시작했습니다. \n",
    "\n",
    "배포 컨테이너를 구동할 때에는 추론을 위한 http 요청을 받아들일 수 있는 RESTful API를 실행하는 serve 명령어가 자동으로 실행되면서 엔드포인트가 시작됩니다. 엔드포인트를 시작할 때, SageMaker는 도커 컨테이너에서 사용 가능한 외부의 모델 아티팩트, 데이터, 그리고 기타 환경 설정 정보 등을 배포 인스턴스의 /opt/ml 폴더로 로딩합니다. \n",
    "\n",
    "![container](imgs/inference_container.png)\n",
    "\n",
    "도커 파일은 오픈 소스로 공개되어 있으며, AWS에서는 구 버전부터 최신 버전까지 다양한 버전을 제공하고 있습니다.\n",
    "각 프레임워크의 도커 파일은 아래 링크를 참조하십시오.\n",
    "\n",
    "- TensorFlow containes: https://github.com/aws/sagemaker-tensorflow-containers \n",
    "- PyTorch container: https://github.com/aws/sagemaker-pytorch-container   \n",
    "- MXNet containes: https://github.com/aws/sagemaker-mxnet-containers\n",
    "- Chainer container: https://github.com/aws/sagemaker-chainer-container \n",
    "- Scikit-learn container: https://github.com/aws/sagemaker-scikit-learn-container\n",
    "- SparkML serving container: https://github.com/aws/sagemaker-sparkml-serving-container\n",
    "\n",
    "또한, AWS CLI를 사용하여 프레임워크별로 지원되는 버전을 간단하게 확인 가능합니다.\n",
    "\n",
    "```sh\n",
    "$ aws ecr list-images --repository-name tensorflow-inference --registry-id 76310435188\n",
    "$ aws ecr list-images --repository-name pytorch-inference --registry-id 763104351884\n",
    "$ aws ecr list-images --repository-name mxnet-inference --registry-id 763104351884\n",
    "\n",
    "# EIA(Elastic Inference)\n",
    "$ aws ecr list-images --repository-name tensorflow-inference-eia --registry-id 763104351884\n",
    "$ aws ecr list-images --repository-name pytorch-inference-eia --registry-id 763104351884\n",
    "$ aws ecr list-images --repository-name mxnet-inference-eia --registry-id 763104351884\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1. Inference script\n",
    "---\n",
    "\n",
    "아래 코드 셀은 `src` 디렉토리에 SageMaker 추론 스크립트인 `inference.py`를 저장합니다.<br>\n",
    "\n",
    "이 스크립트는 SageMaker 상에서 호스팅 엔드포인트를 쉽게 배포할 수 이는 high-level 툴킷인 SageMaker inference toolkit의 인터페이스를\n",
    "사용하고 있으며, 여러분께서는 인터페이스에 정의된 핸들러(handler) 함수들만 구현하시면 됩니다. 아래 인터페이스는 텐서플로를 제외한 프레임워크들에서 공용으로 사용됩니다. \n",
    "- `model_fn()`: S3나 model zoo에 저장된 모델을 추론 인스턴스의 메모리로 로드 후, 모델을 리턴하는 방법을 정의하는 전처리 함수입니다.\n",
    "- `input_fn()`: 사용자로부터 입력받은 내용을 모델 추론에 적합하게 변환하는 전처리 함수로, content_type 인자값을 통해 입력값 포맷을 확인할 수 있습니다.\n",
    "- `predict_fn()`: model_fn()에서 리턴받은 모델과 input_fn()에서 변환된 데이터로 추론을 수행합니다.\n",
    "- `output_fn()`: 추론 결과를 반환하는 후처리 함수입니다.\n",
    "\n",
    "Tip: `input_fn(), predict_fn(), output_fn()`을 각각 구현하는 대신, 세 함수들을 한꺼번에 묶어서 `transform()` 함수에 구현하는 것도 가능합니다. 아래 Code snippet 예시를 참조하십시오.\n",
    "\n",
    "```python\n",
    "# Option 1\n",
    "def model_fn(model_dir):\n",
    "    model = Your_Model()\n",
    "    return model\n",
    "\n",
    "def input_fn(request_body, content_type):\n",
    "    if content_type == 'text/csv'\n",
    "        ...\n",
    "    else:\n",
    "        pass:\n",
    "        \n",
    "def predict_fn(request_body, content_type):\n",
    "    # Preform prediction\n",
    "    return model(input_data)\n",
    "      \n",
    "def output_fn(prediction, content_type):\n",
    "    # Serialize the prediction result \n",
    "```\n",
    "\n",
    "```python\n",
    "# Option 2\n",
    "def model_fn(model_dir):\n",
    "    model = Your_Model()\n",
    "    return model\n",
    "\n",
    "def transform_fn(model, input_data, content_type, accept):\n",
    "    # All-in-one function, including input_fn, predict_fn(), and output_fn()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker 훈련 컨테이너에서 1.6.0을 사용하였기 때문에, 로컬 추론 테스트 시에도 동일한 버전으로 추론합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (1.6.0)\r\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch==1.6.0) (1.19.5)\r\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch==1.6.0) (0.18.2)\r\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "!pip install torch==1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/inference.py\n",
    "\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import io\n",
    "import tarfile\n",
    "\n",
    "import boto3\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import copy\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch import topk\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "JSON_CONTENT_TYPE = 'application/json'\n",
    "\n",
    "# Loads the model into memory from storage and return the model.\n",
    "def model_fn(model_dir):\n",
    "    logger.info(\"==> model_dir : {}\".format(model_dir))\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    last_hidden_units = model.fc.in_features\n",
    "    model.fc = torch.nn.Linear(last_hidden_units, 186)\n",
    "    model.load_state_dict(torch.load(os.path.join(model_dir, 'model.pt')))\n",
    "    return model\n",
    "\n",
    "# Deserialize the request body\n",
    "def input_fn(request_body, request_content_type='application/x-image'):\n",
    "    print('An input_fn that loads a image tensor')\n",
    "    print(request_content_type)\n",
    "    if request_content_type == 'application/x-image':             \n",
    "        img = np.array(Image.open(io.BytesIO(request_body)))\n",
    "    elif request_content_type == 'application/x-npy':    \n",
    "        img = np.frombuffer(request_body, dtype='uint8').reshape(137, 236)   \n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'Requested unsupported ContentType in content_type : ' + request_content_type)\n",
    "\n",
    "    img = 255 - img\n",
    "    img = img[:,:,np.newaxis]\n",
    "    img = np.repeat(img, 3, axis=2)    \n",
    "\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    img_tensor = test_transforms(img)\n",
    "\n",
    "    return img_tensor         \n",
    "        \n",
    "\n",
    "# Predicts on the deserialized object with the model from model_fn()\n",
    "def predict_fn(input_data, model):\n",
    "    logger.info('Entering the predict_fn function')\n",
    "    start_time = time.time()\n",
    "    input_data = input_data.unsqueeze(0)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    input_data = input_data.to(device)\n",
    "                          \n",
    "    result = {}\n",
    "                                                 \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_data)\n",
    "        pred_probs = F.softmax(logits, dim=1).data.squeeze()   \n",
    "        outputs = topk(pred_probs, 5)                  \n",
    "        result['score'] = outputs[0].detach().cpu().numpy()\n",
    "        result['class'] = outputs[1].detach().cpu().numpy()\n",
    "    \n",
    "    print(\"--- Elapsed time: %s secs ---\" % (time.time() - start_time))    \n",
    "    return result        \n",
    "\n",
    "# Serialize the prediction result into the response content type\n",
    "def output_fn(pred_output, accept=JSON_CONTENT_TYPE):\n",
    "    return json.dumps({'score': pred_output['score'].tolist(), \n",
    "                       'class': pred_output['class'].tolist()}), accept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Local Endpoint Inference\n",
    "---\n",
    "\n",
    "충분한 검증 및 테스트 없이 훈련된 모델을 곧바로 실제 운영 환경에 배포하기에는 많은 위험 요소들이 있습니다. 따라서, 로컬 모드를 사용하여 실제 운영 환경에 배포하기 위한 추론 인스턴스를 시작하기 전에 노트북 인스턴스의 로컬 환경에서 모델을 배포하는 것을 권장합니다. 이를 로컬 모드 엔드포인트(Local Mode Endpoint)라고 합니다.\n",
    "\n",
    "먼저, 로컬 모드 엔드포인트의 컨테이너 배포 이전에 로컬 환경 상에서 직접 추론을 수행하여 결과를 확인하고, 곧바로 로컬 모드 엔드포인트를 배포해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`content_type='application/x-image'` 일 경우 추론을 수행하는 예시입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An input_fn that loads a image tensor\n",
      "application/x-image\n",
      "==> model_dir : ./model\n",
      "Entering the predict_fn function\n",
      "--- Elapsed time: 0.08690214157104492 secs ---\n",
      "{'score': array([0.40557373, 0.26362863, 0.11161146, 0.04144654, 0.02641259],\n",
      "      dtype=float32), 'class': array([  3,   2,  70,  64, 169])}\n"
     ]
    }
   ],
   "source": [
    "from src.inference import model_fn, input_fn, predict_fn, output_fn\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "file_path = 'test_imgs/test_0.jpg'\n",
    "with open(file_path, mode='rb') as file:\n",
    "    img_byte = bytearray(file.read())\n",
    "data = input_fn(img_byte)\n",
    "model = model_fn('./model')\n",
    "result = predict_fn(data, model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`content_type='application/x-npy'` 일 경우 추론을 수행하는 예시이며, numpy 행렬을 그대로 전송하게 됩니다. 속도는 `content_type='application/x-image'` 보다 더 빠르지만, `tobytes()`로 \n",
    "변환하여 전송할 경우 numpy 행렬의 `dtype`과 행렬 `shape`이 보존되지 않으므로 별도의 처리가 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An input_fn that loads a image tensor\n",
      "application/x-npy\n",
      "==> model_dir : ./model\n",
      "Entering the predict_fn function\n",
      "--- Elapsed time: 0.020900249481201172 secs ---\n",
      "{'score': array([0.40557373, 0.26362863, 0.11161146, 0.04144654, 0.02641259],\n",
      "      dtype=float32), 'class': array([  3,   2,  70,  64, 169])}\n"
     ]
    }
   ],
   "source": [
    "img_arr = np.array(Image.open(file_path))\n",
    "data = input_fn(img_arr.tobytes(), request_content_type='application/x-npy')\n",
    "model = model_fn('./model')\n",
    "result = predict_fn(data, model)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Mode Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "아래 코드 셀을 실행 후, 로그를 확인해 보세요. MMS에 대한 세팅값들을 확인하실 수 있습니다.\n",
    "\n",
    "```bash\n",
    "sc0es6wfbp-algo-1-f5wnl | 2021-03-02 13:28:03,924 [INFO ] main com.amazonaws.ml.mms.ModelServer - \n",
    "sc0es6wfbp-algo-1-f5wnl | MMS Home: /opt/conda/lib/python3.6/site-packages\n",
    "sc0es6wfbp-algo-1-f5wnl | Current directory: /\n",
    "sc0es6wfbp-algo-1-f5wnl | Temp directory: /home/model-server/tmp\n",
    "sc0es6wfbp-algo-1-f5wnl | Number of GPUs: 0\n",
    "sc0es6wfbp-algo-1-f5wnl | Number of CPUs: 8\n",
    "sc0es6wfbp-algo-1-f5wnl | Max heap size: 3463 M\n",
    "sc0es6wfbp-algo-1-f5wnl | Python executable: /opt/conda/bin/python\n",
    "sc0es6wfbp-algo-1-f5wnl | Config file: /etc/sagemaker-mms.properties\n",
    "sc0es6wfbp-algo-1-f5wnl | Inference address: http://0.0.0.0:8080\n",
    "sc0es6wfbp-algo-1-f5wnl | Management address: http://0.0.0.0:8080\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 디버깅 Tip\n",
    "만약 로컬에서 추론이 잘 되는데, 엔드포인트 배포에서 에러가 발생하면 프레임워크 버전이 맞지 않거나 컨테이너 환경 변수 설정이 잘못되었을 가능성이 높습니다.\n",
    "프레임워크 버전은 최대한 동일한 버전으로 통일하되, 버전이 맞지 않으면 가장 비슷한 버전을 사용해 보세요. 아래 코드는 PyTorch 1.6.0으로 훈련한 모델을 1.5.0 버전 상에서 배포하는 예시입니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to sc0es6wfbp-algo-1-f5wnl\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:03,924 [INFO ] main com.amazonaws.ml.mms.ModelServer - \n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m MMS Home: /opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m Current directory: /\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m Temp directory: /home/model-server/tmp\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m Number of GPUs: 0\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m Number of CPUs: 8\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m Max heap size: 3463 M\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m Python executable: /opt/conda/bin/python\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m Config file: /etc/sagemaker-mms.properties\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m Inference address: http://0.0.0.0:8080\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m Management address: http://0.0.0.0:8080\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m Model Store: /.sagemaker/mms/models\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m Initial Models: ALL\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m Log dir: /logs\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m Metrics dir: /logs\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m Netty threads: 0\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m Netty client threads: 0\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m Default workers per model: 8\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m Blacklist Regex: N/A\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m Maximum Response Size: 6553500\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m Maximum Request Size: 6553500\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:03,962 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model model loaded.\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:03,974 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,133 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9002\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,133 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9001\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,134 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]49\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,134 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]48\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,134 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9006\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,134 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]51\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,134 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9005\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,134 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]50\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,137 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,137 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,137 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,138 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.6\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,138 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.6\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,138 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.6\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,139 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,139 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.6\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,140 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9004\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,140 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]46\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,140 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,140 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.6\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,141 [INFO ] W-9004-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9004\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,142 [INFO ] W-9005-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9005\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,142 [INFO ] W-9001-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9001\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,142 [INFO ] W-9002-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9002\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,141 [INFO ] W-9006-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9006\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,142 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,145 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9007\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,146 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]53\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m Model server started.\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,146 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,146 [INFO ] W-9007-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9007\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,146 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.6\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,149 [WARN ] pool-2-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,151 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9003\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,151 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]52\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,151 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,151 [INFO ] W-9003-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9003\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,152 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.6\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,153 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9005.\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,153 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9001.\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,153 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9002.\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,153 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9006.\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,153 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9004.\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,160 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9007.\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,164 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9003.\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,169 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,169 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]47\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,169 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,170 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,170 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.6\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:04,170 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,390 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - ==> model_dir : /.sagemaker/mms/models/model\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,390 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - ==> model_dir : /.sagemaker/mms/models/model\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,392 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - ==> model_dir : /.sagemaker/mms/models/model\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,392 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - ==> model_dir : /.sagemaker/mms/models/model\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,393 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - ==> model_dir : /.sagemaker/mms/models/model\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,393 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - ==> model_dir : /.sagemaker/mms/models/model\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,396 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - ==> model_dir : /.sagemaker/mms/models/model\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,396 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - ==> model_dir : /.sagemaker/mms/models/model\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,397 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - ==> model_dir : /.sagemaker/mms/models/model\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,397 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - ==> model_dir : /.sagemaker/mms/models/model\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,397 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - ==> model_dir : /.sagemaker/mms/models/model\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,397 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - ==> model_dir : /.sagemaker/mms/models/model\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,399 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - ==> model_dir : /.sagemaker/mms/models/model\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,399 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - ==> model_dir : /.sagemaker/mms/models/model\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,547 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - ==> model_dir : /.sagemaker/mms/models/model\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,548 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - ==> model_dir : /.sagemaker/mms/models/model\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,797 [WARN ] W-9002-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,797 [WARN ] W-9002-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,797 [WARN ] W-9000-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,798 [WARN ] W-9000-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,798 [WARN ] W-9005-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,798 [WARN ] W-9007-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,798 [WARN ] W-9005-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,799 [WARN ] W-9007-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,799 [WARN ] W-9004-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,799 [WARN ] W-9004-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,799 [WARN ] W-9003-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,799 [WARN ] W-9003-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,799 [WARN ] W-9001-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,800 [WARN ] W-9001-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,821 [WARN ] W-9006-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,821 [WARN ] W-9006-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,898 [WARN ] W-9002-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   0%|          | 0.00/44.7M [00:00<?, ?B/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,898 [WARN ] W-9000-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   0%|          | 0.00/44.7M [00:00<?, ?B/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,899 [WARN ] W-9005-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   0%|          | 0.00/44.7M [00:00<?, ?B/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,899 [WARN ] W-9007-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   0%|          | 0.00/44.7M [00:00<?, ?B/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,899 [WARN ] W-9004-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   0%|          | 0.00/44.7M [00:00<?, ?B/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,907 [WARN ] W-9003-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   0%|          | 0.00/44.7M [00:00<?, ?B/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,907 [WARN ] W-9001-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   0%|          | 0.00/44.7M [00:00<?, ?B/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,921 [WARN ] W-9006-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   0%|          | 0.00/44.7M [00:00<?, ?B/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,998 [WARN ] W-9002-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  25%|██▌       | 11.3M/44.7M [00:00<00:00, 118MB/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,999 [WARN ] W-9007-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  26%|██▌       | 11.5M/44.7M [00:00<00:00, 121MB/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,999 [WARN ] W-9005-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  21%|██▏       | 9.52M/44.7M [00:00<00:00, 99.8MB/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:05,999 [WARN ] W-9000-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  36%|███▌      | 16.0M/44.7M [00:00<00:00, 167MB/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:06,000 [WARN ] W-9004-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  17%|█▋        | 7.62M/44.7M [00:00<00:00, 79.7MB/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:06,007 [WARN ] W-9003-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  25%|██▌       | 11.2M/44.7M [00:00<00:00, 109MB/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:06,013 [WARN ] W-9001-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  39%|███▊      | 17.2M/44.7M [00:00<00:00, 168MB/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:06,021 [WARN ] W-9006-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  33%|███▎      | 14.8M/44.7M [00:00<00:00, 155MB/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:06,098 [WARN ] W-9002-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  59%|█████▉    | 26.4M/44.7M [00:00<00:00, 128MB/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:06,098 [WARN ] W-9001-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  74%|███████▍  | 33.2M/44.7M [00:00<00:00, 165MB/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:06,099 [WARN ] W-9007-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  61%|██████    | 27.0M/44.7M [00:00<00:00, 131MB/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:06,099 [WARN ] W-9005-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  47%|████▋     | 21.1M/44.7M [00:00<00:00, 105MB/s] \n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:06,099 [WARN ] W-9004-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  35%|███▍      | 15.4M/44.7M [00:00<00:00, 80.3MB/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:06,100 [WARN ] W-9000-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  69%|██████▊   | 30.7M/44.7M [00:00<00:00, 163MB/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:06,100 [WARN ] W-9006-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  66%|██████▋   | 29.6M/44.7M [00:00<00:00, 155MB/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:06,101 [WARN ] W-9000-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - 100%|██████████| 44.7M/44.7M [00:00<00:00, 157MB/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:06,107 [WARN ] W-9003-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  50%|█████     | 22.4M/44.7M [00:00<00:00, 111MB/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:06,119 [WARN ] W-9002-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  90%|█████████ | 40.3M/44.7M [00:00<00:00, 133MB/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:06,126 [WARN ] W-9007-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  94%|█████████▍| 42.0M/44.7M [00:00<00:00, 138MB/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:06,164 [WARN ] W-9003-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  75%|███████▌  | 33.6M/44.7M [00:00<00:00, 113MB/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:06,197 [WARN ] W-9005-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  73%|███████▎  | 32.7M/44.7M [00:00<00:00, 110MB/s]\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:06,199 [WARN ] W-9004-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  55%|█████▌    | 24.6M/44.7M [00:00<00:00, 84.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:06,209 [WARN ] W-9004-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  93%|█████████▎| 41.6M/44.7M [00:00<00:00, 100MB/s] \n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:06,630 [INFO ] W-9001-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2431\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:08,085 [INFO ] W-9002-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3894\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:08,105 [INFO ] W-9004-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3919\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:08,106 [INFO ] W-9005-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3915\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:08,107 [INFO ] W-9006-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3921\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:08,128 [INFO ] pool-1-thread-9 ACCESS_LOG - /172.18.0.1:37932 \"GET /ping HTTP/1.1\" 200 18\n",
      "!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.pytorch.model.PyTorchPredictor at 0x7f6c65e25dd8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:08,219 [INFO ] W-9003-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 4033\r\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:08,235 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 4048\r\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:08,238 [INFO ] W-9007-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 4031\r\n"
     ]
    }
   ],
   "source": [
    "local_model_path = f'file://{os.getcwd()}/model/model.tar.gz'\n",
    "endpoint_name = \"local-endpoint-bangali-classifier-{}\".format(int(time.time()))\n",
    "\n",
    "local_pytorch_model = PyTorchModel(model_data=local_model_path,\n",
    "                                   role=role,\n",
    "                                   entry_point='./src/inference.py',\n",
    "                                   framework_version='1.5.0',\n",
    "                                   py_version='py3')\n",
    "\n",
    "local_pytorch_model.deploy(instance_type='local', \n",
    "                           initial_instance_count=1, \n",
    "                           endpoint_name=endpoint_name,\n",
    "                           wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로컬에서 컨테이너를 배포했기 때문에 컨테이너가 현재 실행 중임을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE                                                                               COMMAND                  CREATED             STATUS              PORTS                              NAMES\r\n",
      "afeebf748adc        763104351884.dkr.ecr.ap-northeast-2.amazonaws.com/pytorch-inference:1.5.0-cpu-py3   \"python /usr/local/b…\"   23 seconds ago      Up 15 seconds       0.0.0.0:8080->8080/tcp, 8081/tcp   sc0es6wfbp-algo-1-f5wnl\r\n"
     ]
    }
   ],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker SDK `predict()` 메서드로 추론을 수행할 수도 있지만, 이번에는 boto3의 `invoke_endpoint()` 메서드로 추론을 수행해 보겠습니다.<br>\n",
    "Boto3는 서비스 레벨의 low-level SDK로, ML 실험에 초점을 맞춰 일부 기능들이 추상화된 high-level SDK인 SageMaker SDK와 달리\n",
    "SageMaker API를 완벽하게 제어할 수 있습으며, 프로덕션 및 자동화 작업에 적합합니다.\n",
    "\n",
    "참고로 `invoke_endpoint()` 호출을 위한 런타임 클라이언트 인스턴스 생성 시, 로컬 배포 모드에서는 `sagemaker.local.LocalSagemakerRuntimeClient()`를 호출해야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:19,702 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - An input_fn that loads a image tensor\r\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:19,702 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - application/x-npy\r\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:19,703 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Entering the predict_fn function\r\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:19,703 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Entering the predict_fn function\r\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:19,759 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - --- Elapsed time: 0.05574846267700195 secs ---\r\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:19,759 [INFO ] W-9001-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 58\r\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:19,759 [INFO ] W-9001-model ACCESS_LOG - /172.18.0.1:38414 \"POST /invocations HTTP/1.1\" 200 61\r\n",
      "{\"score\": [0.4055737257003784, 0.26362863183021545, 0.11161146312952042, 0.041446536779403687, 0.026412585750222206], \"class\": [3, 2, 70, 64, 169]}\n"
     ]
    }
   ],
   "source": [
    "client = sagemaker.local.LocalSagemakerClient()\n",
    "runtime_client = sagemaker.local.LocalSagemakerRuntimeClient()\n",
    "endpoint_name = local_pytorch_model.endpoint_name\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType='application/x-npy',\n",
    "    Accept='application/json',\n",
    "    Body=img_arr.tobytes()\n",
    "    )\n",
    "print(response['Body'].read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:27,264 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - An input_fn that loads a image tensor\r\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:27,265 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - application/x-image\r\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:27,265 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Entering the predict_fn function\r\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:27,265 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Entering the predict_fn function\r\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:27,320 [INFO ] W-9002-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 63\r\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:27,321 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - --- Elapsed time: 0.05550980567932129 secs ---\r\n",
      "\u001b[36msc0es6wfbp-algo-1-f5wnl |\u001b[0m 2021-03-02 13:28:27,321 [INFO ] W-9002-model ACCESS_LOG - /172.18.0.1:38414 \"POST /invocations HTTP/1.1\" 200 65\r\n",
      "{'score': [0.4055737257003784, 0.26362863183021545, 0.11161146312952042, 0.041446536779403687, 0.026412585750222206], 'class': [3, 2, 70, 64, 169]}\n"
     ]
    }
   ],
   "source": [
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType='application/x-image',\n",
    "    Accept='application/json',\n",
    "    Body=img_byte\n",
    "    )\n",
    "\n",
    "print(json.loads(response['Body'].read().decode()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Mode Endpoint Clean-up\n",
    "\n",
    "엔드포인트를 계속 사용하지 않는다면, 엔드포인트를 삭제해야 합니다. \n",
    "SageMaker SDK에서는 `delete_endpoint()` 메소드로 간단히 삭제할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_endpoint(client, endpoint_name):\n",
    "    response = client.describe_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "    model_name = response['ProductionVariants'][0]['ModelName']\n",
    "\n",
    "    client.delete_model(ModelName=model_name)    \n",
    "    client.delete_endpoint(EndpointName=endpoint_name)\n",
    "    client.delete_endpoint_config(EndpointConfigName=endpoint_name)    \n",
    "    \n",
    "    print(f'--- Deleted model: {model_name}')\n",
    "    print(f'--- Deleted endpoint: {endpoint_name}')\n",
    "    print(f'--- Deleted endpoint_config: {endpoint_name}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gracefully stopping... (press Ctrl+C again to force)\n",
      "--- Deleted model: pytorch-inference-2021-03-02-13-27-11-884\n",
      "--- Deleted endpoint: local-endpoint-bangali-classifier-1614691629\n",
      "--- Deleted endpoint_config: local-endpoint-bangali-classifier-1614691629\n"
     ]
    }
   ],
   "source": [
    "delete_endpoint(client, endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컨테이너가 삭제된 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\r\n"
     ]
    }
   ],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. SageMaker Hosted Endpoint Inference\n",
    "---\n",
    "\n",
    "이제 실제 운영 환경에 엔드포인트 배포를 수행해 보겠습니다. 로컬 모드 엔드포인트와 대부분의 코드가 동일하며, 모델 아티팩트 경로(`model_data`)와 인스턴스 유형(`instance_type`)만 변경해 주시면 됩니다. SageMaker가 관리하는 배포 클러스터를 프로비저닝하는 시간이 소요되기 때문에 추론 서비스를 시작하는 데에는 약 5~10분 정도 소요됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "client = boto3.client('sagemaker')\n",
    "runtime_client = boto3.client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_path(sm_client, max_results=1, name_contains='pytorch'):\n",
    "    training_job = sm_client.list_training_jobs(MaxResults=max_results,\n",
    "                                         NameContains=name_contains,\n",
    "                                         SortBy='CreationTime', \n",
    "                                         SortOrder='Descending')\n",
    "    training_job_name = training_job['TrainingJobSummaries'][0]['TrainingJobName']\n",
    "    training_job_description = sm_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "    model_path = training_job_description['ModelArtifacts']['S3ModelArtifacts']  \n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!CPU times: user 2.88 s, sys: 422 ms, total: 3.3 s\n",
      "Wall time: 7min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_path = get_model_path(client, max_results=3)\n",
    "endpoint_name = \"endpoint-bangali-classifier-{}\".format(int(time.time()))\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data=model_path,\n",
    "                                   role=role,\n",
    "                                   entry_point='./src/inference.py',\n",
    "                                   framework_version='1.5.0',\n",
    "                                   py_version='py3')\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type='ml.m5.xlarge', \n",
    "                                 initial_instance_count=1, \n",
    "                                 endpoint_name=endpoint_name,\n",
    "                                 wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointName': 'endpoint-bangali-classifier-1614692838',\n",
       " 'EndpointArn': 'arn:aws:sagemaker:ap-northeast-2:387793684046:endpoint/endpoint-bangali-classifier-1614692838',\n",
       " 'EndpointConfigName': 'endpoint-bangali-classifier-1614692838',\n",
       " 'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "   'DeployedImages': [{'SpecifiedImage': '763104351884.dkr.ecr.ap-northeast-2.amazonaws.com/pytorch-inference:1.5.0-cpu-py3',\n",
       "     'ResolvedImage': '763104351884.dkr.ecr.ap-northeast-2.amazonaws.com/pytorch-inference@sha256:fdd5a5514161af205d600520f70e762e36f4ce0fa89ad8667a0b59ee2dda44e4',\n",
       "     'ResolutionTime': datetime.datetime(2021, 3, 2, 13, 47, 24, 613000, tzinfo=tzlocal())}],\n",
       "   'CurrentWeight': 1.0,\n",
       "   'DesiredWeight': 1.0,\n",
       "   'CurrentInstanceCount': 1,\n",
       "   'DesiredInstanceCount': 1}],\n",
       " 'EndpointStatus': 'InService',\n",
       " 'CreationTime': datetime.datetime(2021, 3, 2, 13, 47, 22, 26000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2021, 3, 2, 13, 54, 31, 295000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '03d06cfa-7c28-4966-b0f4-56c4b73d1e17',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '03d06cfa-7c28-4966-b0f4-56c4b73d1e17',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '782',\n",
       "   'date': 'Tue, 02 Mar 2021 13:55:22 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "client = boto3.client('sagemaker')\n",
    "runtime_client = boto3.client('sagemaker-runtime')\n",
    "endpoint_name = pytorch_model.endpoint_name\n",
    "client.describe_endpoint(EndpointName = endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추론을 수행합니다. 로컬 모드의 코드와 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': [0.4055737257003784, 0.26362863183021545, 0.11161146312952042, 0.041446536779403687, 0.026412585750222206], 'class': [3, 2, 70, 64, 169]}\n"
     ]
    }
   ],
   "source": [
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType='application/x-image',\n",
    "    Accept='application/json',\n",
    "    Body=img_byte\n",
    "    )\n",
    "\n",
    "print(json.loads(response['Body'].read().decode()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Hosted Endpoint Clean-up\n",
    "\n",
    "엔드포인트를 계속 사용하지 않는다면, 불필요한 과금을 피하기 위해 엔드포인트를 삭제해야 합니다. \n",
    "SageMaker SDK에서는 `delete_endpoint()` 메소드로 간단히 삭제할 수 있으며, UI에서도 쉽게 삭제할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Deleted model: pytorch-inference-2021-03-02-13-47-21-516\n",
      "--- Deleted endpoint: endpoint-bangali-classifier-1614692838\n",
      "--- Deleted endpoint_config: endpoint-bangali-classifier-1614692838\n"
     ]
    }
   ],
   "source": [
    "delete_endpoint(client, endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
