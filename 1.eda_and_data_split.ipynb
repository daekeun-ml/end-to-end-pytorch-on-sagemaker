{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1. EDA and Data Split\n",
    "---\n",
    "\n",
    "본 모듈에서는 EDA(Exploratory Data Analysis; 탐색적 데이터 분석)을 통해 데이터를 간단하게 살펴 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "!pip -q install --upgrade pip nvidia-ml-py3\n",
    "!pip -q install --upgrade awscli boto3 pandas joblib pyarrow iterative-stratification\n",
    "!pip -q install --upgrade kaggle sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, glob2\n",
    "import joblib\n",
    "\n",
    "import pyarrow\n",
    "import PIL.Image as Image, PIL.ImageDraw as ImageDraw, PIL.ImageFont as ImageFont\n",
    "\n",
    "WIDTH = 236\n",
    "HEIGHT = 137\n",
    "data_dir = './input'\n",
    "raw_data_dir = './input/raw'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Prepraing Dataset\n",
    "---\n",
    "### Download dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1. Download using Kaggle API\n",
    "Kaggle API를 통해 데이터셋을 다운로드받은 후, 압축을 해제합니다. 코드 셀 수행을 위해서 Kaggle에 가입하여 `kaggle.json`을 다운로드받으셔야 합니다.\n",
    "\n",
    "[주의] 아래 작업을 로컬에서 원활히 수행하려면 16GB 이상의 메모리가 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash -s {raw_data_dir}\n",
    "# rm -rf ~/.kaggle\n",
    "# mkdir ~/.kaggle\n",
    "# chmod 600 kaggle.json\n",
    "# cp kaggle.json ~/.kaggle/kaggle.json\n",
    "# kaggle competitions download -c bengaliai-cv19 -p $1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip {raw_data_dir}/bengaliai-cv19.zip -d {data_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_files_train = sorted(glob2.glob(f'{data_dir}/train_*.parquet')); print(raw_files_train)\n",
    "# raw_files_test = sorted(glob2.glob(f'{data_dir}/test_*.parquet')); print(raw_files_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in raw_files_train:\n",
    "#     newfile = file.replace('parquet', 'feather')\n",
    "#     print(f'Converting parquet to feather - {file}')\n",
    "#     data = pd.read_parquet(file)\n",
    "#     data.to_feather(newfile)\n",
    "#     os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in raw_files_test:\n",
    "#     newfile = file.replace('parquet', 'feather')\n",
    "#     print(f'Converting parquet to feather - {file}')\n",
    "#     data = pd.read_parquet(file)\n",
    "#     data.to_feather(newfile)\n",
    "#     os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2. Download from Amazon S3\n",
    "Amazon S3에 저장된 데이터셋을 다운로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $data_dir\n",
    "!aws s3 cp --recursive s3://daekeun-workshop-public-material/bangali-handwritten/inputs $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_train = sorted(glob2.glob(f'{data_dir}/train_*.feather')); print(files_train)\n",
    "files_test = sorted(glob2.glob(f'{data_dir}/test_*.feather')); print(files_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read feather files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 데이터셋은 모두 4개의 feather 파일들로 구성되어 있고, 각 feather 파일은 32,333개의 컬럼으로 구성된 5만여 장의 이미지 데이터가 포함되어 있습니다.\n",
    "- $32,333 = 1+(137 \\times 236)$, 첫번째 컬럼은 Train Index로 이를 통해 정답 레이블을 알 수 있습니다.\n",
    "- $137$: Height, $236$: Width\n",
    "\n",
    "참고로, Kaggle의 원 데이터는 parquet 파일이며 parquet 파일은 `to_feather()` 메서드로 feather 포맷으로 쉽게 변환할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_img0 = pd.read_feather(files_train[0])\n",
    "print(train_img0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. EDA (Exploratory Data Analysis)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check raw images\n",
    "\n",
    "실제 raw 이미지 데이터를 확인해 봅니다. 아래 셀을 여러 번 반복해서 실행해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(train_img0)\n",
    "idx = np.random.randint(num_train)\n",
    "raw_img = train_img0.iloc[idx, 1:].values.astype(np.uint8)\n",
    "plt.imshow(255 - raw_img.reshape(HEIGHT, WIDTH), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./input/train.csv')\n",
    "display(train_df.head())\n",
    "print(f'Number of unique grapheme roots: {train_df[\"grapheme_root\"].nunique()}')\n",
    "print(f'Number of unique vowel diacritic: {train_df[\"vowel_diacritic\"].nunique()}')\n",
    "print(f'Number of unique consonant diacritic: {train_df[\"consonant_diacritic\"].nunique()}')\n",
    "print(f'Number of training data: {train_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map_df = pd.read_csv('./input/class_map.csv')\n",
    "display(class_map_df.head())\n",
    "print(f'Size of class map: {class_map_df.shape}')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most used top 10 Grapheme Roots in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n(df, class_map_df, field, n, top=True):\n",
    "    top_graphemes = df.groupby([field]).size().reset_index(name='counts')['counts'].sort_values(ascending=not top)[:n]\n",
    "    top_grapheme_roots = top_graphemes.index\n",
    "    top_grapheme_counts = top_graphemes.values\n",
    "    top_graphemes = class_map_df[class_map_df['component_type'] == field].reset_index().iloc[top_grapheme_roots]\n",
    "    top_graphemes.drop(['component_type', 'label'], axis=1, inplace=True)\n",
    "    top_graphemes.loc[:, 'count'] = top_grapheme_counts\n",
    "    return top_graphemes\n",
    "\n",
    "def image_from_char(char, fontsize=120):\n",
    "    image = Image.new('RGB', (WIDTH, HEIGHT))\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    myfont = ImageFont.truetype('kalpurush-2.ttf', fontsize)\n",
    "    w, h = draw.textsize(char, font=myfont)\n",
    "    draw.text(((WIDTH - w) / 2,(HEIGHT - h) / 3), char, font=myfont)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_roots = get_n(train_df, class_map_df, 'grapheme_root', 10)\n",
    "top_10_roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 5, figsize=(12, 5))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i in range(10):\n",
    "    ax[i].imshow(image_from_char(top_10_roots['component'].iloc[i]), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 Vowel Diacritic in taining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_vowels = get_n(train_df, class_map_df, 'vowel_diacritic', 5)\n",
    "top_5_vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 5, figsize=(12, 3))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i in range(5):\n",
    "    ax[i].imshow(image_from_char(top_5_vowels['component'].iloc[i]), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 Consonants Diacritic in taining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_consonants = get_n(train_df, class_map_df, 'consonant_diacritic', 5)\n",
    "top_5_consonants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 5, figsize=(12, 3))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i in range(5):\n",
    "    ax[i].imshow(image_from_char(top_5_consonants['component'].iloc[i]), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_count(feature, title, df, size=1):\n",
    "    '''\n",
    "    Plot count of classes of selected feature; feature is a categorical value\n",
    "    param: feature - the feature for which we present the distribution of classes\n",
    "    param: title - title to show in the plot\n",
    "    param: df - dataframe \n",
    "    param: size - size (from 1 to n), multiplied with 4 - size of plot\n",
    "    '''\n",
    "    f, ax = plt.subplots(1, 1, figsize=(4*size,4))\n",
    "    total = float(len(df))\n",
    "    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette='Set3')\n",
    "    g.set_title(\"Number and percentage of {}\".format(title))\n",
    "    if(size > 2):\n",
    "        plt.xticks(rotation=90, size=8)\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x()+p.get_width()/2.,\n",
    "                height + 3, '{:1.2f}%'.format(100*height/total),ha=\"center\") \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count('grapheme_root', 'grapheme_root (train)', train_df, size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count('vowel_diacritic', 'vowel_diacritic (train)', train_df, size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count('consonant_diacritic', 'consonant_diacritic (train)', train_df, size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Split Train/Validation Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1개의 레이블이 아닌 multi-label에 대한 층화추출이 필요하면 아래 라이브러리를 설치하시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['id'] = train_df['image_id'].apply(lambda x: int(x.split('_')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['id', 'grapheme_root', 'vowel_diacritic', 'consonant_diacritic']\n",
    "X = train_df[cols].values[:,0]\n",
    "y = train_df[cols].values[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "mask = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['fold'] = -1\n",
    "\n",
    "for i, (trn_idx, vld_idx) in enumerate(mask.split(X,y)):\n",
    "    train_df.loc[vld_idx, 'fold'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['fold'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fold0 훈련 데이터셋의 클래스 분포가 전체 훈련 데이터의 분포와 동일한지 확인해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_fold0 = train_df[train_df['fold']==0]\n",
    "plot_count('vowel_diacritic', 'vowel_diacritic (train)', train_df_fold0, size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fold 컬럼이 저장된 데이터프레임을 별도의 csv파일로 저장합니다. k-fold cross validation 시에, 이 csv를 재사용할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(f'{data_dir}/train_folds.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. Copy Files to S3\n",
    "\n",
    "아래 코드 셀은 로컬 환경에 저장되어 있는 훈련 데이터를 S3로 전송합니다. 네트워크 속도가 빠르지 않다면 수 분이 소요될 수 있습니다.\n",
    "참고로, 기본 버킷은 `'sagemaker-[YOUR REGION]-[YOUT ACCOUNT ID]'` 으로 자동으로 지정되어 있고, 여러분의 고유한 S3 생성 후 이를 지정할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3, os\n",
    "import sagemaker\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = 'bangali/train'\n",
    "s3_bucket = boto3.Session().resource('s3').Bucket(bucket)\n",
    "\n",
    "for file in files_train:\n",
    "    f = file.split('/')[-1]\n",
    "    s3_bucket.Object(os.path.join(prefix, f)).upload_file(file)\n",
    "    \n",
    "s3_bucket.Object(os.path.join(prefix, 'train_folds.csv')).upload_file(f'{data_dir}/train_folds.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
